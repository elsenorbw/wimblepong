{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/WimblepongCustomTrainingEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCYY_vKN-fum"
   },
   "source": [
    "# Wimblepong 2124\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Welcome, servant of Sovereign. Thank you for your assistance in training bots for the upcoming human vs AI competition. The stakes are incredibly high; while freeing humanity from Sovereign's control is a noble cause, it also puts Earth at great risk. Our primary goal is to save Earth, and your efforts in training these bots are crucial to achieving this balance. In this Colab notebook, you will guide the creation, training, and evaluation of AI models for the high-stakes tournament of WimblePong. Additionally, you will learn how to export the trained model for deployment across various platforms. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmP2pRIbAjml"
   },
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "We first need to install several libraries that are essential for our environment and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkhwGXZyRfWZ"
   },
   "outputs": [],
   "source": [
    "# you can skip this if you have installed requirements.txt locally..\n",
    "\n",
    "%pip install gym\n",
    "%pip install stable-baselines3[extra]\n",
    "%pip install tensorflowjs\n",
    "%pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
    "\n",
    "# onnx2tf deps:\n",
    "%pip install onnx_graphsurgeon\n",
    "%pip install sng4onnx\n",
    "%pip install onnxsim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eprDnpidBH95"
   },
   "source": [
    "### Mounting Google Drive\n",
    "\n",
    "We will mount Google Drive to save and load our models and other necessary files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vYCJyx3kJikU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get a colab drive.. going local\n",
      "Working in ./data/monday-8-july\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DAY = \"monday-8-july\"\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
    "except:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(f\"Couldn't get a colab drive.. going local\")\n",
    "    DRIVE_PATH = \"./data\"\n",
    "    Path(f\"{DRIVE_PATH}/{DAY}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"Working in {DRIVE_PATH}/{DAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDINV-FRBJO0"
   },
   "source": [
    "### Importing Necessary Libraries\n",
    "\n",
    "Import all the necessary libraries required for creating the custom environment, training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eRePUZ1QRh6C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version: 0.29.1\n",
      "stable-baselines3 version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "from IPython.display import display, Image, HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch as th\n",
    "import torch.onnx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import onnx\n",
    "from onnx2tf import convert\n",
    "import onnxruntime as ort\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import pygame\n",
    "\n",
    "\n",
    "# Check versions\n",
    "print(\"gym version:\", gym.__version__)\n",
    "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEn2sPwCBMkp"
   },
   "source": [
    "### Setting Up Environment Parameters\n",
    "\n",
    "Define the constants that will be used in the Pong environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NLzB2yLbociM"
   },
   "outputs": [],
   "source": [
    "COURT_HEIGHT = 800\n",
    "COURT_WIDTH = 1200\n",
    "PADDLE_HEIGHT = 90\n",
    "PADDLE_WIDTH = 15\n",
    "BALL_RADIUS = 12\n",
    "INITIAL_BALL_SPEED = 10\n",
    "PADDLE_GAP = 10\n",
    "PADDLE_SPEED_DIVISOR = 15\n",
    "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4\n",
    "SPEED_INCREMENT = 0.6\n",
    "SERVING_HEIGHT_MULTIPLIER = 2\n",
    "PLAYER_COLOURS = {'Player1': 'yellow', 'Player2': 'grey'}\n",
    "MAX_COMPUTER_PADDLE_SPEED = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeJpcg2GBUbb"
   },
   "source": [
    "### Creating Helper Classes and Functions\n",
    "\n",
    "Define helper classes and functions to manage game state, players, and reward system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DZzIav-qUBzp"
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "    Player1 = 'Player1'\n",
    "    Player2 = 'Player2'\n",
    "\n",
    "class PlayerPositions:\n",
    "    Initial = 'Initial'\n",
    "    Reversed = 'Reversed'\n",
    "\n",
    "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
    "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
    "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
    "    return normalized_relative_intersect_y * (math.pi / 4)\n",
    "\n",
    "def bounded_value(value, min_value, max_value):\n",
    "    return max(min_value, min(max_value, value))\n",
    "\n",
    "def transform_action(action):\n",
    "    button_pressed = action[0] > 0.5\n",
    "    paddle_direction = max(min(action[1], 1), -1)\n",
    "    actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 30}\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZc0UzyLBrRm"
   },
   "source": [
    "### Computer Player Class\n",
    "\n",
    "Define a class for the computer player with methods for resetting and getting actions. This class gives us a deterministic computer opponent which our model can train against.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N932taVhk14O"
   },
   "outputs": [],
   "source": [
    "class ComputerPlayer:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.serve_delay = random.randint(10, 10)\n",
    "        self.direction = random.randint(-60, 60)\n",
    "        self.offset = random.randint(-PADDLE_HEIGHT/2, PADDLE_HEIGHT/2)\n",
    "        self.serve_delay_counter = 0\n",
    "        self.max_speed = MAX_COMPUTER_PADDLE_SPEED\n",
    "\n",
    "\n",
    "    def get_actions(self, player, state):\n",
    "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
    "        if state['ball']['score_mode']:\n",
    "            return {'button_pressed': False, 'paddle_direction': 0}\n",
    "        paddle = state[player]\n",
    "        if state['ball']['serve_mode']:\n",
    "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
    "                self.direction = -self.direction\n",
    "            if self.serve_delay_counter > self.serve_delay:\n",
    "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
    "            else:\n",
    "                self.serve_delay_counter += 1\n",
    "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
    "        if is_left:\n",
    "            return {\n",
    "                'button_pressed': False,\n",
    "                'paddle_direction': bounded_value(\n",
    "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2,\n",
    "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
    "                    MAX_COMPUTER_PADDLE_SPEED\n",
    "                )\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'button_pressed': False,\n",
    "                'paddle_direction': -bounded_value(\n",
    "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2  ,\n",
    "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
    "                    MAX_COMPUTER_PADDLE_SPEED\n",
    "                )\n",
    "            }\n",
    "\n",
    "\n",
    "class ModelPlayer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def reset(self):\n",
    "        return None\n",
    "\n",
    "    def get_actions(self, player, state):\n",
    "        return transform_action(self.model.predict(get_observation(state, player))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urmczdrcBqnh"
   },
   "source": [
    "### Reward System Class\n",
    "\n",
    "Define a class to manage the reward system for training the model. This class will calculate and apply rewards based on game events and player actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b3MKATB64e3l"
   },
   "outputs": [],
   "source": [
    "class RewardSystem:\n",
    "    def __init__(self, rewarded_player):\n",
    "        self.rewarded_player = rewarded_player\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.step_count += 1\n",
    "\n",
    "    def pre_serve_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            self.total_reward -= 0.05\n",
    "\n",
    "    def serve_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            ball = game_state['ball']\n",
    "            reward = abs(ball['dy']) * abs(ball['dy']) - 30\n",
    "            self.total_reward += reward\n",
    "\n",
    "    def hit_paddle_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            reward = 50\n",
    "            # reward = 50 + abs(game_state[player]['dy'])\n",
    "            self.total_reward += reward\n",
    "\n",
    "    def conceed_point_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            punishment = abs(game_state['ball']['y'] - (game_state[player]['y'] + game_state[player]['height'])) / 8\n",
    "            # punishment = abs(game_state['ball']['y'] - (game_state[player]['y'] + game_state[player]['height'])) / 4\n",
    "            self.total_reward -= punishment\n",
    "\n",
    "    def score_point_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            reward = 200\n",
    "            self.total_reward += reward\n",
    "\n",
    "    def paddle_movement_reward(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "            paddle = game_state[player]\n",
    "            # reward = -0.5 if abs(paddle['dy']) < 0.2 else 0\n",
    "            reward = 0\n",
    "            self.total_reward += reward\n",
    "\n",
    "    def end_episode(self, player, game_state):\n",
    "        if player == self.rewarded_player:\n",
    "          if game_state['ball']['serve_mode'] and player == game_state['server']:\n",
    "            self.total_reward -= 200\n",
    "          else:\n",
    "            self.total_reward += 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1djjfQapBxQt"
   },
   "source": [
    "### Pong Game Class\n",
    "\n",
    "Define the main class for the Pong game, managing the game state and updates. This class handles the game logic, including ball movement, collisions, scoring, and paddle controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iIpdvdwMk14O"
   },
   "outputs": [],
   "source": [
    "class PongGame:\n",
    "    def __init__(self, server, positions_reversed, player, opponent):\n",
    "        self.game_state = {\n",
    "        'server': server,\n",
    "        'positions_reversed': positions_reversed,\n",
    "        'player': player,\n",
    "        'opponent': opponent,\n",
    "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
    "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
    "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
    "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
    "        }\n",
    "        self.apply_meta_game_state()\n",
    "\n",
    "    def apply_meta_game_state(self):\n",
    "        game_state = self.game_state\n",
    "        serving_player = game_state['server']\n",
    "        positions_reversed = game_state['positions_reversed']\n",
    "        if serving_player == Player.Player1:\n",
    "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
    "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
    "        else:\n",
    "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
    "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
    "        if positions_reversed:\n",
    "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
    "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
    "            self.game_state[Player.Player1]['y'] = random.randint(0, COURT_HEIGHT)\n",
    "            self.game_state[Player.Player2]['y'] = random.randint(0, COURT_HEIGHT)\n",
    "        else:\n",
    "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
    "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
    "        ball = self.game_state['ball']\n",
    "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
    "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
    "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
    "        ball['speed'] = INITIAL_BALL_SPEED\n",
    "        ball['serve_mode'] = True\n",
    "        ball['score_mode'] = False\n",
    "        ball['score_mode_timeout'] = 0\n",
    "        self.game_state['stats']['rally_length'] = 0\n",
    "\n",
    "    def update_game_state(self, actions, delta_time, reward_system):\n",
    "        reward = 0\n",
    "        game_state = self.game_state\n",
    "        ball = game_state['ball']\n",
    "        stats = game_state['stats']\n",
    "        server = game_state['server']\n",
    "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
    "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
    "        if ball['score_mode']:\n",
    "            return True\n",
    "        elif ball['serve_mode']:\n",
    "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
    "\n",
    "            reward_system.pre_serve_reward(server, game_state)\n",
    "\n",
    "            if actions[server]['button_pressed']:\n",
    "                ball['speed'] = INITIAL_BALL_SPEED\n",
    "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
    "                ball['serve_mode'] = False\n",
    "                stats['rally_length'] += 1\n",
    "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
    "                stats['server'] = server\n",
    "\n",
    "                reward_system.serve_reward(server, game_state)\n",
    "\n",
    "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
    "            ball['y'] += ball['dy'] * delta_time\n",
    "        else:\n",
    "            ball['x'] += ball['dx'] * delta_time\n",
    "            ball['y'] += ball['dy'] * delta_time\n",
    "            if ball['y'] - ball['radius'] < 0:\n",
    "                ball['dy'] = -ball['dy']\n",
    "                ball['y'] = ball['radius']\n",
    "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
    "                ball['dy'] = -ball['dy']\n",
    "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
    "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
    "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
    "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
    "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
    "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
    "                ball['speed'] += SPEED_INCREMENT\n",
    "                stats['rally_length'] += 1\n",
    "\n",
    "                if paddle_left == game_state['player']:\n",
    "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
    "\n",
    "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
    "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
    "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
    "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
    "                ball['x'] = paddle_right['x'] - ball['radius']\n",
    "                ball['speed'] += SPEED_INCREMENT\n",
    "                stats['rally_length'] += 1\n",
    "\n",
    "                if paddle_right == game_state['player']:\n",
    "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
    "\n",
    "            if ball['x'] - ball['radius'] < 0:\n",
    "                ball['score_mode'] = True\n",
    "\n",
    "                if player_is_left:\n",
    "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
    "                else:\n",
    "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
    "\n",
    "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
    "                ball['score_mode'] =monday True\n",
    "\n",
    "                if not player_is_left:\n",
    "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
    "                else:\n",
    "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
    "\n",
    "        if game_state['positions_reversed']:\n",
    "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
    "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
    "        else:\n",
    "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
    "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
    "\n",
    "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
    "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
    "\n",
    "        if paddle_left['y'] < 0:\n",
    "            paddle_left['y'] = 0\n",
    "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
    "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
    "        if paddle_right['y'] < 0:\n",
    "            paddle_right['y'] = 0\n",
    "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
    "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
    "\n",
    "        reward_system.paddle_movement_reward(self.game_state['player'], game_state)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnjhoq69Zyvu"
   },
   "source": [
    "# Set up observation space\n",
    "\n",
    "The following code defines the observation space for the model we are training as well as a function to tranform the game state into an observation the model can make predictions from.\n",
    "\n",
    "## Normalisation\n",
    "\n",
    "Normalisation is a crucial step in preparing data for machine learning models, especially when the data has features with different ranges. Read more about the specific normalisation happening in `get_observation` [in this document](https://github.com/mrbenbot/wimblepong/blob/main/docs/normalisation.md).\n",
    "\n",
    "## Customising the observation space\n",
    "\n",
    "If you decide to change the observation space or the way it is normalised, you can upload a `.js` file along with your `model.json` and `weights.bin` to ensure your model is receiving the same input when playing real WimblePong as when being trained.\n",
    "\n",
    "[read more here](https://github.com/mrbenbot/wimblepong/blob/main/docs/custom_observations.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K4syobsjZzFX"
   },
   "outputs": [],
   "source": [
    "observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
    "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
    "        )\n",
    "\n",
    "def get_observation(player, game_state):\n",
    "    is_server = 1 if game_state['server'] == player else 0\n",
    "    paddle = game_state[player]\n",
    "    return np.array([\n",
    "        float(game_state['ball']['x'] / COURT_WIDTH),\n",
    "        float(game_state['ball']['y'] / COURT_HEIGHT),\n",
    "        float(game_state['ball']['dx'] / 40),\n",
    "        float(game_state['ball']['dy'] / 40),\n",
    "        float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
    "        float(paddle['y'] / COURT_HEIGHT),\n",
    "        float(int(game_state['ball']['serve_mode'])),\n",
    "        float(int(is_server)),\n",
    "    ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mgmY-tWBzjh"
   },
   "source": [
    "### Custom Pong Environment Class\n",
    "\n",
    "Define a custom gym environment for the Pong game. This environment will interface with the stable-baselines3 library for training the reinforcement learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ExmAn7VtUakq"
   },
   "outputs": [],
   "source": [
    "class CustomPongEnv(gym.Env):\n",
    "    def __init__(self, computer_player):\n",
    "        super(CustomPongEnv, self).__init__()\n",
    "\n",
    "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
    "        self.observation_space = observation_space\n",
    "        self.starting_states = [\n",
    "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
    "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
    "           {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
    "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
    "        ]\n",
    "        self.starting_state_index = 0\n",
    "\n",
    "        self.computer_player = computer_player\n",
    "        self.screen = None\n",
    "        self.frame_count = 0\n",
    "        self.last_event = None\n",
    "        self.reset(seed=0)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
    "        starting_state = self.starting_states[self.starting_state_index]\n",
    "\n",
    "        server = starting_state['server']\n",
    "        positions_reversed = starting_state['positions_reversed']\n",
    "        player = starting_state['player']\n",
    "        opponent = starting_state['opponent']\n",
    "\n",
    "        self.computer_player.reset()\n",
    "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
    "        self.reward_system = RewardSystem(rewarded_player=player)\n",
    "        self.step_count = 0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        self.reward_system.reset()\n",
    "\n",
    "        model_player_actions = transform_action(action)\n",
    "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
    "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
    "        terminated = self.game.update_game_state(actions, 3, self.reward_system)\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        truncated = False\n",
    "        if self.step_count > 1000:\n",
    "            self.reward_system.end_episode(self.game.game_state['player'], self.game.game_state)\n",
    "            terminated = True\n",
    "\n",
    "        reward = self.reward_system.total_reward\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        state = self.game.game_state\n",
    "        player = state['player']\n",
    "        return get_observation(player, state)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            if pygame.get_init():\n",
    "                pygame.quit()\n",
    "            return\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
    "        if not os.path.exists('./frames'):\n",
    "            os.makedirs(\"./frames\")\n",
    "\n",
    "        # Clear screen\n",
    "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
    "        state = self.game.game_state\n",
    "        # Render paddles\n",
    "        paddle1 = state[Player.Player1]\n",
    "        paddle2 = state[Player.Player2]\n",
    "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
    "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
    "\n",
    "        # Render ball\n",
    "        ball = state['ball']\n",
    "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Save frame as image\n",
    "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
    "        pygame.image.save(self.screen, frame_path)\n",
    "        self.frame_count += 1\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if not os.path.exists('./frames'):\n",
    "            print(\"No frames directory found, skipping video creation.\")\n",
    "            return\n",
    "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
    "\n",
    "        # Create a video clip from the image sequence\n",
    "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
    "\n",
    "        # Write the video file\n",
    "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
    "        pygame.quit()\n",
    "        frames_dir = \"./frames\"\n",
    "        if os.path.exists(frames_dir):\n",
    "            for filename in os.listdir(frames_dir):\n",
    "                file_path = os.path.join(frames_dir, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            os.rmdir(frames_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mF115fJyB1jq"
   },
   "source": [
    "### Testing the Environment\n",
    "\n",
    "Create and test a single instance of the custom Pong environment. This will help ensure that the environment behaves as expected before moving on to training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R172XbXX5Am5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: (array([0.03083333, 0.57      , 0.25      , 0.25      , 0.        ,\n",
      "       0.4575    , 1.        , 1.        ], dtype=float32), {})\n",
      "Action taken: [ 0.88911664 -0.54378265]\n",
      "Observation: [0.03083333 0.57       0.25       0.         0.         0.51867557\n",
      " 0.         1.        ]\n",
      "Reward: 69.95\n",
      "iteration: 1\n",
      "Done: False\n",
      "Action taken: [ 0.7888137 -0.9907963]\n",
      "Observation: [0.05583333 0.57       0.25       0.         0.         0.6301401\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 2\n",
      "Done: False\n",
      "Action taken: [ 0.56966525 -0.16431963]\n",
      "Observation: [0.08083333 0.57       0.25       0.         0.         0.6486261\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 3\n",
      "Done: False\n",
      "Action taken: [ 0.5759172  -0.07733651]\n",
      "Observation: [0.10583334 0.57       0.25       0.         0.         0.65732646\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 4\n",
      "Done: False\n",
      "Action taken: [ 0.8929942  -0.41949427]\n",
      "Observation: [0.13083333 0.57       0.25       0.         0.         0.70451957\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 5\n",
      "Done: False\n",
      "Action taken: [ 0.11944298 -0.02869564]\n",
      "Observation: [0.15583333 0.57       0.25       0.         0.         0.7077478\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 6\n",
      "Done: False\n",
      "Action taken: [0.30876344 0.19237922]\n",
      "Observation: [0.18083334 0.57       0.25       0.         0.         0.68610513\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 7\n",
      "Done: False\n",
      "Action taken: [ 0.6562658 -0.5292166]\n",
      "Observation: [0.20583333 0.57       0.25       0.         0.         0.745642\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 8\n",
      "Done: False\n",
      "Action taken: [0.05599041 0.537097  ]\n",
      "Observation: [0.23083334 0.57       0.25       0.         0.         0.68521863\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 9\n",
      "Done: False\n",
      "Action taken: [ 0.53712904 -0.9579981 ]\n",
      "Observation: [0.25583333 0.57       0.25       0.         0.         0.775\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 10\n",
      "Done: False\n",
      "Action taken: [ 0.5319553 -0.7505384]\n",
      "Observation: [0.28083333 0.57       0.25       0.         0.         0.775\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 11\n",
      "Done: False\n",
      "Action taken: [0.9207334 0.5992778]\n",
      "Observation: [0.30583334 0.57       0.25       0.         0.         0.7075812\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 12\n",
      "Done: False\n",
      "Action taken: [0.6457979  0.13036034]\n",
      "Observation: [0.33083335 0.57       0.25       0.         0.         0.69291574\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 13\n",
      "Done: False\n",
      "Action taken: [0.6938147  0.85527855]\n",
      "Observation: [0.35583332 0.57       0.25       0.         0.         0.59669685\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 14\n",
      "Done: False\n",
      "Action taken: [0.36505514 0.8609987 ]\n",
      "Observation: [0.38083333 0.57       0.25       0.         0.         0.4998345\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 15\n",
      "Done: False\n",
      "Action taken: [0.4313175 0.6204592]\n",
      "Observation: [0.40583333 0.57       0.25       0.         0.         0.43003285\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 16\n",
      "Done: False\n",
      "Action taken: [0.39163265 0.58536375]\n",
      "Observation: [0.43083334 0.57       0.25       0.         0.         0.36417943\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 17\n",
      "Done: False\n",
      "Action taken: [ 0.13027744 -0.58079326]\n",
      "Observation: [0.45583335 0.57       0.25       0.         0.         0.42951867\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 18\n",
      "Done: False\n",
      "Action taken: [ 0.8704716 -0.9035956]\n",
      "Observation: [0.48083332 0.57       0.25       0.         0.         0.53117317\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 19\n",
      "Done: False\n",
      "Action taken: [0.5854041  0.32464468]\n",
      "Observation: [0.5058333  0.57       0.25       0.         0.         0.49465066\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 20\n",
      "Done: False\n",
      "Action taken: [ 0.8449732  -0.20526594]\n",
      "Observation: [0.5308333  0.57       0.25       0.         0.         0.51774305\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 21\n",
      "Done: False\n",
      "Action taken: [ 0.05899774 -0.89898306]\n",
      "Observation: [0.55583334 0.57       0.25       0.         0.         0.61887866\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 22\n",
      "Done: False\n",
      "Action taken: [0.5423591  0.31777525]\n",
      "Observation: [0.5808333 0.57      0.25      0.        0.        0.583129  0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 23\n",
      "Done: False\n",
      "Action taken: [ 0.41682333 -0.22120857]\n",
      "Observation: [0.60583335 0.57       0.25       0.         0.         0.60801494\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 24\n",
      "Done: False\n",
      "Action taken: [0.92241585 0.37384272]\n",
      "Observation: [0.6308333 0.57      0.25      0.        0.        0.5659576 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 25\n",
      "Done: False\n",
      "Action taken: [0.33590654 0.90526676]\n",
      "Observation: [0.6558333 0.57      0.25      0.        0.        0.4641151 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 26\n",
      "Done: False\n",
      "Action taken: [ 0.64956033 -0.4843909 ]\n",
      "Observation: [0.68083334 0.57       0.25       0.         0.         0.5186091\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 27\n",
      "Done: False\n",
      "Action taken: [ 0.3474199  -0.43298316]\n",
      "Observation: [0.7058333 0.57      0.25      0.        0.        0.5673197 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 28\n",
      "Done: False\n",
      "Action taken: [0.39412475 0.79808015]\n",
      "Observation: [0.73083335 0.57       0.25       0.         0.         0.47753567\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 29\n",
      "Done: False\n",
      "Action taken: [ 0.9051093  -0.98891133]\n",
      "Observation: [0.7558333 0.57      0.25      0.        0.        0.5887882 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 30\n",
      "Done: False\n",
      "Action taken: [0.83906597 0.4210534 ]\n",
      "Observation: [0.7808333 0.57      0.25      0.        0.        0.5414197 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 31\n",
      "Done: False\n",
      "Action taken: [0.9393265  0.48268718]\n",
      "Observation: [0.80583334 0.57       0.25       0.         0.         0.48711738\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 32\n",
      "Done: False\n",
      "Action taken: [0.5328249  0.00883434]\n",
      "Observation: [0.8308333  0.57       0.25       0.         0.         0.48612353\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 33\n",
      "Done: False\n",
      "Action taken: [ 0.98354864 -0.9198521 ]\n",
      "Observation: [0.85583335 0.57       0.25       0.         0.         0.5896069\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 34\n",
      "Done: False\n",
      "Action taken: [ 0.74488306 -0.24299803]\n",
      "Observation: [0.8808333  0.57       0.25       0.         0.         0.61694413\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 35\n",
      "Done: False\n",
      "Action taken: [0.20428872 0.9007962 ]\n",
      "Observation: [0.9058333 0.57      0.25      0.        0.        0.5156046 0.\n",
      " 1.       ]\n",
      "Reward: 0\n",
      "iteration: 36\n",
      "Done: False\n",
      "Action taken: [0.00716834 0.2749958 ]\n",
      "Observation: [0.93083334 0.57       0.25       0.         0.         0.48466757\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 37\n",
      "Done: False\n",
      "Action taken: [ 0.4259619 -0.9803174]\n",
      "Observation: [0.9558333  0.57       0.25       0.         0.         0.59495324\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 38\n",
      "Done: False\n",
      "Action taken: [0.5235632  0.45862725]\n",
      "Observation: [ 0.9691667   0.57       -0.33514443  0.28121957  0.          0.54335773\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 39\n",
      "Done: False\n",
      "Action taken: [ 0.45278344 -0.61416626]\n",
      "Observation: [ 0.9356522   0.6121829  -0.33514443  0.28121957  0.          0.61245143\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 40\n",
      "Done: False\n",
      "Action taken: [ 0.33144853 -0.61053103]\n",
      "Observation: [ 0.90213776  0.6543659  -0.33514443  0.28121957  0.          0.68113613\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 41\n",
      "Done: False\n",
      "Action taken: [0.915318   0.49732783]\n",
      "Observation: [ 0.8686233   0.6965488  -0.33514443  0.28121957  0.          0.62518674\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 42\n",
      "Done: False\n",
      "Action taken: [0.19866638 0.54025465]\n",
      "Observation: [ 0.8351089   0.73873174 -0.33514443  0.28121957  0.          0.5644081\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 43\n",
      "Done: False\n",
      "Action taken: [0.41182107 0.87652725]\n",
      "Observation: [ 0.80159444  0.78091466 -0.33514443  0.28121957  0.          0.4657988\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 44\n",
      "Done: False\n",
      "Action taken: [0.83853793 0.9929917 ]\n",
      "Observation: [ 0.76808     0.82309765 -0.33514443  0.28121957  0.          0.35408723\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 45\n",
      "Done: False\n",
      "Action taken: [0.04426664 0.83929926]\n",
      "Observation: [ 0.73456556  0.86528057 -0.33514443  0.28121957  0.          0.2596661\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 46\n",
      "Done: False\n",
      "Action taken: [ 0.83244616 -0.47456777]\n",
      "Observation: [ 0.7010511   0.9074635  -0.33514443  0.28121957  0.          0.31305495\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 47\n",
      "Done: False\n",
      "Action taken: [0.23146254 0.6240734 ]\n",
      "Observation: [ 0.6675367   0.9496464  -0.33514443  0.28121957  0.          0.24284668\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 48\n",
      "Done: False\n",
      "Action taken: [0.29668078 0.7633679 ]\n",
      "Observation: [ 0.63402224  0.985      -0.33514443 -0.28121957  0.          0.1569678\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 49\n",
      "Done: False\n",
      "Action taken: [0.95544106 0.3508976 ]\n",
      "Observation: [ 0.6005078   0.9428171  -0.33514443 -0.28121957  0.          0.11749182\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 50\n",
      "Done: False\n",
      "Action taken: [ 0.14146595 -0.82453394]\n",
      "Observation: [ 0.56699336  0.9006341  -0.33514443 -0.28121957  0.          0.21025188\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 51\n",
      "Done: False\n",
      "Action taken: [ 0.9204143  -0.51561725]\n",
      "Observation: [ 0.5334789   0.8584512  -0.33514443 -0.28121957  0.          0.26825884\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 52\n",
      "Done: False\n",
      "Action taken: [ 0.35788244 -0.18890399]\n",
      "Observation: [ 0.49996445  0.81626827 -0.33514443 -0.28121957  0.          0.28951052\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 53\n",
      "Done: False\n",
      "Action taken: [ 0.72747785 -0.9820684 ]\n",
      "Observation: [ 0.46645     0.77408534 -0.33514443 -0.28121957  0.          0.3999932\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 54\n",
      "Done: False\n",
      "Action taken: [ 0.70344186 -0.3399668 ]\n",
      "Observation: [ 0.43293557  0.73190236 -0.33514443 -0.28121957  0.          0.43823949\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 55\n",
      "Done: False\n",
      "Action taken: [0.37167668 0.73507255]\n",
      "Observation: [ 0.39942113  0.68971944 -0.33514443 -0.28121957  0.          0.35554382\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 56\n",
      "Done: False\n",
      "Action taken: [0.21286628 0.7441084 ]\n",
      "Observation: [ 0.36590666  0.6475365  -0.33514443 -0.28121957  0.          0.27183163\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 57\n",
      "Done: False\n",
      "Action taken: [0.4703878 0.5133242]\n",
      "Observation: [ 0.33239222  0.6053536  -0.33514443 -0.28121957  0.          0.21408266\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 58\n",
      "Done: False\n",
      "Action taken: [ 0.14314026 -0.6454613 ]\n",
      "Observation: [ 0.29887778  0.5631706  -0.33514443 -0.28121957  0.          0.28669706\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 59\n",
      "Done: False\n",
      "Action taken: [0.8782643 0.7401203]\n",
      "Observation: [ 0.26536334  0.5209877  -0.33514443 -0.28121957  0.          0.20343353\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 60\n",
      "Done: False\n",
      "Action taken: [ 0.86130446 -0.39544663]\n",
      "Observation: [ 0.2318489   0.47880477 -0.33514443 -0.28121957  0.          0.24792127\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 61\n",
      "Done: False\n",
      "Action taken: [0.7691134 0.8979588]\n",
      "Observation: [ 0.19833444  0.43662181 -0.33514443 -0.28121957  0.          0.1469009\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 62\n",
      "Done: False\n",
      "Action taken: [ 0.28616464 -0.2728862 ]\n",
      "Observation: [ 0.16482     0.3944389  -0.33514443 -0.28121957  0.          0.1776006\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 63\n",
      "Done: False\n",
      "Action taken: [ 0.3434922  -0.07349569]\n",
      "Observation: [ 0.13130556  0.35225594 -0.33514443 -0.28121957  0.          0.18586887\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 64\n",
      "Done: False\n",
      "Action taken: [ 0.38273436 -0.40525755]\n",
      "Observation: [ 0.09779111  0.31007302 -0.33514443 -0.28121957  0.          0.23146035\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 65\n",
      "Done: False\n",
      "Action taken: [0.5203469  0.45402917]\n",
      "Observation: [ 0.06427667  0.26789007 -0.33514443 -0.28121957  0.          0.18038206\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 66\n",
      "Done: False\n",
      "Action taken: [0.83242035 0.94182026]\n",
      "Observation: [ 0.03083333  0.22570713  0.31232846 -0.1582475   0.          0.07442728\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 67\n",
      "Done: False\n",
      "Action taken: [ 0.40698358 -0.3885622 ]\n",
      "Observation: [ 0.06206618  0.20197001  0.31232846 -0.1582475   0.          0.11814053\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 68\n",
      "Done: False\n",
      "Action taken: [0.7623828 0.5705569]\n",
      "Observation: [ 0.09329903  0.17823288  0.31232846 -0.1582475   0.          0.05395288\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 69\n",
      "Done: False\n",
      "Action taken: [0.09354286 0.6909479 ]\n",
      "Observation: [ 0.12453187  0.15449576  0.31232846 -0.1582475   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 70\n",
      "Done: False\n",
      "Action taken: [ 0.8186853  -0.21458492]\n",
      "Observation: [ 0.15576473  0.13075863  0.31232846 -0.1582475   0.          0.0241408\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 71\n",
      "Done: False\n",
      "Action taken: [ 0.81615674 -0.8021156 ]\n",
      "Observation: [ 0.18699756  0.10702151  0.31232846 -0.1582475   0.          0.11437881\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 72\n",
      "Done: False\n",
      "Action taken: [ 0.4700589 -0.8571286]\n",
      "Observation: [ 0.21823041  0.08328439  0.31232846 -0.1582475   0.          0.21080577\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 73\n",
      "Done: False\n",
      "Action taken: [ 0.19122113 -0.07893845]\n",
      "Observation: [ 0.24946326  0.05954726  0.31232846 -0.1582475   0.          0.21968636\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 74\n",
      "Done: False\n",
      "Action taken: [ 0.60984445 -0.33856153]\n",
      "Observation: [ 0.28069612  0.03581014  0.31232846 -0.1582475   0.          0.25777453\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 75\n",
      "Done: False\n",
      "Action taken: [ 0.31398773 -0.8928656 ]\n",
      "Observation: [0.31192896 0.015      0.31232846 0.1582475  0.         0.35822192\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 76\n",
      "Done: False\n",
      "Action taken: [ 0.45595905 -0.12993142]\n",
      "Observation: [0.3431618  0.03873713 0.31232846 0.1582475  0.         0.37283918\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 77\n",
      "Done: False\n",
      "Action taken: [ 0.74280494 -0.53728044]\n",
      "Observation: [0.37439466 0.06247425 0.31232846 0.1582475  0.         0.43328324\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 78\n",
      "Done: False\n",
      "Action taken: [ 0.21170545 -0.72361904]\n",
      "Observation: [0.4056275  0.08621138 0.31232846 0.1582475  0.         0.5146904\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 79\n",
      "Done: False\n",
      "Action taken: [0.84267956 0.44962764]\n",
      "Observation: [0.43686035 0.1099485  0.31232846 0.1582475  0.         0.46410728\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 80\n",
      "Done: False\n",
      "Action taken: [0.8536331 0.470185 ]\n",
      "Observation: [0.4680932  0.13368562 0.31232846 0.1582475  0.         0.41121146\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 81\n",
      "Done: False\n",
      "Action taken: [ 0.0908321 -0.3268914]\n",
      "Observation: [0.49932605 0.15742275 0.31232846 0.1582475  0.         0.44798675\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 82\n",
      "Done: False\n",
      "Action taken: [ 0.5940796  -0.75318384]\n",
      "Observation: [0.5305589  0.18115987 0.31232846 0.1582475  0.         0.5327199\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 83\n",
      "Done: False\n",
      "Action taken: [0.91419363 0.98172975]\n",
      "Observation: [0.5617917  0.204897   0.31232846 0.1582475  0.         0.42227533\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 84\n",
      "Done: False\n",
      "Action taken: [0.6175209 0.7034788]\n",
      "Observation: [0.5930246  0.22863412 0.31232846 0.1582475  0.         0.34313396\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 85\n",
      "Done: False\n",
      "Action taken: [0.5627492 0.788715 ]\n",
      "Observation: [0.62425745 0.25237125 0.31232846 0.1582475  0.         0.25440353\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 86\n",
      "Done: False\n",
      "Action taken: [ 0.59759367 -0.81374896]\n",
      "Observation: [0.6554903  0.27610838 0.31232846 0.1582475  0.         0.34595028\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 87\n",
      "Done: False\n",
      "Action taken: [0.7217669  0.16935882]\n",
      "Observation: [0.6867231  0.2998455  0.31232846 0.1582475  0.         0.3268974\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 88\n",
      "Done: False\n",
      "Action taken: [0.8293748 0.6523501]\n",
      "Observation: [0.71795595 0.32358262 0.31232846 0.1582475  0.         0.25350803\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 89\n",
      "Done: False\n",
      "Action taken: [0.02479121 0.46719956]\n",
      "Observation: [0.74918884 0.34731975 0.31232846 0.1582475  0.         0.20094807\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 90\n",
      "Done: False\n",
      "Action taken: [ 0.8974701  -0.24490297]\n",
      "Observation: [0.7804217  0.37105688 0.31232846 0.1582475  0.         0.22849965\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 91\n",
      "Done: False\n",
      "Action taken: [0.70794004 0.65959424]\n",
      "Observation: [0.8116545  0.39479402 0.31232846 0.1582475  0.         0.15429531\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 92\n",
      "Done: False\n",
      "Action taken: [ 0.23498292 -0.38059255]\n",
      "Observation: [0.84288734 0.41853112 0.31232846 0.1582475  0.         0.19711196\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 93\n",
      "Done: False\n",
      "Action taken: [ 0.11979702 -0.18244947]\n",
      "Observation: [0.87412024 0.44226825 0.31232846 0.1582475  0.         0.21763754\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 94\n",
      "Done: False\n",
      "Action taken: [0.31730494 0.56093717]\n",
      "Observation: [0.90535307 0.46600538 0.31232846 0.1582475  0.         0.1545321\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 95\n",
      "Done: False\n",
      "Action taken: [ 0.36822954 -0.6452473 ]\n",
      "Observation: [0.9365859  0.48974252 0.31232846 0.1582475  0.         0.22712243\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 96\n",
      "Done: False\n",
      "Action taken: [0.02386547 0.1177692 ]\n",
      "Observation: [0.96781874 0.51347965 0.31232846 0.1582475  0.         0.21387339\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 97\n",
      "Done: False\n",
      "Action taken: [0.02498819 0.57371235]\n",
      "Observation: [ 0.9691667   0.5372168  -0.45304543 -0.11535201  0.          0.14933075\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 98\n",
      "Done: False\n",
      "Action taken: [ 0.39984527 -0.70998394]\n",
      "Observation: [ 0.9238621   0.519914   -0.45304543 -0.11535201  0.          0.22920394\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 99\n",
      "Done: False\n",
      "Action taken: [ 0.75234413 -0.13743326]\n",
      "Observation: [ 0.87855756  0.50261116 -0.45304543 -0.11535201  0.          0.24466519\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 100\n",
      "Done: False\n",
      "Action taken: [ 0.12829772 -0.7577827 ]\n",
      "Observation: [ 0.833253    0.48530835 -0.45304543 -0.11535201  0.          0.32991573\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 101\n",
      "Done: False\n",
      "Action taken: [0.7796125  0.02732026]\n",
      "Observation: [ 0.7879485   0.46800554 -0.45304543 -0.11535201  0.          0.32684222\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 102\n",
      "Done: False\n",
      "Action taken: [0.08626913 0.01566301]\n",
      "Observation: [ 0.74264395  0.45070276 -0.45304543 -0.11535201  0.          0.32508013\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 103\n",
      "Done: False\n",
      "Action taken: [0.48611948 0.73268116]\n",
      "Observation: [ 0.6973394   0.43339995 -0.45304543 -0.11535201  0.          0.24265349\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 104\n",
      "Done: False\n",
      "Action taken: [0.68801284 0.8371973 ]\n",
      "Observation: [ 0.6520349   0.41609713 -0.45304543 -0.11535201  0.          0.14846879\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 105\n",
      "Done: False\n",
      "Action taken: [0.2210866 0.3716969]\n",
      "Observation: [ 0.60673034  0.39879432 -0.45304543 -0.11535201  0.          0.10665289\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 106\n",
      "Done: False\n",
      "Action taken: [ 0.7174033  -0.48229223]\n",
      "Observation: [ 0.5614258   0.38149154 -0.45304543 -0.11535201  0.          0.16091077\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 107\n",
      "Done: False\n",
      "Action taken: [ 0.19052902 -0.4942363 ]\n",
      "Observation: [ 0.5161212   0.36418873 -0.45304543 -0.11535201  0.          0.21651235\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 108\n",
      "Done: False\n",
      "Action taken: [0.7343132 0.6498045]\n",
      "Observation: [ 0.4708167   0.34688592 -0.45304543 -0.11535201  0.          0.14340936\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 109\n",
      "Done: False\n",
      "Action taken: [ 0.94324046 -0.0542236 ]\n",
      "Observation: [ 0.42551214  0.32958314 -0.45304543 -0.11535201  0.          0.1495095\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 110\n",
      "Done: False\n",
      "Action taken: [ 0.40433484 -0.5175964 ]\n",
      "Observation: [ 0.3802076   0.31228033 -0.45304543 -0.11535201  0.          0.2077391\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 111\n",
      "Done: False\n",
      "Action taken: [0.9741272  0.06049909]\n",
      "Observation: [ 0.33490306  0.29497752 -0.45304543 -0.11535201  0.          0.20093295\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 112\n",
      "Done: False\n",
      "Action taken: [0.11630667 0.9849235 ]\n",
      "Observation: [ 0.28959852  0.27767473 -0.45304543 -0.11535201  0.          0.09012906\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 113\n",
      "Done: False\n",
      "Action taken: [0.3975471  0.94368106]\n",
      "Observation: [ 0.24429397  0.26037192 -0.45304543 -0.11535201  0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 114\n",
      "Done: False\n",
      "Action taken: [ 0.92516834 -0.3897614 ]\n",
      "Observation: [ 0.19898944  0.24306913 -0.45304543 -0.11535201  0.          0.04384816\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 115\n",
      "Done: False\n",
      "Action taken: [ 0.9100034  -0.41966647]\n",
      "Observation: [ 0.15368488  0.22576632 -0.45304543 -0.11535201  0.          0.09106063\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 116\n",
      "Done: False\n",
      "Action taken: [0.01439297 0.8738789 ]\n",
      "Observation: [ 0.10838035  0.20846352 -0.45304543 -0.11535201  0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 117\n",
      "Done: False\n",
      "Action taken: [0.7546161  0.15383483]\n",
      "Observation: [ 0.0630758   0.19116071 -0.45304543 -0.11535201  0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 118\n",
      "Done: False\n",
      "Action taken: [0.8168386  0.51146245]\n",
      "Observation: [0.03083333 0.17385791 0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 119\n",
      "Done: False\n",
      "Action taken: [ 0.5266395  -0.86656284]\n",
      "Observation: [0.06029176 0.1940356  0.29458427 0.13451791 0.         0.09748832\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 120\n",
      "Done: False\n",
      "Action taken: [ 0.30613533 -0.67588234]\n",
      "Observation: [0.08975019 0.21421328 0.29458427 0.13451791 0.         0.17352508\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 121\n",
      "Done: False\n",
      "Action taken: [0.8733058  0.33528405]\n",
      "Observation: [0.11920861 0.23439097 0.29458427 0.13451791 0.         0.13580562\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 122\n",
      "Done: False\n",
      "Action taken: [0.9454103 0.8472109]\n",
      "Observation: [0.14866704 0.25456867 0.29458427 0.13451791 0.         0.0404944\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 123\n",
      "Done: False\n",
      "Action taken: [0.83943206 0.3147432 ]\n",
      "Observation: [0.17812547 0.27474633 0.29458427 0.13451791 0.         0.00508579\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 124\n",
      "Done: False\n",
      "Action taken: [ 0.5090292  -0.10482454]\n",
      "Observation: [0.20758389 0.29492402 0.29458427 0.13451791 0.         0.01687855\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 125\n",
      "Done: False\n",
      "Action taken: [0.64770836 0.8965963 ]\n",
      "Observation: [0.23704232 0.3151017  0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 126\n",
      "Done: False\n",
      "Action taken: [0.44922134 0.28171512]\n",
      "Observation: [0.26650074 0.3352794  0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 127\n",
      "Done: False\n",
      "Action taken: [ 0.77111363 -0.5293278 ]\n",
      "Observation: [0.29595917 0.35545707 0.29458427 0.13451791 0.         0.05954938\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 128\n",
      "Done: False\n",
      "Action taken: [ 0.6949496  -0.02422317]\n",
      "Observation: [0.3254176  0.37563476 0.29458427 0.13451791 0.         0.06227449\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 129\n",
      "Done: False\n",
      "Action taken: [0.8173419  0.20049839]\n",
      "Observation: [0.35487604 0.39581245 0.29458427 0.13451791 0.         0.03971842\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 130\n",
      "Done: False\n",
      "Action taken: [0.8072776 0.4456945]\n",
      "Observation: [0.38433444 0.41599014 0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 131\n",
      "Done: False\n",
      "Action taken: [0.762974  0.0927908]\n",
      "Observation: [0.41379288 0.4361678  0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 132\n",
      "Done: False\n",
      "Action taken: [ 0.5743911  -0.43515256]\n",
      "Observation: [0.4432513  0.4563455  0.29458427 0.13451791 0.         0.04895466\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 133\n",
      "Done: False\n",
      "Action taken: [0.39564744 0.454363  ]\n",
      "Observation: [0.47270975 0.4765232  0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 134\n",
      "Done: False\n",
      "Action taken: [0.5888637 0.9746938]\n",
      "Observation: [0.5021682  0.49670088 0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 135\n",
      "Done: False\n",
      "Action taken: [ 0.68914735 -0.9006007 ]\n",
      "Observation: [0.5316266  0.51687855 0.29458427 0.13451791 0.         0.10131758\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 136\n",
      "Done: False\n",
      "Action taken: [0.31093764 0.55754244]\n",
      "Observation: [0.56108505 0.53705627 0.29458427 0.13451791 0.         0.03859405\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 137\n",
      "Done: False\n",
      "Action taken: [ 0.52938956 -0.6414002 ]\n",
      "Observation: [0.59054345 0.5572339  0.29458427 0.13451791 0.         0.11075158\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 138\n",
      "Done: False\n",
      "Action taken: [0.81944984 0.17975868]\n",
      "Observation: [0.62000185 0.5774116  0.29458427 0.13451791 0.         0.09052873\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 139\n",
      "Done: False\n",
      "Action taken: [0.12959307 0.8210048 ]\n",
      "Observation: [0.6494603  0.5975893  0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 140\n",
      "Done: False\n",
      "Action taken: [ 0.98896843 -0.90598077]\n",
      "Observation: [0.6789187  0.617767   0.29458427 0.13451791 0.         0.10192284\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 141\n",
      "Done: False\n",
      "Action taken: [0.172236  0.3679679]\n",
      "Observation: [0.7083771  0.6379447  0.29458427 0.13451791 0.         0.06052645\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 142\n",
      "Done: False\n",
      "Action taken: [0.8384445 0.7871463]\n",
      "Observation: [0.7378356  0.65812236 0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 143\n",
      "Done: False\n",
      "Action taken: [ 0.48775157 -0.598328  ]\n",
      "Observation: [0.767294   0.6783     0.29458427 0.13451791 0.         0.0673119\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 144\n",
      "Done: False\n",
      "Action taken: [ 0.7521563 -0.4319647]\n",
      "Observation: [0.79675245 0.69847775 0.29458427 0.13451791 0.         0.11590793\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 145\n",
      "Done: False\n",
      "Action taken: [ 0.65702343 -0.3402199 ]\n",
      "Observation: [0.82621086 0.7186554  0.29458427 0.13451791 0.         0.15418266\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 146\n",
      "Done: False\n",
      "Action taken: [0.5086938  0.39876717]\n",
      "Observation: [0.85566926 0.73883307 0.29458427 0.13451791 0.         0.10932136\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 147\n",
      "Done: False\n",
      "Action taken: [0.6489865  0.79461485]\n",
      "Observation: [0.8851277  0.7590108  0.29458427 0.13451791 0.         0.01992719\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 148\n",
      "Done: False\n",
      "Action taken: [0.17164733 0.8679775 ]\n",
      "Observation: [0.9145861  0.77918845 0.29458427 0.13451791 0.         0.\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 149\n",
      "Done: False\n",
      "Action taken: [ 0.9193274  -0.88177925]\n",
      "Observation: [0.9440446  0.7993662  0.29458427 0.13451791 0.         0.09920017\n",
      " 0.         1.        ]\n",
      "Reward: 0\n",
      "iteration: 150\n",
      "Done: False\n",
      "Action taken: [0.3783487 0.8704627]\n",
      "Observation: [ 0.9691667   0.81954384 -0.22908959  0.3332619   0.          0.00127311\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 151\n",
      "Done: False\n",
      "Action taken: [0.47125533 0.37728757]\n",
      "Observation: [ 0.9462577   0.8695331  -0.22908959  0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 152\n",
      "Done: False\n",
      "Action taken: [0.8097676  0.08584628]\n",
      "Observation: [ 0.9233487   0.9195224  -0.22908959  0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 153\n",
      "Done: False\n",
      "Action taken: [ 0.11733162 -0.8075947 ]\n",
      "Observation: [ 0.9004398   0.9695117  -0.22908959  0.3332619   0.          0.09085441\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 154\n",
      "Done: False\n",
      "Action taken: [0.7529279  0.46458668]\n",
      "Observation: [ 0.8775308   0.985      -0.22908959 -0.3332619   0.          0.0385884\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 155\n",
      "Done: False\n",
      "Action taken: [ 0.44037738 -0.48186806]\n",
      "Observation: [ 0.8546219   0.93501073 -0.22908959 -0.3332619   0.          0.09279856\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 156\n",
      "Done: False\n",
      "Action taken: [0.72434795 0.19299625]\n",
      "Observation: [ 0.8317129   0.88502145 -0.22908959 -0.3332619   0.          0.07108648\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 157\n",
      "Done: False\n",
      "Action taken: [0.6344528  0.37021378]\n",
      "Observation: [ 0.808804    0.83503217 -0.22908959 -0.3332619   0.          0.02943743\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 158\n",
      "Done: False\n",
      "Action taken: [0.8103805  0.25461978]\n",
      "Observation: [ 7.8589499e-01  7.8504288e-01 -2.2908959e-01 -3.3326191e-01\n",
      "  0.0000000e+00  7.9270842e-04  0.0000000e+00  1.0000000e+00]\n",
      "Reward: 0\n",
      "iteration: 159\n",
      "Done: False\n",
      "Action taken: [ 0.46253777 -0.85577637]\n",
      "Observation: [ 0.762986    0.73505354 -0.22908959 -0.3332619   0.          0.09706755\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 160\n",
      "Done: False\n",
      "Action taken: [ 0.86656064 -0.5208862 ]\n",
      "Observation: [ 0.7400771   0.68506426 -0.22908959 -0.3332619   0.          0.15566725\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 161\n",
      "Done: False\n",
      "Action taken: [0.14389038 0.7931948 ]\n",
      "Observation: [ 0.7171681   0.635075   -0.22908959 -0.3332619   0.          0.06643283\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 162\n",
      "Done: False\n",
      "Action taken: [ 0.16892453 -0.5200266 ]\n",
      "Observation: [ 0.69425917  0.5850857  -0.22908959 -0.3332619   0.          0.12493583\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 163\n",
      "Done: False\n",
      "Action taken: [ 0.893682   -0.75868684]\n",
      "Observation: [ 0.6713502   0.5350964  -0.22908959 -0.3332619   0.          0.21028809\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 164\n",
      "Done: False\n",
      "Action taken: [0.64591295 0.92771727]\n",
      "Observation: [ 0.64844126  0.48510712 -0.22908959 -0.3332619   0.          0.10591991\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 165\n",
      "Done: False\n",
      "Action taken: [ 0.1820843  -0.42759553]\n",
      "Observation: [ 0.62553227  0.43511784 -0.22908959 -0.3332619   0.          0.1540244\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 166\n",
      "Done: False\n",
      "Action taken: [ 0.6730351  -0.34097132]\n",
      "Observation: [ 0.60262334  0.38512856 -0.22908959 -0.3332619   0.          0.19238368\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 167\n",
      "Done: False\n",
      "Action taken: [0.9977324 0.7978917]\n",
      "Observation: [ 0.57971436  0.33513927 -0.22908959 -0.3332619   0.          0.10262086\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 168\n",
      "Done: False\n",
      "Action taken: [0.13884851 0.96454996]\n",
      "Observation: [ 0.5568054   0.28515    -0.22908959 -0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 169\n",
      "Done: False\n",
      "Action taken: [0.63402295 0.3696213 ]\n",
      "Observation: [ 0.53389645  0.2351607  -0.22908959 -0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 170\n",
      "Done: False\n",
      "Action taken: [ 0.17640147 -0.31992185]\n",
      "Observation: [ 0.51098746  0.18517141 -0.22908959 -0.3332619   0.          0.03599121\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 171\n",
      "Done: False\n",
      "Action taken: [0.5907488  0.35516536]\n",
      "Observation: [ 0.48807853  0.13518213 -0.22908959 -0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 172\n",
      "Done: False\n",
      "Action taken: [ 0.85955834 -0.04418738]\n",
      "Observation: [ 0.46516955  0.08519284 -0.22908959 -0.3332619   0.          0.00497108\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 173\n",
      "Done: False\n",
      "Action taken: [ 0.72073126 -0.30029383]\n",
      "Observation: [ 0.4422606   0.03520355 -0.22908959 -0.3332619   0.          0.03875414\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 174\n",
      "Done: False\n",
      "Action taken: [0.7559943 0.7662028]\n",
      "Observation: [ 0.41935164  0.015      -0.22908959  0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 175\n",
      "Done: False\n",
      "Action taken: [0.06710172 0.56394064]\n",
      "Observation: [ 0.39644268  0.06498928 -0.22908959  0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 176\n",
      "Done: False\n",
      "Action taken: [0.3481432 0.0010775]\n",
      "Observation: [ 0.37353373  0.11497857 -0.22908959  0.3332619   0.          0.\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 177\n",
      "Done: False\n",
      "Action taken: [ 0.35587758 -0.91577876]\n",
      "Observation: [ 0.35062477  0.16496786 -0.22908959  0.3332619   0.          0.10302511\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 178\n",
      "Done: False\n",
      "Action taken: [ 0.6550587  -0.49106693]\n",
      "Observation: [ 0.3277158   0.21495715 -0.22908959  0.3332619   0.          0.15827014\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 179\n",
      "Done: False\n",
      "Action taken: [0.51843935 0.20836747]\n",
      "Observation: [ 0.30480686  0.26494643 -0.22908959  0.3332619   0.          0.1348288\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 180\n",
      "Done: False\n",
      "Action taken: [ 0.47939318 -0.07897685]\n",
      "Observation: [ 0.2818979   0.3149357  -0.22908959  0.3332619   0.          0.1437137\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 181\n",
      "Done: False\n",
      "Action taken: [ 0.6880736  -0.92582154]\n",
      "Observation: [ 0.25898892  0.364925   -0.22908959  0.3332619   0.          0.24786861\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 182\n",
      "Done: False\n",
      "Action taken: [0.38845062 0.4448166 ]\n",
      "Observation: [ 0.23607998  0.41491428 -0.22908959  0.3332619   0.          0.19782676\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 183\n",
      "Done: False\n",
      "Action taken: [0.7332885 0.8447245]\n",
      "Observation: [ 0.213171    0.4649036  -0.22908959  0.3332619   0.          0.10279525\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 184\n",
      "Done: False\n",
      "Action taken: [ 0.17523167 -0.9326493 ]\n",
      "Observation: [ 0.19026205  0.5148929  -0.22908959  0.3332619   0.          0.2077183\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 185\n",
      "Done: False\n",
      "Action taken: [ 0.0036712 -0.6000547]\n",
      "Observation: [ 0.1673531   0.56488216 -0.22908959  0.3332619   0.          0.27522445\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 186\n",
      "Done: False\n",
      "Action taken: [ 0.45391583 -0.63081586]\n",
      "Observation: [ 0.14444414  0.61487144 -0.22908959  0.3332619   0.          0.34619123\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 187\n",
      "Done: False\n",
      "Action taken: [ 0.928321  -0.6935789]\n",
      "Observation: [ 0.12153517  0.6648607  -0.22908959  0.3332619   0.          0.42421886\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 188\n",
      "Done: False\n",
      "Action taken: [ 0.0253278  -0.45750374]\n",
      "Observation: [ 0.09862621  0.71485    -0.22908959  0.3332619   0.          0.47568804\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 189\n",
      "Done: False\n",
      "Action taken: [0.6620037  0.53612465]\n",
      "Observation: [ 0.07571726  0.7648393  -0.22908959  0.3332619   0.          0.415374\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 190\n",
      "Done: False\n",
      "Action taken: [0.00758861 0.53130144]\n",
      "Observation: [ 0.0528083   0.8148286  -0.22908959  0.3332619   0.          0.3556026\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 191\n",
      "Done: False\n",
      "Action taken: [0.9610351 0.9401389]\n",
      "Observation: [ 0.02989934  0.86481786 -0.22908959  0.3332619   0.          0.24983697\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 192\n",
      "Done: False\n",
      "Action taken: [ 0.9358147 -0.5599671]\n",
      "Observation: [ 0.00699038  0.91480714 -0.22908959  0.3332619   0.          0.31283328\n",
      "  0.          1.        ]\n",
      "Reward: -43.9970191043205\n",
      "iteration: 193\n",
      "Done: False\n",
      "Action taken: [ 0.8981072 -0.166863 ]\n",
      "Observation: [ 0.00699038  0.91480714 -0.22908959  0.3332619   0.          0.31283328\n",
      "  0.          1.        ]\n",
      "Reward: 0\n",
      "iteration: 194\n",
      "Done: True\n",
      "Environment reset\n",
      "Moviepy - Building video ./game_video.mp4.\n",
      "Moviepy - Writing video ./game_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./game_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Create and test single environment\n",
    "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "i = 0\n",
    "while True:\n",
    "    i+=1\n",
    "    action = env.action_space.sample()  # Sample random action\n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    print(\"Action taken:\", action)\n",
    "    print(\"Observation:\", obs)\n",
    "    print(\"Reward:\", reward)\n",
    "    print('iteration:', i)\n",
    "    print(\"Done:\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        print(\"Environment reset\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO6NXae9B39b"
   },
   "source": [
    "### Creating and Testing Vectorized Environment\n",
    "\n",
    "Create and test a vectorized environment to allow for more efficient training by running multiple instances of the game in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4LjxARS9zCF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [[0.03083333 0.71625    0.25       0.25       0.         0.60375\n",
      "  1.         1.        ]]\n",
      "Action taken: [0.03171353 0.20344384]\n",
      "Observation: [[0.03083333 0.71625    0.25       0.         0.         0.5808626\n",
      "  1.         1.        ]]\n",
      "Reward: [-4.993763]\n",
      "iteration: 1\n",
      "Done: [False]\n",
      "Action taken: [ 0.8686842  -0.99376386]\n",
      "Observation: [[ 0.03083333  0.7116725   0.25       -0.03051658  0.          0.692661\n",
      "   0.          1.        ]]\n",
      "Reward: [-2.0000327]\n",
      "iteration: 2\n",
      "Done: [False]\n",
      "Action taken: [0.08936843 0.29426393]\n",
      "Observation: [[ 0.05583333  0.707095    0.25       -0.03051658  0.          0.6595563\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 3\n",
      "Done: [False]\n",
      "Action taken: [0.67478675 0.4008985 ]\n",
      "Observation: [[ 0.08083333  0.70251757  0.25       -0.03051658  0.          0.6144552\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 4\n",
      "Done: [False]\n",
      "Action taken: [ 0.13338178 -0.08049645]\n",
      "Observation: [[ 0.10583334  0.69794005  0.25       -0.03051658  0.          0.6235111\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 5\n",
      "Done: [False]\n",
      "Action taken: [0.4393804  0.23603755]\n",
      "Observation: [[ 0.13083333  0.6933626   0.25       -0.03051658  0.          0.59695685\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 6\n",
      "Done: [False]\n",
      "Action taken: [0.9318357  0.37510908]\n",
      "Observation: [[ 0.15583333  0.6887851   0.25       -0.03051658  0.          0.55475706\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 7\n",
      "Done: [False]\n",
      "Action taken: [0.5189332 0.3409998]\n",
      "Observation: [[ 0.18083334  0.6842076   0.25       -0.03051658  0.          0.5163946\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 8\n",
      "Done: [False]\n",
      "Action taken: [0.12444615 0.1774962 ]\n",
      "Observation: [[ 0.20583333  0.6796301   0.25       -0.03051658  0.          0.49642628\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 9\n",
      "Done: [False]\n",
      "Action taken: [0.5571871 0.5320427]\n",
      "Observation: [[ 0.23083334  0.67505264  0.25       -0.03051658  0.          0.43657148\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 10\n",
      "Done: [False]\n",
      "Action taken: [0.9892126 0.8808407]\n",
      "Observation: [[ 0.25583333  0.6704751   0.25       -0.03051658  0.          0.3374769\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 11\n",
      "Done: [False]\n",
      "Action taken: [0.8514718  0.04654065]\n",
      "Observation: [[ 0.28083333  0.66589767  0.25       -0.03051658  0.          0.3322411\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 12\n",
      "Done: [False]\n",
      "Action taken: [ 0.7785353 -0.3153285]\n",
      "Observation: [[ 0.30583334  0.66132015  0.25       -0.03051658  0.          0.36771554\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 13\n",
      "Done: [False]\n",
      "Action taken: [0.18348911 0.76888335]\n",
      "Observation: [[ 0.33083335  0.6567427   0.25       -0.03051658  0.          0.28121617\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 14\n",
      "Done: [False]\n",
      "Action taken: [ 0.9820544 -0.6368848]\n",
      "Observation: [[ 0.35583332  0.6521652   0.25       -0.03051658  0.          0.3528657\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 15\n",
      "Done: [False]\n",
      "Action taken: [ 0.67365193 -0.5010855 ]\n",
      "Observation: [[ 0.38083333  0.6475877   0.25       -0.03051658  0.          0.40923783\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 16\n",
      "Done: [False]\n",
      "Action taken: [0.5450473  0.72899884]\n",
      "Observation: [[ 0.40583333  0.6430102   0.25       -0.03051658  0.          0.32722545\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 17\n",
      "Done: [False]\n",
      "Action taken: [0.4871345  0.78606606]\n",
      "Observation: [[ 0.43083334  0.63843274  0.25       -0.03051658  0.          0.23879302\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 18\n",
      "Done: [False]\n",
      "Action taken: [0.42178327 0.39322963]\n",
      "Observation: [[ 0.45583335  0.6338552   0.25       -0.03051658  0.          0.19455469\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 19\n",
      "Done: [False]\n",
      "Action taken: [ 0.5264772  -0.61280096]\n",
      "Observation: [[ 0.48083332  0.62927777  0.25       -0.03051658  0.          0.2634948\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 20\n",
      "Done: [False]\n",
      "Action taken: [0.5449677  0.94275916]\n",
      "Observation: [[ 0.5058333   0.62470025  0.25       -0.03051658  0.          0.15743439\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 21\n",
      "Done: [False]\n",
      "Action taken: [ 0.47387773 -0.40914923]\n",
      "Observation: [[ 0.5308333   0.6201228   0.25       -0.03051658  0.          0.20346367\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 22\n",
      "Done: [False]\n",
      "Action taken: [ 0.8817051  -0.89512366]\n",
      "Observation: [[ 0.55583334  0.6155453   0.25       -0.03051658  0.          0.3041651\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 23\n",
      "Done: [False]\n",
      "Action taken: [ 0.46231973 -0.5612282 ]\n",
      "Observation: [[ 0.5808333   0.6109678   0.25       -0.03051658  0.          0.36730325\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 24\n",
      "Done: [False]\n",
      "Action taken: [ 0.41860422 -0.6493403 ]\n",
      "Observation: [[ 0.60583335  0.6063903   0.25       -0.03051658  0.          0.44035405\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 25\n",
      "Done: [False]\n",
      "Action taken: [ 0.7699306  -0.73216826]\n",
      "Observation: [[ 0.6308333   0.60181284  0.25       -0.03051658  0.          0.52272296\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 26\n",
      "Done: [False]\n",
      "Action taken: [ 0.5772278  -0.30001286]\n",
      "Observation: [[ 0.6558333   0.5972354   0.25       -0.03051658  0.          0.55647445\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 27\n",
      "Done: [False]\n",
      "Action taken: [0.5746747 0.2713797]\n",
      "Observation: [[ 0.68083334  0.59265786  0.25       -0.03051658  0.          0.52594423\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 28\n",
      "Done: [False]\n",
      "Action taken: [ 0.21591692 -0.15310684]\n",
      "Observation: [[ 0.7058333   0.5880804   0.25       -0.03051658  0.          0.5431687\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 29\n",
      "Done: [False]\n",
      "Action taken: [ 0.81745   -0.6480431]\n",
      "Observation: [[ 0.73083335  0.5835029   0.25       -0.03051658  0.          0.61607355\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 30\n",
      "Done: [False]\n",
      "Action taken: [ 0.52115554 -0.70912766]\n",
      "Observation: [[ 0.7558333   0.57892543  0.25       -0.03051658  0.          0.69585043\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 31\n",
      "Done: [False]\n",
      "Action taken: [0.19700304 0.3773332 ]\n",
      "Observation: [[ 0.7808333   0.5743479   0.25       -0.03051658  0.          0.6534004\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 32\n",
      "Done: [False]\n",
      "Action taken: [ 0.1292892 -0.7723938]\n",
      "Observation: [[ 0.80583334  0.56977046  0.25       -0.03051658  0.          0.74029475\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 33\n",
      "Done: [False]\n",
      "Action taken: [ 0.95967036 -0.8940905 ]\n",
      "Observation: [[ 0.8308333   0.56519294  0.25       -0.03051658  0.          0.775\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 34\n",
      "Done: [False]\n",
      "Action taken: [0.89550984 0.47007474]\n",
      "Observation: [[ 0.85583335  0.5606155   0.25       -0.03051658  0.          0.7221166\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 35\n",
      "Done: [False]\n",
      "Action taken: [0.62567776 0.9597852 ]\n",
      "Observation: [[ 0.8808333   0.55603796  0.25       -0.03051658  0.          0.61414075\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 36\n",
      "Done: [False]\n",
      "Action taken: [0.63325924 0.6382018 ]\n",
      "Observation: [[ 0.9058333   0.5514605   0.25       -0.03051658  0.          0.5423431\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 37\n",
      "Done: [False]\n",
      "Action taken: [0.91024715 0.0577768 ]\n",
      "Observation: [[ 0.93083334  0.546883    0.25       -0.03051658  0.          0.5358432\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 38\n",
      "Done: [False]\n",
      "Action taken: [0.9544491  0.16092944]\n",
      "Observation: [[ 0.9558333   0.5423055   0.25       -0.03051658  0.          0.5177386\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 39\n",
      "Done: [False]\n",
      "Action taken: [ 0.21073863 -0.14136864]\n",
      "Observation: [[ 0.98083335  0.537728    0.25       -0.03051658  0.          0.5336426\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 40\n",
      "Done: [False]\n",
      "Action taken: [0.10547777 0.8529071 ]\n",
      "Observation: [[ 0.9691667   0.53315055 -0.3960459  -0.16522972  0.          0.43769053\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 41\n",
      "Done: [False]\n",
      "Action taken: [0.944267   0.16822939]\n",
      "Observation: [[ 0.9295621   0.5083661  -0.3960459  -0.16522972  0.          0.4187647\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 42\n",
      "Done: [False]\n",
      "Action taken: [0.7547726 0.9520615]\n",
      "Observation: [[ 0.8899575   0.48358163 -0.3960459  -0.16522972  0.          0.31165782\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 43\n",
      "Done: [False]\n",
      "Action taken: [ 0.43840387 -0.24167086]\n",
      "Observation: [[ 0.8503529   0.45879716 -0.3960459  -0.16522972  0.          0.33884576\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 44\n",
      "Done: [False]\n",
      "Action taken: [ 0.6599681  -0.95438766]\n",
      "Observation: [[ 0.81074834  0.4340127  -0.3960459  -0.16522972  0.          0.44621438\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 45\n",
      "Done: [False]\n",
      "Action taken: [ 0.49627236 -0.89176726]\n",
      "Observation: [[ 0.77114373  0.40922824 -0.3960459  -0.16522972  0.          0.5465382\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 46\n",
      "Done: [False]\n",
      "Action taken: [ 0.4290676  -0.02621105]\n",
      "Observation: [[ 0.73153913  0.3844438  -0.3960459  -0.16522972  0.          0.54948694\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 47\n",
      "Done: [False]\n",
      "Action taken: [ 0.70066404 -0.90972054]\n",
      "Observation: [[ 0.6919345   0.35965934 -0.3960459  -0.16522972  0.          0.6518305\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 48\n",
      "Done: [False]\n",
      "Action taken: [ 0.22095005 -0.864486  ]\n",
      "Observation: [[ 0.65233     0.33487487 -0.3960459  -0.16522972  0.          0.7490852\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 49\n",
      "Done: [False]\n",
      "Action taken: [0.16065627 0.5519657 ]\n",
      "Observation: [[ 0.6127254   0.31009042 -0.3960459  -0.16522972  0.          0.686989\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 50\n",
      "Done: [False]\n",
      "Action taken: [0.34101892 0.81502736]\n",
      "Observation: [[ 0.5731208   0.28530595 -0.3960459  -0.16522972  0.          0.59529847\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 51\n",
      "Done: [False]\n",
      "Action taken: [0.94490767 0.25322333]\n",
      "Observation: [[ 0.53351617  0.2605215  -0.3960459  -0.16522972  0.          0.56681085\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 52\n",
      "Done: [False]\n",
      "Action taken: [0.78706133 0.95438886]\n",
      "Observation: [[ 0.4939116   0.23573704 -0.3960459  -0.16522972  0.          0.45944208\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 53\n",
      "Done: [False]\n",
      "Action taken: [0.98164743 0.19584747]\n",
      "Observation: [[ 0.45430702  0.21095258 -0.3960459  -0.16522972  0.          0.43740925\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 54\n",
      "Done: [False]\n",
      "Action taken: [0.87484276 0.97364175]\n",
      "Observation: [[ 0.41470242  0.18616812 -0.3960459  -0.16522972  0.          0.32787454\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 55\n",
      "Done: [False]\n",
      "Action taken: [0.4803952  0.37491995]\n",
      "Observation: [[ 0.37509784  0.16138366 -0.3960459  -0.16522972  0.          0.28569606\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 56\n",
      "Done: [False]\n",
      "Action taken: [ 0.93950397 -0.79682636]\n",
      "Observation: [[ 0.33549324  0.1365992  -0.3960459  -0.16522972  0.          0.37533903\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 57\n",
      "Done: [False]\n",
      "Action taken: [0.84365225 0.11939077]\n",
      "Observation: [[ 0.29588866  0.11181474 -0.3960459  -0.16522972  0.          0.36190757\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 58\n",
      "Done: [False]\n",
      "Action taken: [0.72253627 0.46111193]\n",
      "Observation: [[ 0.25628406  0.08703028 -0.3960459  -0.16522972  0.          0.31003246\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 59\n",
      "Done: [False]\n",
      "Action taken: [ 0.84732044 -0.52745664]\n",
      "Observation: [[ 0.21667948  0.06224582 -0.3960459  -0.16522972  0.          0.36937135\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 60\n",
      "Done: [False]\n",
      "Action taken: [0.7861484  0.88668305]\n",
      "Observation: [[ 0.1770749   0.03746136 -0.3960459  -0.16522972  0.          0.2696195\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 61\n",
      "Done: [False]\n",
      "Action taken: [ 0.9855085 -0.7387099]\n",
      "Observation: [[ 0.1374703   0.015      -0.3960459   0.16522972  0.          0.35272437\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 62\n",
      "Done: [False]\n",
      "Action taken: [0.7330125 0.5233573]\n",
      "Observation: [[ 0.09786571  0.03978446 -0.3960459   0.16522972  0.          0.29384667\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 63\n",
      "Done: [False]\n",
      "Action taken: [0.39355692 0.2664102 ]\n",
      "Observation: [[ 0.05826112  0.06456891 -0.3960459   0.16522972  0.          0.2638755\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 64\n",
      "Done: [False]\n",
      "Action taken: [ 0.7927652  -0.99874014]\n",
      "Observation: [[ 0.01865653  0.08935338 -0.3960459   0.16522972  0.          0.3762338\n",
      "   0.          1.        ]]\n",
      "Reward: [0.]\n",
      "iteration: 65\n",
      "Done: [False]\n",
      "Action taken: [ 0.3962541 -0.5463493]\n",
      "Observation: [[-0.02094806  0.11413784 -0.3960459   0.16522972  0.          0.4376981\n",
      "   0.          1.        ]]\n",
      "Reward: [-6.824125]\n",
      "iteration: 66\n",
      "Done: [False]\n",
      "Action taken: [0.70812786 0.05305269]\n",
      "Observation: [[0.9691667 0.51625   0.25      0.25      0.        0.78125   1.\n",
      "  0.       ]]\n",
      "Reward: [0.]\n",
      "iteration: 67\n",
      "Done: [ True]\n",
      "No frames directory found, skipping video creation.\n"
     ]
    }
   ],
   "source": [
    "# Create and test vectorised environment\n",
    "# Create a vectorized environment\n",
    "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
    "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "i = 0\n",
    "while True:\n",
    "    i+=1\n",
    "    action = env.action_space.sample()  # Sample random action\n",
    "    print(\"Action taken:\", action)\n",
    "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
    "    print(\"Observation:\", obs)\n",
    "    print(\"Reward:\", reward)\n",
    "    print('iteration:', i)\n",
    "    print(\"Done:\", done)\n",
    "    if np.any(done):\n",
    "        obs = env.reset()\n",
    "        break\n",
    "        print(\"Environment reset\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWUit3VbB6Xl"
   },
   "source": [
    "### Custom Evaluation Callback\n",
    "\n",
    "Define a custom evaluation callback to periodically evaluate and log the performance of the trained model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xcGJuyx_jaw2"
   },
   "outputs": [],
   "source": [
    "class CustomEvalCallback(EvalCallback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        result = super()._on_step()\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8sLdG3FCCS9"
   },
   "source": [
    "### Setting Up Hyperparameters and Training the Model\n",
    "\n",
    "Define hyperparameters for the PPO algorithm and set up the training process. This section includes creating the training and evaluation environments, initializing the model, and starting the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mL_LYMHeejpF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-174.37 +/- 94.86\n",
      "Episode length: 639.20 +/- 444.01\n",
      "New best mean reward!\n",
      "Evaluation at step 10000: mean reward -174.37\n",
      "Eval num_timesteps=80000, episode_reward=-110.10 +/- 114.50\n",
      "Episode length: 431.60 +/- 464.91\n",
      "New best mean reward!\n",
      "Evaluation at step 20000: mean reward -110.10\n",
      "Eval num_timesteps=120000, episode_reward=-175.49 +/- 93.94\n",
      "Episode length: 621.40 +/- 464.91\n",
      "Evaluation at step 30000: mean reward -175.49\n",
      "Eval num_timesteps=160000, episode_reward=-135.81 +/- 95.15\n",
      "Episode length: 431.60 +/- 464.91\n",
      "Evaluation at step 40000: mean reward -135.81\n",
      "Eval num_timesteps=200000, episode_reward=67.97 +/- 106.73\n",
      "Episode length: 90.60 +/- 39.95\n",
      "New best mean reward!\n",
      "Evaluation at step 50000: mean reward 67.97\n",
      "Eval num_timesteps=240000, episode_reward=6.31 +/- 39.71\n",
      "Episode length: 57.20 +/- 6.40\n",
      "Evaluation at step 60000: mean reward 6.31\n",
      "Eval num_timesteps=280000, episode_reward=16.07 +/- 33.36\n",
      "Episode length: 113.00 +/- 85.15\n",
      "Evaluation at step 70000: mean reward 16.07\n",
      "Eval num_timesteps=320000, episode_reward=6.41 +/- 39.56\n",
      "Episode length: 69.00 +/- 28.82\n",
      "Evaluation at step 80000: mean reward 6.41\n",
      "Eval num_timesteps=360000, episode_reward=24.18 +/- 31.09\n",
      "Episode length: 61.20 +/- 7.68\n",
      "Evaluation at step 90000: mean reward 24.18\n",
      "Eval num_timesteps=400000, episode_reward=0.48 +/- 49.37\n",
      "Episode length: 102.80 +/- 89.28\n",
      "Evaluation at step 100000: mean reward 0.48\n",
      "Eval num_timesteps=440000, episode_reward=65.24 +/- 105.79\n",
      "Episode length: 173.00 +/- 118.56\n",
      "Evaluation at step 110000: mean reward 65.24\n",
      "Eval num_timesteps=480000, episode_reward=52.19 +/- 112.54\n",
      "Episode length: 122.20 +/- 69.76\n",
      "Evaluation at step 120000: mean reward 52.19\n",
      "Eval num_timesteps=520000, episode_reward=29.77 +/- 41.61\n",
      "Episode length: 245.40 +/- 217.91\n",
      "Evaluation at step 130000: mean reward 29.77\n",
      "Eval num_timesteps=560000, episode_reward=16.07 +/- 39.44\n",
      "Episode length: 141.20 +/- 49.20\n",
      "Evaluation at step 140000: mean reward 16.07\n",
      "Eval num_timesteps=600000, episode_reward=34.30 +/- 37.16\n",
      "Episode length: 206.20 +/- 126.34\n",
      "Evaluation at step 150000: mean reward 34.30\n",
      "Training completed and logs are saved.\n"
     ]
    }
   ],
   "source": [
    "create_new = True\n",
    "\n",
    "# TODO: detect cuda properly \n",
    "has_gpu = True\n",
    "\n",
    "if has_gpu:\n",
    "    target_device = \"cuda\"\n",
    "else:\n",
    "    target_device = \"cpu\"\n",
    "\n",
    "# Hyperparameters for PPO\n",
    "hyperparams = {\n",
    "    'n_steps': 2048,\n",
    "    'batch_size': 64,  # Try a smaller batch size for more frequent updates\n",
    "    'n_epochs': 4,  # Fewer epochs to reduce overfitting to on-policy data\n",
    "    'gamma': 0.99,  # Slightly higher discount factor to consider more future rewards\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'clip_range_vf': 0.2,\n",
    "    'ent_coef': 0.001,\n",
    "    'vf_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'target_kl': 0.01,\n",
    "    'tensorboard_log': f'{DRIVE_PATH}/{DAY}/logs/tensorboard_logs'\n",
    "}\n",
    "\n",
    "# Bry changes..\n",
    "hyperparams['batch_size'] = 64\n",
    "hyperparams['n_steps'] = 8192\n",
    "total_training_steps = 600000\n",
    "total_training_steps *= 1\n",
    "\n",
    "\n",
    "\n",
    "bot_name = 'edwin'\n",
    "\n",
    "eval_freq = 10000\n",
    "\n",
    "# can train against a previous model if desired, if not then set this to None\n",
    "# model2 = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\")\n",
    "model2 = None\n",
    "\n",
    "# Load the training environment and ensure it's wrapped with VecNormalize\n",
    "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
    "# train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ModelPlayer(model2) if i < 2 else ComputerPlayer())) for i in range(4)])\n",
    "if create_new:\n",
    "  # create training env when creating model\n",
    "  train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
    "else:\n",
    "  # or, load training env when loading model\n",
    "  train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/pong_bot_1_normalize\", train_env)\n",
    "train_env.training = True  # Ensure it's in training mode\n",
    "\n",
    "# Create the evaluation environment and wrap it with VecNormalize\n",
    "if model2 is not None:\n",
    "    eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ModelPlayer(model2)))])\n",
    "else:\n",
    "    eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
    "    \n",
    "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
    "eval_env.training = False  # Ensure it's not in training mode\n",
    "\n",
    "# Create the CustomEvalCallback with the evaluation environment\n",
    "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
    "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
    "                                   deterministic=True, render=False)\n",
    "\n",
    "# Create a new model\n",
    "if create_new:\n",
    "  model = PPO('MlpPolicy', env=train_env, **hyperparams, device=target_device)\n",
    "else:\n",
    "# or, Load the pre-trained model\n",
    "  model = PPO.load(f\"{DRIVE_PATH}/{DAY}/pong_bot_1\", env=train_env, **hyperparams, force_reset=True, device=target_device)\n",
    "\n",
    "# # Resume training the model with the callback\n",
    "model.learn(total_timesteps=total_training_steps, callback=eval_callback)\n",
    "\n",
    "# # Save the model and the normalization statistics\n",
    "model.save(f\"{DRIVE_PATH}/{DAY}/{bot_name}\")\n",
    "train_env.save(f\"{DRIVE_PATH}/{DAY}/{bot_name}_normalize\")\n",
    "\n",
    "print(\"Training completed and logs are saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYiTt3lrCJXA"
   },
   "source": [
    "### Evaluating the Trained Model\n",
    "\n",
    "Load the trained model and evaluate its performance. This section demonstrates how to use the trained model to play the game and render the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pUCiob4fHHv2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "Moviepy - Building video ./game_video.mp4.\n",
      "Moviepy - Writing video ./game_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./game_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and evaluate\n",
    "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/{bot_name}\")\n",
    "# model2 = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\")\n",
    "\n",
    "# Create a new environment for rendering\n",
    "env = CustomPongEnv(computer_player=ComputerPlayer())\n",
    "# env = CustomPongEnv(computer_player=ModelPlayer(model2))\n",
    "\n",
    "\n",
    "obs, _ = env.reset()\n",
    "count = 0\n",
    "\n",
    "\n",
    "while count < 4:\n",
    "    action = model.predict(obs)  # Get action from the trained model\n",
    "    obs, reward, done, info, _ = env.step(action[0])\n",
    "    env.render()\n",
    "    if done:\n",
    "      count += 1\n",
    "      print(\"reset\")\n",
    "      env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTsbiePqCQZb"
   },
   "source": [
    "### Exporting the Model to ONNX\n",
    "\n",
    "Export the trained model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "i-hPY2KxRxsE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved at: ./data/monday-8-july/edwin_dynamo.onnx\n"
     ]
    }
   ],
   "source": [
    "class OnnxableSB3Policy(th.nn.Module):\n",
    "    def __init__(self, policy: BasePolicy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        # Run the policy in deterministic mode\n",
    "        actions, values, log_prob = self.policy(observation, deterministic=True)\n",
    "        return actions\n",
    "\n",
    "\n",
    "# Load the trained PyTorch model\n",
    "# model_path = f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\"\n",
    "model_path = f\"{DRIVE_PATH}/{DAY}/{bot_name}\"\n",
    "\n",
    "model = PPO.load(model_path, device=\"cpu\")\n",
    "\n",
    "onnx_policy = OnnxableSB3Policy(model.policy)\n",
    "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/{bot_name}_dynamo.onnx\"\n",
    "\n",
    "# Define dummy input based on the observation space shape\n",
    "observation_size = model.observation_space.shape\n",
    "dummy_input = th.randn(1, *observation_size)\n",
    "\n",
    "# Export the model to ONNX\n",
    "th.onnx.export(\n",
    "    onnx_policy,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    opset_version=11,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"actions\"]\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved at: {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLmeiNQfCa1-"
   },
   "source": [
    "### Exporting the ONNX Model to TensorFlow\n",
    "\n",
    "Convert the ONNX format model to TensorFlow SavedModel format. This allows for compatibility with various deployment platforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CPSegjRq1cOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add             │ 2              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Cast            │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Constant        │ 9              │ \u001b[1;38;5;46m6               \u001b[0m │\n",
      "│ ConstantOfShape │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Exp             │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Expand          │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Flatten         │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Gemm            │ 3              │ 3                │\n",
      "│ Mul             │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Reshape         │ 1              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Shape           │ 2              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Tanh            │ 2              │ 2                │\n",
      "│ Model Size      │ 21.8KiB        │ \u001b[1;38;5;46m20.8KiB         \u001b[0m │\n",
      "└─────────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Constant   │ 6              │ 6                │\n",
      "│ Gemm       │ 3              │ 3                │\n",
      "│ Tanh       │ 2              │ 2                │\n",
      "│ Model Size │ 20.8KiB        │ 20.8KiB          │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Constant   │ 6              │ 6                │\n",
      "│ Gemm       │ 3              │ 3                │\n",
      "│ Tanh       │ 2              │ 2                │\n",
      "│ Model Size │ 20.8KiB        │ 20.8KiB          │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "\u001b[32mModel optimizing complete!\u001b[0m\n",
      "\n",
      "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
      "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
      "\n",
      "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
      "\n",
      "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 6\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_3/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 6\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_3/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_2/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m4 / 6\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_4/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m5 / 6\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_4/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_3/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m6 / 6\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_5/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
      "Saved artifact at './data/monday-8-july/edwin_tf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serving_default'\n",
      "  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 8), dtype=tf.float32, name='input')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(1, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132556820978272: TensorSpec(shape=(8, 64), dtype=tf.float32, name=None)\n",
      "  132556820980736: TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
      "  132562340394816: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
      "  132556820152912: TensorSpec(shape=(64, 64), dtype=tf.float32, name=None)\n",
      "  132562336856048: TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
      "  132556820230592: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
      "  132562338592752: TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)\n",
      "  132556820154320: TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
      "  132562336300224: TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n",
      "\u001b[32msaved_model output complete!\u001b[0m\n",
      "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
      "\u001b[32mFloat16 tflite output complete!\u001b[0m\n",
      "TensorFlow SavedModel saved at: ./data/monday-8-july/edwin_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 17:04:32.266589: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.266764: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-07-08 17:04:32.266831: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-08 17:04:32.267007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.267163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.267307: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.267489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.267634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.267755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7046 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "W0000 00:00:1720454672.287194   58010 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1720454672.287210   58010 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-07-08 17:04:32.298074: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3064] Estimated count of arithmetic ops: 0.018 M  ops, equivalently 0.009 M  MACs\n",
      "2024-07-08 17:04:32.307215: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.307386: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-07-08 17:04:32.307453: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-08 17:04:32.307637: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.307792: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.307937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.308119: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.308276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 17:04:32.308400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7046 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "W0000 00:00:1720454672.327188   58010 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1720454672.327205   58010 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-07-08 17:04:32.339247: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3064] Estimated count of arithmetic ops: 0.018 M  ops, equivalently 0.009 M  MACs\n"
     ]
    }
   ],
   "source": [
    "tf_model_path = f\"{DRIVE_PATH}/{DAY}/{bot_name}_tf\"\n",
    "\n",
    "# Convert ONNX to TensorFlow SavedModel\n",
    "convert(\n",
    "    input_onnx_file_path=onnx_file_path,\n",
    "    output_folder_path=tf_model_path,\n",
    "    output_signaturedefs=True,\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow SavedModel saved at: {tf_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-6S9n_yCnGO"
   },
   "source": [
    "### Converting TensorFlow Model to TensorFlow.js\n",
    "\n",
    "Convert the TensorFlow model to TensorFlow.js format for deployment in a web browser. Go to [wimblepong.netlify.app/upload](https://wimblepong.netlify.app/upload) to play it in the browser.\n",
    "\n",
    "Be careful - if the model or the code handling the model has security vulnerabilities, it can be exploited by malicious actors. This could include executing arbitrary code, data leakage, or other malicious activities. A model trained by yourself should not be cause for alarm. See the [source code](https://https://github.com/mrbenbot/wimblepong/blob/main/src/libs/tensorFlowPlayer.ts) to make sure you are happy with the way the model is being run in the browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wnaMw0pAURsM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow.js model saved at: ./data/monday-8-july/edwin_tfjs\n"
     ]
    }
   ],
   "source": [
    "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/{bot_name}_tfjs\"\n",
    "\n",
    "# Convert the TensorFlow model to TensorFlow.js\n",
    "subprocess.run([\n",
    "    'tensorflowjs_converter',\n",
    "    '--input_format', 'tf_saved_model',\n",
    "    '--output_format', 'tfjs_graph_model',\n",
    "    \"--signature_name\", \"serving_default\",\n",
    "    tf_model_path,\n",
    "    tfjs_model_path\n",
    "])\n",
    "\n",
    "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n",
    "f'{DRIVE_PATH}/{DAY}/logs/tensorboard_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MGBxjLDCpQx"
   },
   "source": [
    "### Visualizing Training with TensorBoard\n",
    "\n",
    "Set up TensorBoard to visualize the training process, including metrics like rewards and losses over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logs = f'{DRIVE_PATH}/{DAY}/logs/tensorboard_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ergejPlZAKs4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a48770c85fe4ba8c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a48770c85fe4ba8c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# change path to logs as needed\n",
    "%tensorboard --logdir=$tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
