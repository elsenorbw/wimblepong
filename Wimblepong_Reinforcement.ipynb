{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "71e9b7b4-57bc-4d97-f712-a9c58317518d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=463e477b6718ad85733a7b02061a3307cd12865a4bf697cdd168eec8278becae\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.0)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.0 ydf-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup Google Colab Environment\n",
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install imageio pillow\n",
        "%pip install tensorflowjs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "2e0b228f-8fc9-4189-d58a-650ea6dd6954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "07fd6b03-a06c-4569-9a72-91a1576f1b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'blue', 'Player2': 'red'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1))/100,\n",
        "    # \"conceed_point\": lambda ball, paddle, rally_length: -0.1,\n",
        "    \"serve\": lambda ball_speed: ball_speed / 10,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "ExmAn7VtUakq",
        "outputId": "c53e4879-c545-4875-b832-369cfb07e940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CustomPongEnv-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -60]), high=np.array([1, 60]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -np.inf, -np.inf, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([COURT_WIDTH, COURT_HEIGHT, np.inf, np.inf, COURT_WIDTH, COURT_HEIGHT, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "\n",
        "        self.starting_state_index = 0\n",
        "        self.serve_delay = 50\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 15\n",
        "\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "\n",
        "        self.is_done = False\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"Environment reset\")\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        computer = starting_state['opponent']\n",
        "        player = starting_state['player']\n",
        "\n",
        "        self.game_state = {\n",
        "            'server': server,\n",
        "            'positions_reversed': positions_reversed,\n",
        "            'opponent': computer,\n",
        "            'player': player,\n",
        "            Player.Player1: {'x': 0, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "            Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "            'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "            'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 30 * np.random.rand()\n",
        "        self.serve_delay = 100 * np.random.rand()\n",
        "        self.direction = self.direction if np.random.rand() > 0.5 else -self.direction\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.is_done = False\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "            self.game_state[Player.Player2]['x'] = 0\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = 0\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius']\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction}\n",
        "        computer_player_actions = self.get_computer_player_actions(self.game_state['opponent'])\n",
        "        actions = {self.game_state['opponent']: computer_player_actions, self.game_state['player']: model_player_actions}\n",
        "        reward = self.update_game_state(actions, 2.5)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        terminated = self.check_done()\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def update_game_state(self, actions, delta_time):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        model_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            self.is_done = True\n",
        "            return 0.01\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            if serving_from_left:\n",
        "                ball['x'] = game_state[server]['width'] + ball['radius']\n",
        "            else:\n",
        "                ball['x'] = COURT_WIDTH - game_state[server]['width'] - ball['radius']\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if model_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward\n",
        "\n",
        "    def get_computer_player_actions(self, player):\n",
        "        state = self.game_state\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4 ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "\n",
        "    def bounded_value(self, value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x']),\n",
        "            float(state['ball']['y']),\n",
        "            float(state['ball']['dx']),\n",
        "            float(state['ball']['dy']),\n",
        "            float(paddle['x']),\n",
        "            float(paddle['y']),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(is_server),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def check_done(self):\n",
        "        if self.game_state['stats']['rally_length'] > 100:\n",
        "            return True\n",
        "        if self.step_count > 1000:\n",
        "            return True\n",
        "        return self.is_done\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "outputId": "33460732-d787-4f91-8939-f22c24a8a1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment reset\n",
            "Environment reset\n",
            "Initial observation: [[0.00999315 0.00999997 0.00995036 0.00995036 0.         0.00999996\n",
            "  0.00707071 0.00707071]]\n",
            "Action taken: [ 0.14008403 51.690395  ]\n",
            "Action taken: [ 0.14008403 51.690395  ]\n",
            "Observation: [[ 0.00706622  0.00707105  0.00703597 -0.999949    0.         -0.99929583\n",
            "   0.00499969  0.00499969]]\n",
            "Reward: [0.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [ 0.7669443 -4.942801 ]\n",
            "Action taken: [ 0.7669443 -4.942801 ]\n",
            "Observation: [[ 0.00576955 -1.3709505   0.00574484 -1.1932889   0.         -0.59796655\n",
            "  -1.4140368   0.00408214]]\n",
            "Reward: [1.9998039]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [  0.91694933 -44.478245  ]\n",
            "Action taken: [  0.91694933 -44.478245  ]\n",
            "Observation: [[ 1.7319448  -1.4962285   0.00497518 -0.8874667   0.          0.9496263\n",
            "  -0.99992496  0.00353516]]\n",
            "Reward: [0.02100232]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [  0.675326 -34.00926 ]\n",
            "Action taken: [  0.675326 -34.00926 ]\n",
            "Observation: [[ 1.7499822  -1.5388896   0.00444993 -0.73779315  0.          1.4516258\n",
            "  -0.8164489   0.00316187]]\n",
            "Reward: [0.02286555]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [  0.74023485 -50.629353  ]\n",
            "Action taken: [  0.74023485 -50.629353  ]\n",
            "Observation: [[ 1.7320484  -1.5629351   0.00406221 -0.6448931   0.          1.7469234\n",
            "  -0.7070714   0.00288631]]\n",
            "Reward: [0.02475386]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [  0.8116776 -31.510416 ]\n",
            "Action taken: [  0.8116776 -31.510416 ]\n",
            "Observation: [[ 1.7162344  -1.579819    0.00376087 -0.5800728   0.          1.5917844\n",
            "  -0.63242704  0.00267214]]\n",
            "Reward: [0.0265699]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [ 0.6800271 -8.751628 ]\n",
            "Action taken: [ 0.6800271 -8.751628 ]\n",
            "Observation: [[ 1.704989   -1.5929409   0.00351797 -0.53154564  0.          1.2976042\n",
            "  -0.5773262   0.00249949]]\n",
            "Reward: [0.02829872]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [  0.7622731 -58.189236 ]\n",
            "Action taken: [  0.7622731 -58.189236 ]\n",
            "Observation: [[ 1.6973403  -1.6036838   0.00331678 -0.49346063  0.          1.1228597\n",
            "  -0.5345015   0.00235649]]\n",
            "Reward: [0.02994372]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [ 0.6328975 54.800438 ]\n",
            "Action taken: [ 0.6328975 54.800438 ]\n",
            "Observation: [[ 1.6922317  -1.6127485   0.00314657 -0.4625405   0.          0.22081165\n",
            "  -0.49998122  0.0022355 ]]\n",
            "Reward: [0.03151249]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [  0.71356833 -14.97114   ]\n",
            "Action taken: [  0.71356833 -14.97114   ]\n",
            "Observation: [[ 1.6888847  -1.6205478   0.00300013 -0.43678814  0.          0.44948488\n",
            "  -0.47138748  0.00213142]]\n",
            "Reward: [0.03301294]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [ 0.08481074 -3.7663636 ]\n",
            "Action taken: [ 0.08481074 -3.7663636 ]\n",
            "Observation: [[ 1.6867636  -1.6273533   0.00287241 -0.41490775  0.          0.4888278\n",
            "  -0.4471979   0.00204063]]\n",
            "Reward: [0.03445232]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [  0.7519573 -53.334553 ]\n",
            "Action taken: [  0.7519573 -53.334553 ]\n",
            "Observation: [[ 1.6855024  -1.6333554   0.00275972 -0.3960172   0.          1.0516053\n",
            "  -0.4263869   0.00196052]]\n",
            "Reward: [0.03583692]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [  0.8371961 -13.807061 ]\n",
            "Action taken: [  0.8371961 -13.807061 ]\n",
            "Observation: [[ 1.6848496  -1.6386951   0.00265933 -0.3794922   0.          0.975555\n",
            "  -0.40823466  0.00188916]]\n",
            "Reward: [0.03717221]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [ 0.8753082 -9.90486  ]\n",
            "Action taken: [ 0.8753082 -9.90486  ]\n",
            "Observation: [[ 1.6846302  -1.6434796   0.00256916 -0.364877    0.          0.9139288\n",
            "  -0.39221942  0.00182505]]\n",
            "Reward: [0.03846288]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [0.8252467 9.44821  ]\n",
            "Action taken: [0.8252467 9.44821  ]\n",
            "Observation: [[ 1.6847203  -1.6477935   0.00248757 -0.35182995  0.          0.70637673\n",
            "  -0.3779523   0.00176706]]\n",
            "Reward: [0.03971298]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [  0.35820228 -24.693464  ]\n",
            "Action taken: [  0.35820228 -24.693464  ]\n",
            "Observation: [[ 1.685032   -1.651704    0.0024133  -0.34008935  0.          0.83580834\n",
            "  -0.3651368   0.00171425]]\n",
            "Reward: [0.04092598]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [ 0.98675746 58.96386   ]\n",
            "Action taken: [ 0.98675746 58.96386   ]\n",
            "Observation: [[ 1.6855016e+00 -1.6552659e+00  2.3453042e-03 -3.2945073e-01\n",
            "   0.0000000e+00 -2.5468978e-01 -3.5354233e-01  1.6659149e-03]]\n",
            "Reward: [0.04210494]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [ 0.9905151 25.710934 ]\n",
            "Action taken: [ 0.9905151 25.710934 ]\n",
            "Observation: [[ 1.6860826e+00 -1.6585245e+00  2.2827503e-03 -3.1975186e-01\n",
            "   0.0000000e+00 -7.1623731e-01 -3.4298655e-01  1.6214420e-03]]\n",
            "Reward: [0.04325252]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [  0.20594497 -28.556849  ]\n",
            "Action taken: [  0.20594497 -28.556849  ]\n",
            "Observation: [[ 1.6867411e+00 -1.6615171e+00  2.2249487e-03 -3.1086203e-01\n",
            "   0.0000000e+00 -1.5875442e-01 -3.3332312e-01  1.5803468e-03]]\n",
            "Reward: [0.04437107]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [ 0.5299981 22.67005  ]\n",
            "Action taken: [ 0.5299981 22.67005  ]\n",
            "Observation: [[ 1.6874517e+00 -1.6642753e+00  2.1713264e-03 -3.0267471e-01\n",
            "   0.0000000e+00 -5.8920711e-01 -3.2443300e-01  1.5422222e-03]]\n",
            "Reward: [0.04546265]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [  0.92059505 -18.731735  ]\n",
            "Action taken: [  0.92059505 -18.731735  ]\n",
            "Observation: [[ 1.6881957e+00 -1.6668255e+00  2.1214033e-03 -2.9510200e-01\n",
            "   0.0000000e+00 -2.0619331e-01 -3.1621826e-01  1.5067265e-03]]\n",
            "Reward: [0.04652911]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [  0.60557944 -47.743263  ]\n",
            "Action taken: [  0.60557944 -47.743263  ]\n",
            "Observation: [[ 1.6889592e+00 -1.5535223e+00  2.0747723e-03  2.9189365e+00\n",
            "   0.0000000e+00  7.5182056e-01 -3.0859751e-01  1.4735709e-03]]\n",
            "Reward: [0.04757209]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [  0.50815123 -12.935049  ]\n",
            "Action taken: [  0.50815123 -12.935049  ]\n",
            "Observation: [[ 1.6897317e+00 -1.3175129e+00  2.0310869e-03  2.4547784e+00\n",
            "   0.0000000e+00  9.6671122e-01 -3.0150241e-01  1.4425089e-03]]\n",
            "Reward: [0.04859307]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [ 0.1397466 34.035435 ]\n",
            "Action taken: [ 0.1397466 34.035435 ]\n",
            "Observation: [[ 1.6905054e+00 -1.1129811e+00  1.9900496e-03  2.1590149e+00\n",
            "   0.0000000e+00  2.5211355e-01 -2.9487523e-01  1.4133290e-03]]\n",
            "Reward: [0.04959337]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [ 0.30435336 11.955004  ]\n",
            "Action taken: [ 0.30435336 11.955004  ]\n",
            "Observation: [[ 1.6912746e+00 -9.2772579e-01  1.9514033e-03  1.9495313e+00\n",
            "   0.0000000e+00 -1.7095020e-03 -2.8866670e-01  1.3858486e-03]]\n",
            "Reward: [0.0505742]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [0.88998234 7.4729266 ]\n",
            "Action taken: [0.88998234 7.4729266 ]\n",
            "Observation: [[ 1.6920350e+00 -7.5432545e-01  1.9149244e-03  1.7911705e+00\n",
            "   0.0000000e+00 -1.6039826e-01 -2.8283450e-01  1.3599087e-03]]\n",
            "Reward: [0.05153665]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [ 0.31993443 16.780048  ]\n",
            "Action taken: [ 0.31993443 16.780048  ]\n",
            "Observation: [[ 1.6927832e+00 -5.8800608e-01  1.8804175e-03  1.6660342e+00\n",
            "   0.0000000e+00 -5.1834846e-01 -2.7734208e-01  1.3353706e-03]]\n",
            "Reward: [0.05248171]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [ 0.23499402 -7.2024155 ]\n",
            "Action taken: [ 0.23499402 -7.2024155 ]\n",
            "Observation: [[ 1.6935174e+00 -4.2561543e-01  1.8477112e-03  1.5639241e+00\n",
            "   0.0000000e+00 -3.5039431e-01 -2.7215770e-01  1.3121122e-03]]\n",
            "Reward: [0.0534103]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [ 3.758253e-02 -5.574232e+01]\n",
            "Action taken: [ 3.758253e-02 -5.574232e+01]\n",
            "Observation: [[ 1.6942358e+00 -2.6509693e-01  1.8166540e-03  1.4785470e+00\n",
            "   0.0000000e+00  8.8701087e-01 -2.6725358e-01  1.2900262e-03]]\n",
            "Reward: [0.05432323]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [  0.3510004 -42.814594 ]\n",
            "Action taken: [  0.3510004 -42.814594 ]\n",
            "Observation: [[ 1.69493723e+00 -1.05209775e-01  1.78711209e-03  1.40578103e+00\n",
            "   0.00000000e+00  1.01800537e+00 -2.62605369e-01  1.26901711e-03]]\n",
            "Reward: [0.05522129]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [ 0.16984628 20.675707  ]\n",
            "Action taken: [ 0.16984628 20.675707  ]\n",
            "Observation: [[ 1.6956213e+00  5.4626100e-02  1.7589660e-03  1.3427992e+00\n",
            "   0.0000000e+00  5.3668886e-01 -2.5819156e-01  1.2490002e-03]]\n",
            "Reward: [0.05610517]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [  0.11487572 -12.713313  ]\n",
            "Action taken: [  0.11487572 -12.713313  ]\n",
            "Observation: [[ 1.6962873e+00  2.1442117e-01  1.7321091e-03  1.2875885e+00\n",
            "   0.0000000e+00  8.0802578e-01 -2.5399306e-01  1.2298997e-03]]\n",
            "Reward: [0.05697553]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [  0.16701926 -52.821274  ]\n",
            "Action taken: [  0.16701926 -52.821274  ]\n",
            "Observation: [[ 1.6969353e+00  3.7368506e-01  1.7064459e-03  1.2386720e+00\n",
            "   0.0000000e+00  9.6352440e-01 -2.4999295e-01  1.2116478e-03]]\n",
            "Reward: [0.05783296]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [  0.7513193 -36.64002  ]\n",
            "Action taken: [  0.7513193 -36.64002  ]\n",
            "Observation: [[ 1.6975654e+00  5.3149545e-01  1.6818907e-03  1.1949381e+00\n",
            "   0.0000000e+00  9.3731034e-01 -2.4617606e-01  1.1941832e-03]]\n",
            "Reward: [0.05867804]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [  0.78906524 -25.451601  ]\n",
            "Action taken: [  0.78906524 -25.451601  ]\n",
            "Observation: [[ 1.6981777e+00  6.8658471e-01  1.6583657e-03  1.1555316e+00\n",
            "   0.0000000e+00  9.1312563e-01 -2.4252883e-01  1.1774512e-03]]\n",
            "Reward: [0.05951127]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [ 0.49634475 38.841396  ]\n",
            "Action taken: [ 0.49634475 38.841396  ]\n",
            "Observation: [[ 1.6987725e+00  8.3744925e-01  1.6358010e-03  1.1197833e+00\n",
            "   0.0000000e+00  1.1505973e-02 -2.3903903e-01  1.1614017e-03]]\n",
            "Reward: [0.06033316]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [  0.97740984 -12.889339  ]\n",
            "Action taken: [  0.97740984 -12.889339  ]\n",
            "Observation: [[ 1.6993501e+00  9.8248321e-01  1.6141331e-03  1.0871598e+00\n",
            "   0.0000000e+00  3.1020415e-01 -2.3569569e-01  1.1459898e-03]]\n",
            "Reward: [0.06114413]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [ 0.9492313 49.81825  ]\n",
            "Action taken: [ 0.9492313 49.81825  ]\n",
            "Observation: [[ 1.6999110e+00  1.1201197e+00  1.5933039e-03  1.0572309e+00\n",
            "   0.0000000e+00 -8.5645747e-01 -2.3248881e-01  1.1311739e-03]]\n",
            "Reward: [0.06194463]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [ 3.4737363e-02 -4.2338779e+01]\n",
            "Action taken: [ 3.4737363e-02 -4.2338779e+01]\n",
            "Observation: [[ 1.7004555e+00  1.2489668e+00  1.5732608e-03  1.0296452e+00\n",
            "   0.0000000e+00  1.5306135e-01 -2.2940937e-01  1.1169169e-03]]\n",
            "Reward: [0.06273504]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [1.9221675e-02 4.1493137e+01]\n",
            "Action taken: [1.9221675e-02 4.1493137e+01]\n",
            "Observation: [[ 1.7009841e+00  1.3679183e+00  1.5539555e-03  1.0041119e+00\n",
            "   0.0000000e+00 -8.3282506e-01 -2.2644915e-01  1.1031844e-03]]\n",
            "Reward: [0.06351575]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [  0.6622016 -52.374306 ]\n",
            "Action taken: [  0.6622016 -52.374306 ]\n",
            "Observation: [[ 1.7014974e+00  1.4762237e+00  1.5353438e-03  9.8038858e-01\n",
            "   0.0000000e+00  4.3233591e-01 -2.2360063e-01  1.0899450e-03]]\n",
            "Reward: [0.0642871]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [  0.97206324 -16.335321  ]\n",
            "Action taken: [  0.97206324 -16.335321  ]\n",
            "Observation: [[ 1.7019960e+00  1.5735164e+00  1.5173851e-03  9.5827097e-01\n",
            "   0.0000000e+00  8.1647390e-01 -2.2085696e-01  1.0771698e-03]]\n",
            "Reward: [0.06504943]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [  0.16630718 -13.909167  ]\n",
            "Action taken: [  0.16630718 -13.909167  ]\n",
            "Observation: [[ 1.7024800e+00  1.6597952e+00  1.5000423e-03  9.3758577e-01\n",
            "   0.0000000e+00  9.4824916e-01 -2.1821189e-01  1.0648323e-03]]\n",
            "Reward: [0.06580304]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [ 0.29339206 22.971933  ]\n",
            "Action taken: [ 0.29339206 22.971933  ]\n",
            "Observation: [[ 1.7029501e+00  1.7353765e+00  1.4832808e-03  9.1818476e-01\n",
            "   0.0000000e+00  3.7759054e-01 -2.1565963e-01  1.0529081e-03]]\n",
            "Reward: [0.06654821]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [ 0.22244188 11.323298  ]\n",
            "Action taken: [ 0.22244188 11.323298  ]\n",
            "Observation: [[ 1.7034068e+00  1.8008256e+00  1.4670689e-03  8.9994031e-01\n",
            "   0.0000000e+00  9.4957590e-02 -2.1319488e-01  1.0413746e-03]]\n",
            "Reward: [0.06728525]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [  0.78096974 -14.451772  ]\n",
            "Action taken: [  0.78096974 -14.451772  ]\n",
            "Observation: [[ 1.7038505e+00  1.8568814e+00  1.4513772e-03  8.8274193e-01\n",
            "   0.0000000e+00  4.5235029e-01 -2.1081275e-01  1.0302109e-03]]\n",
            "Reward: [0.06801441]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [0.84633183 7.3007827 ]\n",
            "Action taken: [0.84633183 7.3007827 ]\n",
            "Observation: [[ 1.7042818e+00  1.9043870e+00  1.4361783e-03  8.6649328e-01\n",
            "   0.0000000e+00  2.6424980e-01 -2.0850872e-01  1.0193976e-03]]\n",
            "Reward: [0.06873593]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [ 0.7069919 50.985058 ]\n",
            "Action taken: [ 0.7069919 50.985058 ]\n",
            "Observation: [[ 1.6942092e+00  1.9442290e+00 -6.9281349e+00  7.0059717e-02\n",
            "   0.0000000e+00 -1.0192538e+00 -2.0627862e-01  1.0089169e-03]]\n",
            "Reward: [0.13886634]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [0.95469   3.0477664]\n",
            "Action taken: [0.95469   3.0477664]\n",
            "Observation: [[ 1.5544248e+00  1.8825405e+00 -4.8989573e+00  6.9352180e-02\n",
            "   0.0000000e+00 -1.0733646e+00 -2.0411859e-01  9.9875184e-04]]\n",
            "Reward: [0.14020097]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [ 0.963488 42.136223]\n",
            "Action taken: [ 0.963488 42.136223]\n",
            "Observation: [[ 1.4273878e+00  1.8286525e+00 -3.9999890e+00  6.8665653e-02\n",
            "   0.0000000e+00 -2.0380800e+00 -2.0202503e-01  9.8888704e-04]]\n",
            "Reward: [0.1414652]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [  0.98969084 -23.326212  ]\n",
            "Action taken: [  0.98969084 -23.326212  ]\n",
            "Observation: [[ 1.3101273e+00  1.7812072e+00 -3.4640951e+00  6.7999125e-02\n",
            "   0.0000000e+00 -1.4189129e+00 -1.9999458e-01  9.7930792e-04]]\n",
            "Reward: [0.14264928]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [ 0.597986 54.11012 ]\n",
            "Action taken: [ 0.597986 54.11012 ]\n",
            "Observation: [[ 1.2004529e+00  1.7391551e+00 -3.0983827e+00  6.7351632e-02\n",
            "   0.0000000e+00 -2.5413888e+00 -1.9802414e-01  9.7000098e-04]]\n",
            "Reward: [0.1437444]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [ 0.25578952 14.274653  ]\n",
            "Action taken: [ 0.25578952 14.274653  ]\n",
            "Observation: [[ 1.0967064e+00  1.7016698e+00 -2.8284245e+00  6.6722289e-02\n",
            "   0.0000000e+00 -2.6493268e+00 -1.9611083e-01  9.6095359e-04]]\n",
            "Reward: [0.14474267]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [  0.20742986 -12.855875  ]\n",
            "Action taken: [  0.20742986 -12.855875  ]\n",
            "Observation: [[ 9.9760586e-01  1.6680914e+00 -2.6186128e+00  6.6110261e-02\n",
            "   0.0000000e+00 -2.2370636e+00 -1.9425192e-01  9.5215382e-04]]\n",
            "Reward: [0.14563733]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [  0.28771216 -56.070522  ]\n",
            "Action taken: [  0.28771216 -56.070522  ]\n",
            "Observation: [[ 9.0214288e-01  1.6378851e+00 -2.4494884e+00  6.5514781e-02\n",
            "   0.0000000e+00 -1.0465270e+00 -1.9244489e-01  9.4359065e-04]]\n",
            "Reward: [0.14642267]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [  0.12221459 -21.597454  ]\n",
            "Action taken: [  0.12221459 -21.597454  ]\n",
            "Observation: [[ 8.0951297e-01  1.6106126e+00 -2.3094003e+00  6.4935103e-02\n",
            "   0.0000000e+00 -5.8909631e-01 -1.9068737e-01  9.3525363e-04]]\n",
            "Reward: [0.14709416]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [ 0.85104454 -7.9795823 ]\n",
            "Action taken: [ 0.85104454 -7.9795823 ]\n",
            "Observation: [[ 7.1906793e-01  1.5859100e+00 -2.1908896e+00  6.4370543e-02\n",
            "   0.0000000e+00 -4.1745916e-01 -1.8897715e-01  9.2713290e-04]]\n",
            "Reward: [0.14764845]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [ 0.9154222 16.99503  ]\n",
            "Action taken: [ 0.9154222 16.99503  ]\n",
            "Observation: [[ 6.3028169e-01  1.5634733e+00 -2.0889316e+00  6.3820459e-02\n",
            "   0.0000000e+00 -7.6592660e-01 -1.8731213e-01  9.1921940e-04]]\n",
            "Reward: [0.14808342]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [  0.86782575 -23.188414  ]\n",
            "Action taken: [  0.86782575 -23.188414  ]\n",
            "Observation: [[ 5.4272640e-01  1.5430459e+00 -1.9999999e+00  6.3284241e-02\n",
            "   0.0000000e+00 -2.7196088e-01 -1.8569034e-01  9.1150432e-04]]\n",
            "Reward: [0.14839813]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [  0.05032205 -11.768784  ]\n",
            "Action taken: [  0.05032205 -11.768784  ]\n",
            "Observation: [[ 4.5605582e-01  1.5244092e+00 -1.9215379e+00  6.2761314e-02\n",
            "   0.0000000e+00 -2.0394415e-02 -1.8410999e-01  9.0397958e-04]]\n",
            "Reward: [0.1485928]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [  0.78741825 -31.281694  ]\n",
            "Action taken: [  0.78741825 -31.281694  ]\n",
            "Observation: [[ 3.6999178e-01  1.5073760e+00 -1.8516403e+00  6.2251139e-02\n",
            "   0.0000000e+00  6.4585757e-01 -1.8256930e-01  8.9663744e-04]]\n",
            "Reward: [0.14866872]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [0.33510795 3.2736907 ]\n",
            "Action taken: [0.33510795 3.2736907 ]\n",
            "Observation: [[ 2.8431553e-01  1.4917847e+00 -1.7888546e+00  6.1753210e-02\n",
            "   0.0000000e+00  5.6894225e-01 -1.8106663e-01  8.8947063e-04]]\n",
            "Reward: [0.14862828]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [  0.8669332 -12.762331 ]\n",
            "Action taken: [  0.8669332 -12.762331 ]\n",
            "Observation: [[ 1.9886152e-01  1.4774953e+00 -1.7320510e+00  6.1267037e-02\n",
            "   0.0000000e+00  8.3530521e-01 -1.7960049e-01  8.8247232e-04]]\n",
            "Reward: [0.14847471]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.33984804 19.934324  ]\n",
            "Action taken: [ 0.33984804 19.934324  ]\n",
            "Observation: [[ 1.1351154e-01  1.4643856e+00 -1.6803365e+00  6.0792170e-02\n",
            "   0.0000000e+00  3.9701357e-01 -1.7816940e-01  8.7563595e-04]]\n",
            "Reward: [0.14821212]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [ 0.91387135 48.06726   ]\n",
            "Action taken: [ 0.91387135 48.06726   ]\n",
            "Observation: [[ 2.8191388e-02  1.4523476e+00 -1.6329936e+00  6.0328178e-02\n",
            "   0.0000000e+00 -6.5091014e-01 -1.7677197e-01  8.6895534e-04]]\n",
            "Reward: [0.14784536]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [  0.80014575 -12.974813  ]\n",
            "Action taken: [  0.80014575 -12.974813  ]\n",
            "Observation: [[-5.7131398e-02  1.4412870e+00 -1.5894393e+00  5.9874650e-02\n",
            "   0.0000000e+00 -3.6177945e-01 -1.7540692e-01  8.6242473e-04]]\n",
            "Reward: [0.1473798]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [  0.44519332 -57.299503  ]\n",
            "Action taken: [  0.44519332 -57.299503  ]\n",
            "Observation: [[-1.4245118e-01  1.4311202e+00 -1.5491937e+00  5.9431199e-02\n",
            "   0.0000000e+00  8.9753783e-01 -1.7407301e-01  8.5603859e-04]]\n",
            "Reward: [0.14682135]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [ 0.5788971 46.950718 ]\n",
            "Action taken: [ 0.5788971 46.950718 ]\n",
            "Observation: [[-2.2772501e-01  1.4217722e+00 -1.5118583e+00  5.8997456e-02\n",
            "   0.0000000e+00 -1.4463383e-01 -1.7276908e-01  8.4979157e-04]]\n",
            "Reward: [0.14617626]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [ 0.4223005 20.942654 ]\n",
            "Action taken: [ 0.4223005 20.942654 ]\n",
            "Observation: [[-3.1287602e-01  1.4131770e+00 -1.4770983e+00  5.8573075e-02\n",
            "   0.0000000e+00 -6.0729438e-01 -1.7149402e-01  8.4367878e-04]]\n",
            "Reward: [0.14545101]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [ 0.37375483 17.470314  ]\n",
            "Action taken: [ 0.37375483 17.470314  ]\n",
            "Observation: [[-3.9779523e-01  1.4052746e+00 -1.4446307e+00  5.8157723e-02\n",
            "   0.0000000e+00 -9.8605305e-01 -1.7024677e-01  8.3769549e-04]]\n",
            "Reward: [0.14465222]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [ 0.18039332 -7.6277637 ]\n",
            "Action taken: [ 0.18039332 -7.6277637 ]\n",
            "Observation: [[-4.8234382e-01  1.3980118e+00 -1.4142141e+00  5.7751082e-02\n",
            "   0.0000000e+00 -8.0518013e-01 -1.6902636e-01  8.3183707e-04]]\n",
            "Reward: [0.14378653]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [ 0.819844 12.657101]\n",
            "Action taken: [ 0.819844 12.657101]\n",
            "Observation: [[-5.6635725e-01  1.3913400e+00 -1.3856412e+00  5.7352852e-02\n",
            "   0.0000000e+00 -1.0742275e+00 -1.6783181e-01  8.2609936e-04]]\n",
            "Reward: [0.14286059]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [ 0.46762678 -7.591777  ]\n",
            "Action taken: [ 0.46762678 -7.591777  ]\n",
            "Observation: [[-6.4964831e-01  1.3852160e+00 -1.3587329e+00  5.6962751e-02\n",
            "   0.0000000e+00 -8.9230180e-01 -1.6666223e-01  8.2047819e-04]]\n",
            "Reward: [0.14188088]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [  0.9683885 -45.058174 ]\n",
            "Action taken: [  0.9683885 -45.058174 ]\n",
            "Observation: [[-7.3201132e-01  1.3795996e+00 -1.3333338e+00  5.6580499e-02\n",
            "   0.0000000e+00  1.2248494e-01 -1.6551678e-01  8.1496965e-04]]\n",
            "Reward: [0.14085369]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [ 0.18908495 58.429096  ]\n",
            "Action taken: [ 0.18908495 58.429096  ]\n",
            "Observation: [[-8.1322694e-01  1.3744549e+00 -1.3093079e+00  5.6205843e-02\n",
            "   0.0000000e+00 -1.1840901e+00 -1.6439462e-01  8.0957013e-04]]\n",
            "Reward: [0.1397851]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [ 0.5529569 59.586487 ]\n",
            "Action taken: [ 0.5529569 59.586487 ]\n",
            "Observation: [[-8.9306712e-01  1.3697491e+00 -1.2865356e+00  5.5838533e-02\n",
            "   0.0000000e+00 -2.4171615e+00 -1.6329499e-01  8.0427597e-04]]\n",
            "Reward: [0.13868089]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [ 0.4619011 45.176857 ]\n",
            "Action taken: [ 0.4619011 45.176857 ]\n",
            "Observation: [[-9.7130078e-01  1.3654523e+00 -1.2649117e+00  5.5478331e-02\n",
            "   0.0000000e+00 -3.1571884e+00 -1.6221713e-01  7.9908379e-04]]\n",
            "Reward: [0.13754655]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [ 0.8545015 34.286793 ]\n",
            "Action taken: [ 0.8545015 34.286793 ]\n",
            "Observation: [[-1.0476987e+00  1.3615370e+00 -1.2443427e+00  5.5125009e-02\n",
            "   0.0000000e+00 -3.5193768e+00 -1.6116032e-01  7.9399045e-04]]\n",
            "Reward: [0.13638724]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [  0.41531926 -46.522667  ]\n",
            "Action taken: [  0.41531926 -46.522667  ]\n",
            "Observation: [[-1.1220396e+00  1.3579777e+00 -1.2247455e+00  5.4778356e-02\n",
            "   0.0000000e+00 -2.5134985e+00 -1.6012391e-01  7.8899274e-04]]\n",
            "Reward: [0.13520777]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [  0.07480267 -34.734467  ]\n",
            "Action taken: [  0.07480267 -34.734467  ]\n",
            "Observation: [[-1.1941144e+00  1.3547515e+00 -1.2060460e+00  5.4438159e-02\n",
            "   0.0000000e+00 -1.8266501e+00 -1.5910724e-01  7.8408781e-04]]\n",
            "Reward: [0.13401258]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [ 0.7612069 15.110138 ]\n",
            "Action taken: [ 0.7612069 15.110138 ]\n",
            "Observation: [[-1.2637317e+00  1.3518372e+00 -1.1881777e+00  5.4104224e-02\n",
            "   0.0000000e+00 -2.0324023e+00 -1.5810969e-01  7.7927270e-04]]\n",
            "Reward: [0.13280581]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [ 0.98949456 25.497618  ]\n",
            "Action taken: [ 0.98949456 25.497618  ]\n",
            "Observation: [[-1.3307214e+00  1.3492143e+00 -1.1710807e+00  5.3776361e-02\n",
            "   0.0000000e+00 -2.3812187e+00 -1.5713069e-01  7.7454478e-04]]\n",
            "Reward: [0.1315912]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [ 0.8267217 59.9103   ]\n",
            "Action taken: [ 0.8267217 59.9103   ]\n",
            "Observation: [[-1.3949375e+00  1.3468651e+00 -1.1547011e+00  5.3454384e-02\n",
            "   0.0000000e+00 -2.9576497e+00 -1.5616964e-01  7.6990144e-04]]\n",
            "Reward: [0.13037223]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [  0.9441775 -29.245249 ]\n",
            "Action taken: [  0.9441775 -29.245249 ]\n",
            "Observation: [[-1.4562612e+00  1.3447725e+00 -1.1389902e+00  5.3138122e-02\n",
            "   0.0000000e+00 -2.3809333e+00 -1.5522601e-01  7.6534011e-04]]\n",
            "Reward: [0.12915199]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [  0.47888523 -10.875937  ]\n",
            "Action taken: [  0.47888523 -10.875937  ]\n",
            "Observation: [[-1.5146016e+00  1.3429208e+00 -1.1239036e+00  5.2827410e-02\n",
            "   0.0000000e+00 -2.1358528e+00 -1.5429927e-01  7.6085853e-04]]\n",
            "Reward: [0.12793328]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 0.7269542 28.485455 ]\n",
            "Action taken: [ 0.7269542 28.485455 ]\n",
            "Observation: [[-1.5698968e+00  1.3412956e+00 -1.1094010e+00  5.2522086e-02\n",
            "   0.0000000e+00 -2.4703991e+00 -1.5338895e-01  7.5645430e-04]]\n",
            "Reward: [0.12671863]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [ 4.345965e-03 -5.005672e+00]\n",
            "Action taken: [ 4.345965e-03 -5.005672e+00]\n",
            "Observation: [[-1.6221130e+00  1.3398832e+00 -1.0954458e+00  5.2221995e-02\n",
            "   0.0000000e+00 -2.3074296e+00 -1.5249455e-01  7.5212528e-04]]\n",
            "Reward: [0.12551026]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [  0.35676265 -58.955627  ]\n",
            "Action taken: [  0.35676265 -58.955627  ]\n",
            "Observation: [[-1.6712438e+00  1.3386710e+00 -1.0820042e+00  5.1926989e-02\n",
            "   0.0000000e+00 -1.4185497e+00 -1.5161560e-01  7.4786932e-04]]\n",
            "Reward: [-9.237092]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [  0.67567503 -50.38167   ]\n",
            "Action taken: [  0.67567503 -50.38167   ]\n",
            "Environment reset\n",
            "Observation: [[ 1.6923544  -0.01376218  0.9147326   1.3356948   0.         -0.4478551\n",
            "   5.385074   -9.433028  ]]\n",
            "Reward: [0.01938651]\n",
            "iteration: 88\n",
            "Done: [ True]\n",
            "Environment reset\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0') for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv())\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "R172XbXX5Am5",
        "outputId": "c3121d36-8d19-421e-de6a-1a26d03f966f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([ 27., 445.,  10.,  10.,   0., 355.,   1.,   1.], dtype=float32), {})\n",
            "Action taken: [  0.66004217 -12.0962925 ]\n",
            "Observation: [ 27.      445.       10.        0.        0.      385.24072   0.\n",
            "   1.     ]\n",
            "Reward: 2.01\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.6016889 32.372963 ]\n",
            "Observation: [ 52.      445.       10.        0.        0.      304.30832   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [  0.17336531 -56.311985  ]\n",
            "Observation: [ 77.     445.      10.       0.       0.     445.0883   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [ 0.19099507 59.82043   ]\n",
            "Observation: [102.     445.      10.       0.       0.     295.5372   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [  0.2484102 -16.875475 ]\n",
            "Observation: [127.     445.      10.       0.       0.     337.7259   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [  0.30493116 -20.778341  ]\n",
            "Observation: [152.      445.       10.        0.        0.      389.67175   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [  0.8561414 -31.87609  ]\n",
            "Observation: [177.      445.       10.        0.        0.      469.36197   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [ 0.9072654 26.886889 ]\n",
            "Observation: [202.      445.       10.        0.        0.      402.14474   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.8929637 14.996436 ]\n",
            "Observation: [227.      445.       10.        0.        0.      364.65366   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [ 0.47929013 -4.8493624 ]\n",
            "Observation: [252.      445.       10.        0.        0.      376.77707   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [  0.9917547 -40.350822 ]\n",
            "Observation: [277.     445.      10.       0.       0.     477.6541   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.02770284 -7.1651936 ]\n",
            "Observation: [302.     445.      10.       0.       0.     495.5671   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [ 0.6805457 16.982347 ]\n",
            "Observation: [327.      445.       10.        0.        0.      453.11124   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [ 0.5744178 13.01097  ]\n",
            "Observation: [352.     445.      10.       0.       0.     420.5838   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [ 0.7886371 17.087025 ]\n",
            "Observation: [377.      445.       10.        0.        0.      377.86624   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [  0.35461393 -47.470867  ]\n",
            "Observation: [402.      445.       10.        0.        0.      496.54343   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.44390455 19.387302  ]\n",
            "Observation: [427.      445.       10.        0.        0.      448.07516   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [  0.8794741 -27.131998 ]\n",
            "Observation: [452.      445.       10.        0.        0.      515.90515   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [0.05155954 0.58159256]\n",
            "Observation: [477.     445.      10.       0.       0.     514.4512   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [ 0.8775724 -0.9023846]\n",
            "Observation: [502.      445.       10.        0.        0.      516.70715   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [  0.49742207 -28.982546  ]\n",
            "Observation: [527.     445.      10.       0.       0.     589.1635   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [ 0.1035682 12.406218 ]\n",
            "Observation: [552.      445.       10.        0.        0.      558.14795   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [0.86385316 7.107284  ]\n",
            "Observation: [577.      445.       10.        0.        0.      540.37976   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [ 0.64350146 58.33064   ]\n",
            "Observation: [602.      445.       10.        0.        0.      394.55316   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [ 0.23401365 24.44776   ]\n",
            "Observation: [627.      445.       10.        0.        0.      333.43375   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.79426074 10.603076  ]\n",
            "Observation: [652.      445.       10.        0.        0.      306.92606   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [  0.44762865 -14.880095  ]\n",
            "Observation: [677.     445.      10.       0.       0.     344.1263   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [  0.6042116 -24.273357 ]\n",
            "Observation: [702.     445.      10.       0.       0.     404.8097   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [ 0.7496413 36.375916 ]\n",
            "Observation: [727.     445.      10.       0.       0.     313.8699   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [  0.4191699 -50.629482 ]\n",
            "Observation: [752.     445.      10.       0.       0.     440.4436   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [1.947008e-02 3.543862e+01]\n",
            "Observation: [777.      445.       10.        0.        0.      351.84705   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [ 0.9611383 36.124035 ]\n",
            "Observation: [802.      445.       10.        0.        0.      261.53696   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [  0.07286284 -58.002594  ]\n",
            "Observation: [827.      445.       10.        0.        0.      406.54346   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [  0.3934275 -40.82572  ]\n",
            "Observation: [852.      445.       10.        0.        0.      508.60776   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [ 0.8704625 49.190323 ]\n",
            "Observation: [877.      445.       10.        0.        0.      385.63196   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [0.561186 7.387314]\n",
            "Observation: [902.      445.       10.        0.        0.      367.16367   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [ 0.32931307 25.309465  ]\n",
            "Observation: [927.   445.    10.     0.     0.   303.89   0.     1.  ]\n",
            "Reward: 0.01\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [ 0.56520957 10.719815  ]\n",
            "Observation: [952.      445.       10.        0.        0.      277.09045   0.\n",
            "   1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [ 0.5748961 14.009982 ]\n",
            "Observation: [977.     445.      10.       0.       0.     242.0655   0.       1.    ]\n",
            "Reward: 0.01\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.26835954 58.31985   ]\n",
            "Observation: [1.0020000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 9.6265884e+01 0.0000000e+00 1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [0.16140041 5.157737  ]\n",
            "Observation: [1.027000e+03 4.450000e+02 1.000000e+01 0.000000e+00 0.000000e+00\n",
            " 8.337154e+01 0.000000e+00 1.000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [  0.9029685 -44.54996  ]\n",
            "Observation: [1.0520000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 1.9474644e+02 0.0000000e+00 1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [ 0.6429723 30.903906 ]\n",
            "Observation: [1.0770000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 1.1748668e+02 0.0000000e+00 1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [ 0.90543264 34.740734  ]\n",
            "Observation: [1.1020000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 3.0634842e+01 0.0000000e+00 1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [ 0.8871223 44.53777  ]\n",
            "Observation: [1.127e+03 4.450e+02 1.000e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [  0.17305751 -50.204914  ]\n",
            "Observation: [1.1520000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 1.2551228e+02 0.0000000e+00 1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [ 0.890163 27.76302 ]\n",
            "Observation: [ 1.1730000e+03  4.4500000e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  5.6104736e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [ 0.04089537 13.769283  ]\n",
            "Observation: [ 1.1477576e+03  4.2657767e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  2.1681528e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.611958 40.88406 ]\n",
            "Observation: [ 1.1225151e+03  4.0815533e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [  0.8605651 -12.566361 ]\n",
            "Observation: [ 1.0972727e+03  3.8973297e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  3.1415903e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [  0.82686865 -54.258476  ]\n",
            "Observation: [ 1.0720303e+03  3.7131064e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  1.6706209e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [ 0.9610811 21.361584 ]\n",
            "Observation: [ 1.04678784e+03  3.52888306e+02 -1.00969696e+01 -7.36893559e+00\n",
            "  0.00000000e+00  1.13658134e+02  0.00000000e+00  1.00000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [ 0.40173435 55.187473  ]\n",
            "Observation: [ 1.0215455e+03  3.3446597e+02 -1.0096970e+01 -7.3689356e+00\n",
            "  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.02\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [ 0.8714568 38.83945  ]\n",
            "Observation: [996.30304   316.0436    -10.09697    -7.3689356   0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.7483297 -6.765873 ]\n",
            "Observation: [971.0606    297.62128   -10.09697    -7.3689356   0.         16.914682\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [3.3985067e-02 5.1750313e+01]\n",
            "Observation: [945.8182    279.19894   -10.09697    -7.3689356   0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [  0.2641018 -44.5191   ]\n",
            "Observation: [920.57574   260.7766    -10.09697    -7.3689356   0.        111.29775\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [  0.65447223 -32.241463  ]\n",
            "Observation: [895.3334    242.35426   -10.09697    -7.3689356   0.        191.90141\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [  0.82597345 -58.6785    ]\n",
            "Observation: [870.09094   223.93193   -10.09697    -7.3689356   0.        338.59766\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [  0.6956803 -42.065315 ]\n",
            "Observation: [844.8485    205.50958   -10.09697    -7.3689356   0.        443.76096\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [3.506612e-03 3.714232e+01]\n",
            "Observation: [819.6061    187.08725   -10.09697    -7.3689356   0.        350.90515\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [ 0.89997226 32.411953  ]\n",
            "Observation: [794.36365   168.66492   -10.09697    -7.3689356   0.        269.87527\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [ 0.9098884 22.386229 ]\n",
            "Observation: [769.1212    150.24257   -10.09697    -7.3689356   0.        213.9097\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [ 0.29441318 11.757505  ]\n",
            "Observation: [743.8788    131.82024   -10.09697    -7.3689356   0.        184.51593\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [  0.06370922 -32.887627  ]\n",
            "Observation: [718.6364    113.39789   -10.09697    -7.3689356   0.        266.735\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [  0.30056944 -35.05654   ]\n",
            "Observation: [693.394      94.975555  -10.09697    -7.3689356   0.        354.37634\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [ 0.8168944 36.9628   ]\n",
            "Observation: [668.15155    76.553215  -10.09697    -7.3689356   0.        261.96936\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [ 0.4294897 -1.9275916]\n",
            "Observation: [642.9091     58.130875  -10.09697    -7.3689356   0.        266.78833\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [  0.5413103 -57.6651   ]\n",
            "Observation: [617.6667     39.708534  -10.09697    -7.3689356   0.        410.95108\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [  0.458609 -28.330044]\n",
            "Observation: [592.42426    21.286194  -10.09697    -7.3689356   0.        481.77618\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [  0.42246065 -33.52298   ]\n",
            "Observation: [567.1818     12.        -10.09697     7.3689356   0.        565.5836\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [  0.9786949 -22.861292 ]\n",
            "Observation: [541.93945    30.422338  -10.09697     7.3689356   0.        620.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 71\n",
            "Done: False\n",
            "Action taken: [0.40061554 5.630842  ]\n",
            "Observation: [516.697      48.844677  -10.09697     7.3689356   0.        605.9229\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 72\n",
            "Done: False\n",
            "Action taken: [  0.5549091 -49.350258 ]\n",
            "Observation: [491.4546     67.26702   -10.09697     7.3689356   0.        620.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 73\n",
            "Done: False\n",
            "Action taken: [  0.41688526 -52.729996  ]\n",
            "Observation: [466.21216    85.689354  -10.09697     7.3689356   0.        620.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 74\n",
            "Done: False\n",
            "Action taken: [  0.52007335 -43.45395   ]\n",
            "Observation: [440.96973   104.111694  -10.09697     7.3689356   0.        620.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 75\n",
            "Done: False\n",
            "Action taken: [0.2132981 3.0134518]\n",
            "Observation: [415.72733   122.534035  -10.09697     7.3689356   0.        612.4664\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 76\n",
            "Done: False\n",
            "Action taken: [ 0.16374542 38.306274  ]\n",
            "Observation: [390.4849    140.95638   -10.09697     7.3689356   0.        516.7007\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 77\n",
            "Done: False\n",
            "Action taken: [ 0.59504855 10.68945   ]\n",
            "Observation: [365.24246   159.37871   -10.09697     7.3689356   0.        489.97705\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 78\n",
            "Done: False\n",
            "Action taken: [ 0.73597056 17.184456  ]\n",
            "Observation: [340.00003   177.80106   -10.09697     7.3689356   0.        447.01593\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 79\n",
            "Done: False\n",
            "Action taken: [  0.25900003 -40.641476  ]\n",
            "Observation: [314.75763   196.22339   -10.09697     7.3689356   0.        548.6196\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 80\n",
            "Done: False\n",
            "Action taken: [  0.9657021 -29.25572  ]\n",
            "Observation: [289.5152    214.64574   -10.09697     7.3689356   0.        620.\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 81\n",
            "Done: False\n",
            "Action taken: [ 0.9475983 15.761537 ]\n",
            "Observation: [264.27277   233.06807   -10.09697     7.3689356   0.        580.5961\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 82\n",
            "Done: False\n",
            "Action taken: [ 0.78417754 45.991547  ]\n",
            "Observation: [239.03035   251.49042   -10.09697     7.3689356   0.        465.61728\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 83\n",
            "Done: False\n",
            "Action taken: [  0.4024079 -21.241508 ]\n",
            "Observation: [213.78793   269.91275   -10.09697     7.3689356   0.        518.72107\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 84\n",
            "Done: False\n",
            "Action taken: [  0.9270104 -40.461636 ]\n",
            "Observation: [188.5455    288.33508   -10.09697     7.3689356   0.        619.8751\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 85\n",
            "Done: False\n",
            "Action taken: [ 0.9274638 56.40798  ]\n",
            "Observation: [163.30309   306.75742   -10.09697     7.3689356   0.        478.8552\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 86\n",
            "Done: False\n",
            "Action taken: [  0.70330936 -10.491436  ]\n",
            "Observation: [138.06067   325.17978   -10.09697     7.3689356   0.        505.0838\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 87\n",
            "Done: False\n",
            "Action taken: [ 0.69631034 -9.384991  ]\n",
            "Observation: [112.81824   343.6021    -10.09697     7.3689356   0.        528.54626\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 88\n",
            "Done: False\n",
            "Action taken: [ 0.20885968 56.433613  ]\n",
            "Observation: [ 87.57581   362.02444   -10.09697     7.3689356   0.        387.46222\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 89\n",
            "Done: False\n",
            "Action taken: [0.9998988 8.300835 ]\n",
            "Observation: [ 62.333393  380.44678   -10.09697     7.3689356   0.        366.71014\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 90\n",
            "Done: False\n",
            "Action taken: [5.1127067e-03 3.8409809e+01]\n",
            "Observation: [ 37.09097   398.86914   -10.09697     7.3689356   0.        270.6856\n",
            "   0.          1.       ]\n",
            "Reward: 0.02\n",
            "iteration: 91\n",
            "Done: False\n",
            "Action taken: [ 0.5391464 50.75502  ]\n",
            "Observation: [ 27.       417.29147   17.787313   9.578651   0.       143.79807\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 92\n",
            "Done: False\n",
            "Action taken: [  0.62014204 -17.05076   ]\n",
            "Observation: [ 71.468285 441.2381    17.787313   9.578651   0.       186.42497\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 93\n",
            "Done: False\n",
            "Action taken: [ 0.70770055 -2.6782289 ]\n",
            "Observation: [115.93657  465.18472   17.787313   9.578651   0.       193.12054\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 94\n",
            "Done: False\n",
            "Action taken: [ 0.91665536 27.340904  ]\n",
            "Observation: [160.40485  489.13135   17.787313   9.578651   0.       124.76829\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 95\n",
            "Done: False\n",
            "Action taken: [ 0.6253478 18.506973 ]\n",
            "Observation: [204.87314  513.078     17.787313   9.578651   0.        78.500854\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 96\n",
            "Done: False\n",
            "Action taken: [  0.7830454 -12.64164  ]\n",
            "Observation: [249.34142  537.0246    17.787313   9.578651   0.       110.10495\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 97\n",
            "Done: False\n",
            "Action taken: [  0.15944175 -49.76146   ]\n",
            "Observation: [293.8097   560.97125   17.787313   9.578651   0.       234.5086\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 98\n",
            "Done: False\n",
            "Action taken: [ 0.6632079 -3.4367027]\n",
            "Observation: [338.27798  584.91785   17.787313   9.578651   0.       243.10036\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 99\n",
            "Done: False\n",
            "Action taken: [  0.9520688 -21.054678 ]\n",
            "Observation: [382.74628  608.8645    17.787313   9.578651   0.       295.73706\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 100\n",
            "Done: False\n",
            "Action taken: [ 0.293828 -9.97032 ]\n",
            "Observation: [427.21454  632.8111    17.787313   9.578651   0.       320.66284\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 101\n",
            "Done: False\n",
            "Action taken: [ 0.16020028 36.154713  ]\n",
            "Observation: [471.68283  656.75775   17.787313   9.578651   0.       230.27606\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 102\n",
            "Done: False\n",
            "Action taken: [ 0.910267 42.423668]\n",
            "Observation: [516.1511   680.7044    17.787313   9.578651   0.       124.216896\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 103\n",
            "Done: False\n",
            "Action taken: [0.8483035 5.4387746]\n",
            "Observation: [560.6194   704.651     17.787313   9.578651   0.       110.619965\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 104\n",
            "Done: False\n",
            "Action taken: [ 0.66121364 49.69681   ]\n",
            "Observation: [605.0877   728.59766   17.787313   9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 105\n",
            "Done: False\n",
            "Action taken: [ 0.16102639 33.91872   ]\n",
            "Observation: [649.55597  752.54425   17.787313   9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 106\n",
            "Done: False\n",
            "Action taken: [0.08263209 5.2096043 ]\n",
            "Observation: [694.02423  776.4909    17.787313   9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 107\n",
            "Done: False\n",
            "Action taken: [  0.47504613 -16.134737  ]\n",
            "Observation: [738.49255  788.        17.787313  -9.578651   0.        40.33684\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 108\n",
            "Done: False\n",
            "Action taken: [ 0.83201927 24.56313   ]\n",
            "Observation: [782.9608   764.05334   17.787313  -9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 109\n",
            "Done: False\n",
            "Action taken: [ 0.9492851 18.670565 ]\n",
            "Observation: [827.4291   740.10675   17.787313  -9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 110\n",
            "Done: False\n",
            "Action taken: [  0.22543126 -57.09329   ]\n",
            "Observation: [871.8974   716.1601    17.787313  -9.578651   0.       142.73322\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 111\n",
            "Done: False\n",
            "Action taken: [0.88368833 7.912627  ]\n",
            "Observation: [916.36566  692.2135    17.787313  -9.578651   0.       122.95165\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 112\n",
            "Done: False\n",
            "Action taken: [ 0.92868686 56.935047  ]\n",
            "Observation: [960.8339   668.26685   17.787313  -9.578651   0.         0.\n",
            "   0.         1.      ]\n",
            "Reward: 0.03\n",
            "iteration: 113\n",
            "Done: False\n",
            "Action taken: [  0.77939266 -54.977055  ]\n",
            "Observation: [ 1.00530225e+03  6.44320251e+02  1.77873135e+01 -9.57865143e+00\n",
            "  0.00000000e+00  1.37442642e+02  0.00000000e+00  1.00000000e+00]\n",
            "Reward: 0.03\n",
            "iteration: 114\n",
            "Done: False\n",
            "Action taken: [ 0.33200535 46.880096  ]\n",
            "Observation: [ 1.0497705e+03  6.2037360e+02  1.7787313e+01 -9.5786514e+00\n",
            "  0.0000000e+00  2.0242395e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.03\n",
            "iteration: 115\n",
            "Done: False\n",
            "Action taken: [  0.5610377 -55.85524  ]\n",
            "Observation: [ 1.0942388e+03  5.9642694e+02  1.7787313e+01 -9.5786514e+00\n",
            "  0.0000000e+00  1.5988049e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.03\n",
            "iteration: 116\n",
            "Done: False\n",
            "Action taken: [ 1.8832970e-02 -3.7389423e+01]\n",
            "Observation: [ 1.1387070e+03  5.7248035e+02  1.7787313e+01 -9.5786514e+00\n",
            "  0.0000000e+00  2.5335405e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.03\n",
            "iteration: 117\n",
            "Done: False\n",
            "Action taken: [  0.11142926 -39.913418  ]\n",
            "Observation: [ 1.1730000e+03  5.4853369e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  3.5313760e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 118\n",
            "Done: False\n",
            "Action taken: [ 0.7559601 24.322298 ]\n",
            "Observation: [ 1.1540793e+03  5.1998425e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  2.9233185e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 119\n",
            "Done: False\n",
            "Action taken: [ 0.32368493 26.663046  ]\n",
            "Observation: [ 1.1351587e+03  4.9143478e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  2.2567424e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 120\n",
            "Done: False\n",
            "Action taken: [ 0.51537955 16.59414   ]\n",
            "Observation: [ 1.1162380e+03  4.6288531e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  1.8418889e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 121\n",
            "Done: False\n",
            "Action taken: [ 0.60464 25.77196]\n",
            "Observation: [ 1.09731738e+03  4.34335876e+02 -7.56825733e+00 -1.14197845e+01\n",
            "  0.00000000e+00  1.19758995e+02  0.00000000e+00  1.00000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 122\n",
            "Done: False\n",
            "Action taken: [ 0.73325944 51.338287  ]\n",
            "Observation: [ 1.0783967e+03  4.0578641e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 123\n",
            "Done: False\n",
            "Action taken: [ 0.36284918 -0.39660802]\n",
            "Observation: [ 1.0594762e+03  3.7723694e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  9.9152005e-01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 124\n",
            "Done: False\n",
            "Action taken: [  0.04207942 -30.06015   ]\n",
            "Observation: [ 1.0405555e+03  3.4868750e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  7.6141899e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 125\n",
            "Done: False\n",
            "Action taken: [  0.58951974 -51.038357  ]\n",
            "Observation: [ 1.0216348e+03  3.2013803e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  2.0373779e+02  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 126\n",
            "Done: False\n",
            "Action taken: [ 0.67436105 25.50147   ]\n",
            "Observation: [ 1.00271423e+03  2.91588562e+02 -7.56825733e+00 -1.14197845e+01\n",
            "  0.00000000e+00  1.39984116e+02  0.00000000e+00  1.00000000e+00]\n",
            "Reward: 0.04\n",
            "iteration: 127\n",
            "Done: False\n",
            "Action taken: [  0.9468544 -48.12611  ]\n",
            "Observation: [983.7936    263.0391     -7.5682573 -11.419785    0.        260.29938\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 128\n",
            "Done: False\n",
            "Action taken: [  0.5767156 -56.054752 ]\n",
            "Observation: [964.8729    234.48964    -7.5682573 -11.419785    0.        400.43628\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 129\n",
            "Done: False\n",
            "Action taken: [  0.36145976 -52.137226  ]\n",
            "Observation: [945.9523    205.94019    -7.5682573 -11.419785    0.        530.77936\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 130\n",
            "Done: False\n",
            "Action taken: [ 0.7487949 11.120232 ]\n",
            "Observation: [927.0316    177.39072    -7.5682573 -11.419785    0.        502.97876\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 131\n",
            "Done: False\n",
            "Action taken: [ 0.4249728 46.224815 ]\n",
            "Observation: [908.11096   148.84126    -7.5682573 -11.419785    0.        387.41672\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 132\n",
            "Done: False\n",
            "Action taken: [  0.2165947 -38.430077 ]\n",
            "Observation: [889.19037   120.291794   -7.5682573 -11.419785    0.        483.4919\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 133\n",
            "Done: False\n",
            "Action taken: [ 0.16742203 48.70815   ]\n",
            "Observation: [870.2697     91.74233    -7.5682573 -11.419785    0.        361.72153\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 134\n",
            "Done: False\n",
            "Action taken: [ 0.41817895 39.203045  ]\n",
            "Observation: [851.34906    63.19287    -7.5682573 -11.419785    0.        263.71393\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 135\n",
            "Done: False\n",
            "Action taken: [0.06043824 2.3373294 ]\n",
            "Observation: [832.4284     34.64341    -7.5682573 -11.419785    0.        257.8706\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 136\n",
            "Done: False\n",
            "Action taken: [  0.9356033 -53.673332 ]\n",
            "Observation: [813.50775    12.         -7.5682573  11.419785    0.        392.05392\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 137\n",
            "Done: False\n",
            "Action taken: [ 0.12567045 46.92522   ]\n",
            "Observation: [794.5871     40.54946    -7.5682573  11.419785    0.        274.74088\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 138\n",
            "Done: False\n",
            "Action taken: [ 0.45411512 54.618984  ]\n",
            "Observation: [775.6665     69.09892    -7.5682573  11.419785    0.        138.19342\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 139\n",
            "Done: False\n",
            "Action taken: [  0.79724264 -34.851734  ]\n",
            "Observation: [756.74585    97.648384   -7.5682573  11.419785    0.        225.32275\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 140\n",
            "Done: False\n",
            "Action taken: [ 0.19145773 -8.209693  ]\n",
            "Observation: [737.8252    126.197845   -7.5682573  11.419785    0.        245.84698\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 141\n",
            "Done: False\n",
            "Action taken: [0.75670034 5.359939  ]\n",
            "Observation: [718.90454   154.7473     -7.5682573  11.419785    0.        232.44714\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 142\n",
            "Done: False\n",
            "Action taken: [0.2202877 4.979797 ]\n",
            "Observation: [699.9839    183.29677    -7.5682573  11.419785    0.        219.99765\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 143\n",
            "Done: False\n",
            "Action taken: [  0.99019647 -27.202223  ]\n",
            "Observation: [681.06323   211.84622    -7.5682573  11.419785    0.        288.0032\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 144\n",
            "Done: False\n",
            "Action taken: [ 0.33359832 38.9382    ]\n",
            "Observation: [662.14264   240.39569    -7.5682573  11.419785    0.        190.6577\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 145\n",
            "Done: False\n",
            "Action taken: [ 0.83910036 38.080616  ]\n",
            "Observation: [643.222     268.94516    -7.5682573  11.419785    0.         95.456154\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 146\n",
            "Done: False\n",
            "Action taken: [ 0.8389611 16.012104 ]\n",
            "Observation: [624.30133   297.4946     -7.5682573  11.419785    0.         55.425896\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 147\n",
            "Done: False\n",
            "Action taken: [  0.8471047 -51.965015 ]\n",
            "Observation: [605.3807    326.04407    -7.5682573  11.419785    0.        185.33844\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 148\n",
            "Done: False\n",
            "Action taken: [  0.69261605 -27.54127   ]\n",
            "Observation: [586.46      354.59354    -7.5682573  11.419785    0.        254.1916\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 149\n",
            "Done: False\n",
            "Action taken: [ 0.37743777 49.044216  ]\n",
            "Observation: [567.53937   383.143      -7.5682573  11.419785    0.        131.58107\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 150\n",
            "Done: False\n",
            "Action taken: [1.3250953e-02 5.3378944e+01]\n",
            "Observation: [548.6188    411.69244    -7.5682573  11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 151\n",
            "Done: False\n",
            "Action taken: [  0.71728104 -42.711147  ]\n",
            "Observation: [529.6981    440.2419     -7.5682573  11.419785    0.        106.77787\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 152\n",
            "Done: False\n",
            "Action taken: [ 0.3305665 40.410877 ]\n",
            "Observation: [510.77747   468.79138    -7.5682573  11.419785    0.          5.750675\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 153\n",
            "Done: False\n",
            "Action taken: [0.11123039 1.997441  ]\n",
            "Observation: [491.8568     497.34085     -7.5682573   11.419785     0.\n",
            "   0.75707257   0.           1.        ]\n",
            "Reward: 0.04\n",
            "iteration: 154\n",
            "Done: False\n",
            "Action taken: [ 0.363094 54.307446]\n",
            "Observation: [472.9362    525.8903     -7.5682573  11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 155\n",
            "Done: False\n",
            "Action taken: [  0.23901981 -57.71519   ]\n",
            "Observation: [454.01553   554.43976    -7.5682573  11.419785    0.        144.28798\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 156\n",
            "Done: False\n",
            "Action taken: [ 0.33501238 26.8364    ]\n",
            "Observation: [435.09488   582.9892     -7.5682573  11.419785    0.         77.196976\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 157\n",
            "Done: False\n",
            "Action taken: [ 0.7315888 51.324833 ]\n",
            "Observation: [416.17426   611.5387     -7.5682573  11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 158\n",
            "Done: False\n",
            "Action taken: [  0.6586793 -56.38425  ]\n",
            "Observation: [397.2536    640.08813    -7.5682573  11.419785    0.        140.96063\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 159\n",
            "Done: False\n",
            "Action taken: [ 0.9995826 42.039608 ]\n",
            "Observation: [378.33295   668.63763    -7.5682573  11.419785    0.         35.861607\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 160\n",
            "Done: False\n",
            "Action taken: [ 0.11196275 37.98291   ]\n",
            "Observation: [359.41232   697.1871     -7.5682573  11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 161\n",
            "Done: False\n",
            "Action taken: [ 0.04810156 12.807378  ]\n",
            "Observation: [340.49167   725.7365     -7.5682573  11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 162\n",
            "Done: False\n",
            "Action taken: [ 0.88434666 -2.7201622 ]\n",
            "Observation: [321.571     754.286      -7.5682573  11.419785    0.          6.8004055\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 163\n",
            "Done: False\n",
            "Action taken: [  0.5085844 -10.876834 ]\n",
            "Observation: [302.6504    782.83545    -7.5682573  11.419785    0.         33.99249\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 164\n",
            "Done: False\n",
            "Action taken: [  0.34768292 -47.94351   ]\n",
            "Observation: [283.72974   788.         -7.5682573 -11.419785    0.        153.85126\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 165\n",
            "Done: False\n",
            "Action taken: [ 0.13880873 21.90151   ]\n",
            "Observation: [264.80908   759.45056    -7.5682573 -11.419785    0.         99.09749\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 166\n",
            "Done: False\n",
            "Action taken: [ 0.9459039 25.030851 ]\n",
            "Observation: [245.88846   730.90106    -7.5682573 -11.419785    0.         36.520355\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 167\n",
            "Done: False\n",
            "Action taken: [  0.14859323 -28.394798  ]\n",
            "Observation: [226.9678    702.3516     -7.5682573 -11.419785    0.        107.507355\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 168\n",
            "Done: False\n",
            "Action taken: [ 0.8806356 38.037167 ]\n",
            "Observation: [208.04716   673.8022     -7.5682573 -11.419785    0.         12.414435\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 169\n",
            "Done: False\n",
            "Action taken: [ 0.76613975 15.168422  ]\n",
            "Observation: [189.12653   645.2527     -7.5682573 -11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 170\n",
            "Done: False\n",
            "Action taken: [ 0.079788 42.949425]\n",
            "Observation: [170.20587   616.70325    -7.5682573 -11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 171\n",
            "Done: False\n",
            "Action taken: [  0.35954344 -47.29926   ]\n",
            "Observation: [151.28523   588.15375    -7.5682573 -11.419785    0.        118.248146\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 172\n",
            "Done: False\n",
            "Action taken: [ 0.5942078 35.825333 ]\n",
            "Observation: [132.3646    559.6043     -7.5682573 -11.419785    0.         28.684816\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 173\n",
            "Done: False\n",
            "Action taken: [ 0.9619654 -9.106527 ]\n",
            "Observation: [113.44395   531.0549     -7.5682573 -11.419785    0.         51.451134\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 174\n",
            "Done: False\n",
            "Action taken: [ 0.9072778 49.27296  ]\n",
            "Observation: [ 94.5233    502.5054     -7.5682573 -11.419785    0.          0.\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 175\n",
            "Done: False\n",
            "Action taken: [  0.7888971 -33.958042 ]\n",
            "Observation: [ 75.60265   473.95593    -7.5682573 -11.419785    0.         84.8951\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 176\n",
            "Done: False\n",
            "Action taken: [  0.30359855 -36.552513  ]\n",
            "Observation: [ 56.68201   445.40646    -7.5682573 -11.419785    0.        176.27638\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 177\n",
            "Done: False\n",
            "Action taken: [ 0.38372612 32.74041   ]\n",
            "Observation: [ 37.761368  416.857      -7.5682573 -11.419785    0.         94.42536\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 178\n",
            "Done: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: [ 0.14228591 20.377926  ]\n",
            "Observation: [ 18.840725  388.30756    -7.5682573 -11.419785    0.         43.48055\n",
            "   0.          1.       ]\n",
            "Reward: 0.04\n",
            "iteration: 179\n",
            "Done: False\n",
            "Action taken: [0.43817136 5.5411897 ]\n",
            "Observation: [-7.9919219e-02  3.5975809e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  2.9627575e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: -0.7506938320819745\n",
            "iteration: 180\n",
            "Done: False\n",
            "Action taken: [ 0.33756116 17.187254  ]\n",
            "Observation: [-7.9919219e-02  3.5975809e+02 -7.5682573e+00 -1.1419785e+01\n",
            "  0.0000000e+00  2.9627575e+01  0.0000000e+00  1.0000000e+00]\n",
            "Reward: 0.01\n",
            "iteration: 181\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ],
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
        "                                   log_path='./logs/results', eval_freq=2000,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1)\n",
        "# model = PPO.load(\"ppo_custom_pong\")\n",
        "model.learn(total_timesteps=500000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(\"ppo_custom_pong_1\")\n",
        "train_env.save(\"vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")"
      ],
      "metadata": {
        "id": "_sjyFuJcGT-w",
        "outputId": "571c4b7a-4021-424f-8e88-ba81975b5a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "collapsed": true
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-1c9fddca2224>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# model = PPO.load(\"ppo_custom_pong\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Save the model and the normalization statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_features_extractor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mpi_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mall\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mare\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlatent_policy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlatent_value\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train\n",
        "\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize.load(\"vecnormalize.pkl\", train_env)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "# Sync normalization statistics from the training environment to the evaluation environment\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "eval_env.ret_rms = train_env.ret_rms\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
        "                                   log_path='./logs/results', eval_freq=2000,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(\"ppo_custom_pong_1\", env=train_env)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=1000000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(\"ppo_custom_pong_1\")\n",
        "train_env.save(\"vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Mean Reward per Evaluation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mL_LYMHeejpF",
        "outputId": "93aba9ec-3b0f-4f3b-ac1a-63c639b57e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=3.36 +/- 3.97\n",
            "Episode length: 146.20 +/- 63.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 146      |\n",
            "|    mean_reward     | 3.36     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward 3.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 138      |\n",
            "|    ep_rew_mean     | 2.19     |\n",
            "| time/              |          |\n",
            "|    fps             | 1619     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=1.99 +/- 2.65\n",
            "Episode length: 114.40 +/- 46.65\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 114         |\n",
            "|    mean_reward          | 1.99        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019285705 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.00172     |\n",
            "|    explained_variance   | 0.931       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0288      |\n",
            "|    n_updates            | 2970        |\n",
            "|    policy_gradient_loss | -0.00666    |\n",
            "|    std                  | 0.242       |\n",
            "|    value_loss           | 0.0457      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 4000: mean reward 1.99\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 3.9      |\n",
            "| time/              |          |\n",
            "|    fps             | 1008     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 16       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=2.95 +/- 3.43\n",
            "Episode length: 148.40 +/- 43.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 148         |\n",
            "|    mean_reward          | 2.95        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023067355 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0363      |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00928    |\n",
            "|    n_updates            | 2980        |\n",
            "|    policy_gradient_loss | -0.0053     |\n",
            "|    std                  | 0.239       |\n",
            "|    value_loss           | 0.0584      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 6000: mean reward 2.95\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.83     |\n",
            "| time/              |          |\n",
            "|    fps             | 898      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 27       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=1.51 +/- 3.59\n",
            "Episode length: 139.80 +/- 31.44\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 140         |\n",
            "|    mean_reward          | 1.51        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020099537 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0448      |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00137     |\n",
            "|    n_updates            | 2990        |\n",
            "|    policy_gradient_loss | -0.00449    |\n",
            "|    std                  | 0.238       |\n",
            "|    value_loss           | 0.0496      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 8000: mean reward 1.51\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 156      |\n",
            "|    ep_rew_mean     | 3.14     |\n",
            "| time/              |          |\n",
            "|    fps             | 842      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=1.19 +/- 2.76\n",
            "Episode length: 108.00 +/- 37.21\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 108         |\n",
            "|    mean_reward          | 1.19        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022245657 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0632      |\n",
            "|    explained_variance   | 0.914       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00856     |\n",
            "|    n_updates            | 3000        |\n",
            "|    policy_gradient_loss | -0.00874    |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 0.0458      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward 1.19\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 2.74     |\n",
            "| time/              |          |\n",
            "|    fps             | 812      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 50       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3.56 +/- 4.76\n",
            "Episode length: 153.20 +/- 69.62\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 153         |\n",
            "|    mean_reward          | 3.56        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017894922 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0689      |\n",
            "|    explained_variance   | 0.905       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.013      |\n",
            "|    n_updates            | 3010        |\n",
            "|    policy_gradient_loss | -0.00312    |\n",
            "|    std                  | 0.235       |\n",
            "|    value_loss           | 0.0436      |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 12000: mean reward 3.56\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 3.7      |\n",
            "| time/              |          |\n",
            "|    fps             | 788      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=2.77 +/- 4.94\n",
            "Episode length: 134.60 +/- 74.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 135         |\n",
            "|    mean_reward          | 2.77        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021048255 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0794      |\n",
            "|    explained_variance   | 0.844       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0273      |\n",
            "|    n_updates            | 3020        |\n",
            "|    policy_gradient_loss | -0.00614    |\n",
            "|    std                  | 0.234       |\n",
            "|    value_loss           | 0.0666      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward 2.77\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 139      |\n",
            "|    ep_rew_mean     | 2.45     |\n",
            "| time/              |          |\n",
            "|    fps             | 778      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=1.88 +/- 2.94\n",
            "Episode length: 114.00 +/- 45.49\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 114         |\n",
            "|    mean_reward          | 1.88        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019287385 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0757      |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0454      |\n",
            "|    n_updates            | 3030        |\n",
            "|    policy_gradient_loss | -0.00732    |\n",
            "|    std                  | 0.235       |\n",
            "|    value_loss           | 0.047       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 16000: mean reward 1.88\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 2.85     |\n",
            "| time/              |          |\n",
            "|    fps             | 769      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3.37 +/- 1.57\n",
            "Episode length: 169.20 +/- 43.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 169         |\n",
            "|    mean_reward          | 3.37        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 72000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020221842 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0986      |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0523      |\n",
            "|    n_updates            | 3040        |\n",
            "|    policy_gradient_loss | -0.00388    |\n",
            "|    std                  | 0.231       |\n",
            "|    value_loss           | 0.0468      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 18000: mean reward 3.37\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 2.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 761      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 96       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=1.55 +/- 3.24\n",
            "Episode length: 126.00 +/- 45.76\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 126         |\n",
            "|    mean_reward          | 1.55        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022043161 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.113       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00552    |\n",
            "|    n_updates            | 3050        |\n",
            "|    policy_gradient_loss | -0.00588    |\n",
            "|    std                  | 0.23        |\n",
            "|    value_loss           | 0.0471      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 20000: mean reward 1.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.6      |\n",
            "| time/              |          |\n",
            "|    fps             | 758      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=2.30 +/- 3.32\n",
            "Episode length: 142.00 +/- 39.83\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 142        |\n",
            "|    mean_reward          | 2.3        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 88000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02041314 |\n",
            "|    clip_fraction        | 0.208      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.12       |\n",
            "|    explained_variance   | 0.903      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.03      |\n",
            "|    n_updates            | 3060       |\n",
            "|    policy_gradient_loss | -0.00469   |\n",
            "|    std                  | 0.229      |\n",
            "|    value_loss           | 0.0467     |\n",
            "----------------------------------------\n",
            "Evaluation at step 22000: mean reward 2.30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 2.66     |\n",
            "| time/              |          |\n",
            "|    fps             | 756      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=12.18 +/- 8.97\n",
            "Episode length: 307.00 +/- 127.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 307         |\n",
            "|    mean_reward          | 12.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022805415 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.124       |\n",
            "|    explained_variance   | 0.928       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00144    |\n",
            "|    n_updates            | 3070        |\n",
            "|    policy_gradient_loss | -0.0062     |\n",
            "|    std                  | 0.229       |\n",
            "|    value_loss           | 0.0395      |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 24000: mean reward 12.18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 2.42     |\n",
            "| time/              |          |\n",
            "|    fps             | 748      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 131      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=4.54 +/- 6.25\n",
            "Episode length: 161.60 +/- 94.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 162         |\n",
            "|    mean_reward          | 4.54        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 104000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019446101 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.136       |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.000641    |\n",
            "|    n_updates            | 3080        |\n",
            "|    policy_gradient_loss | -0.00532    |\n",
            "|    std                  | 0.227       |\n",
            "|    value_loss           | 0.0433      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 26000: mean reward 4.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 139      |\n",
            "|    ep_rew_mean     | 3.33     |\n",
            "| time/              |          |\n",
            "|    fps             | 746      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 142      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=112000, episode_reward=4.67 +/- 6.49\n",
            "Episode length: 193.40 +/- 67.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 193         |\n",
            "|    mean_reward          | 4.67        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 112000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022991091 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.135       |\n",
            "|    explained_variance   | 0.901       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0287     |\n",
            "|    n_updates            | 3090        |\n",
            "|    policy_gradient_loss | -0.00448    |\n",
            "|    std                  | 0.228       |\n",
            "|    value_loss           | 0.0625      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 28000: mean reward 4.67\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 2.5      |\n",
            "| time/              |          |\n",
            "|    fps             | 743      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 154      |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=2.82 +/- 1.94\n",
            "Episode length: 145.60 +/- 53.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 146         |\n",
            "|    mean_reward          | 2.82        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 120000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021500181 |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.117       |\n",
            "|    explained_variance   | 0.926       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.02       |\n",
            "|    n_updates            | 3100        |\n",
            "|    policy_gradient_loss | -0.00662    |\n",
            "|    std                  | 0.23        |\n",
            "|    value_loss           | 0.0426      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 30000: mean reward 2.82\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 3.62     |\n",
            "| time/              |          |\n",
            "|    fps             | 742      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 165      |\n",
            "|    total_timesteps | 122880   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=128000, episode_reward=2.46 +/- 3.91\n",
            "Episode length: 161.00 +/- 16.15\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 161        |\n",
            "|    mean_reward          | 2.46       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 128000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02073865 |\n",
            "|    clip_fraction        | 0.209      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.133      |\n",
            "|    explained_variance   | 0.869      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0092    |\n",
            "|    n_updates            | 3110       |\n",
            "|    policy_gradient_loss | -0.00521   |\n",
            "|    std                  | 0.227      |\n",
            "|    value_loss           | 0.0597     |\n",
            "----------------------------------------\n",
            "Evaluation at step 32000: mean reward 2.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 4.14     |\n",
            "| time/              |          |\n",
            "|    fps             | 738      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 177      |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=136000, episode_reward=1.66 +/- 4.05\n",
            "Episode length: 110.00 +/- 51.24\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 110         |\n",
            "|    mean_reward          | 1.66        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 136000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020263648 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.124       |\n",
            "|    explained_variance   | 0.917       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00331     |\n",
            "|    n_updates            | 3120        |\n",
            "|    policy_gradient_loss | -0.00421    |\n",
            "|    std                  | 0.23        |\n",
            "|    value_loss           | 0.0529      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 34000: mean reward 1.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 161      |\n",
            "|    ep_rew_mean     | 3.4      |\n",
            "| time/              |          |\n",
            "|    fps             | 734      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 189      |\n",
            "|    total_timesteps | 139264   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=144000, episode_reward=6.05 +/- 4.99\n",
            "Episode length: 191.00 +/- 96.07\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 191        |\n",
            "|    mean_reward          | 6.05       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 144000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01953026 |\n",
            "|    clip_fraction        | 0.207      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.121      |\n",
            "|    explained_variance   | 0.94       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0636     |\n",
            "|    n_updates            | 3130       |\n",
            "|    policy_gradient_loss | -0.00481   |\n",
            "|    std                  | 0.229      |\n",
            "|    value_loss           | 0.049      |\n",
            "----------------------------------------\n",
            "Evaluation at step 36000: mean reward 6.05\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 157      |\n",
            "|    ep_rew_mean     | 3.07     |\n",
            "| time/              |          |\n",
            "|    fps             | 732      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 201      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=152000, episode_reward=2.15 +/- 2.41\n",
            "Episode length: 121.60 +/- 34.91\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 122        |\n",
            "|    mean_reward          | 2.15       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 152000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02120651 |\n",
            "|    clip_fraction        | 0.206      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.131      |\n",
            "|    explained_variance   | 0.873      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0032     |\n",
            "|    n_updates            | 3140       |\n",
            "|    policy_gradient_loss | -0.00493   |\n",
            "|    std                  | 0.227      |\n",
            "|    value_loss           | 0.0467     |\n",
            "----------------------------------------\n",
            "Evaluation at step 38000: mean reward 2.15\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 159      |\n",
            "|    ep_rew_mean     | 3.34     |\n",
            "| time/              |          |\n",
            "|    fps             | 733      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 212      |\n",
            "|    total_timesteps | 155648   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=6.10 +/- 7.07\n",
            "Episode length: 205.20 +/- 86.68\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 205         |\n",
            "|    mean_reward          | 6.1         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 160000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020839088 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.132       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0414      |\n",
            "|    n_updates            | 3150        |\n",
            "|    policy_gradient_loss | -0.00687    |\n",
            "|    std                  | 0.227       |\n",
            "|    value_loss           | 0.0515      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 40000: mean reward 6.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 732      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 223      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=168000, episode_reward=5.63 +/- 8.90\n",
            "Episode length: 173.80 +/- 122.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 174         |\n",
            "|    mean_reward          | 5.63        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 168000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018617576 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.142       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0019      |\n",
            "|    n_updates            | 3160        |\n",
            "|    policy_gradient_loss | -0.00549    |\n",
            "|    std                  | 0.226       |\n",
            "|    value_loss           | 0.0386      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 42000: mean reward 5.63\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 3.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 731      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 235      |\n",
            "|    total_timesteps | 172032   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=176000, episode_reward=1.62 +/- 3.37\n",
            "Episode length: 126.60 +/- 43.28\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 127        |\n",
            "|    mean_reward          | 1.62       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 176000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01992188 |\n",
            "|    clip_fraction        | 0.218      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.158      |\n",
            "|    explained_variance   | 0.882      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0247     |\n",
            "|    n_updates            | 3170       |\n",
            "|    policy_gradient_loss | -0.00432   |\n",
            "|    std                  | 0.224      |\n",
            "|    value_loss           | 0.0895     |\n",
            "----------------------------------------\n",
            "Evaluation at step 44000: mean reward 1.62\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 4.07     |\n",
            "| time/              |          |\n",
            "|    fps             | 730      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 246      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=184000, episode_reward=4.36 +/- 5.51\n",
            "Episode length: 163.40 +/- 93.84\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 163         |\n",
            "|    mean_reward          | 4.36        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 184000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018318517 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.17        |\n",
            "|    explained_variance   | 0.899       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0331     |\n",
            "|    n_updates            | 3180        |\n",
            "|    policy_gradient_loss | -0.00559    |\n",
            "|    std                  | 0.222       |\n",
            "|    value_loss           | 0.0415      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 46000: mean reward 4.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 143      |\n",
            "|    ep_rew_mean     | 2.58     |\n",
            "| time/              |          |\n",
            "|    fps             | 729      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 258      |\n",
            "|    total_timesteps | 188416   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=192000, episode_reward=5.33 +/- 8.42\n",
            "Episode length: 160.80 +/- 111.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 161         |\n",
            "|    mean_reward          | 5.33        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 192000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020388281 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.181       |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00471    |\n",
            "|    n_updates            | 3190        |\n",
            "|    policy_gradient_loss | -0.00782    |\n",
            "|    std                  | 0.221       |\n",
            "|    value_loss           | 0.0416      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 48000: mean reward 5.33\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 139      |\n",
            "|    ep_rew_mean     | 2.55     |\n",
            "| time/              |          |\n",
            "|    fps             | 728      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 269      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=2.76 +/- 2.92\n",
            "Episode length: 134.40 +/- 35.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 134         |\n",
            "|    mean_reward          | 2.76        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 200000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020471202 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.188       |\n",
            "|    explained_variance   | 0.928       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.024       |\n",
            "|    n_updates            | 3200        |\n",
            "|    policy_gradient_loss | -0.0048     |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 0.053       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 50000: mean reward 2.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 155      |\n",
            "|    ep_rew_mean     | 3.06     |\n",
            "| time/              |          |\n",
            "|    fps             | 727      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 281      |\n",
            "|    total_timesteps | 204800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=208000, episode_reward=2.49 +/- 4.27\n",
            "Episode length: 168.20 +/- 29.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 168         |\n",
            "|    mean_reward          | 2.49        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 208000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021745294 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.183       |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0183      |\n",
            "|    n_updates            | 3210        |\n",
            "|    policy_gradient_loss | -0.00574    |\n",
            "|    std                  | 0.222       |\n",
            "|    value_loss           | 0.0407      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 52000: mean reward 2.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 3.16     |\n",
            "| time/              |          |\n",
            "|    fps             | 726      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 293      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=216000, episode_reward=2.38 +/- 5.45\n",
            "Episode length: 140.60 +/- 71.60\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 141        |\n",
            "|    mean_reward          | 2.38       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 216000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02092551 |\n",
            "|    clip_fraction        | 0.22       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.191      |\n",
            "|    explained_variance   | 0.924      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0304     |\n",
            "|    n_updates            | 3220       |\n",
            "|    policy_gradient_loss | -0.0046    |\n",
            "|    std                  | 0.22       |\n",
            "|    value_loss           | 0.0475     |\n",
            "----------------------------------------\n",
            "Evaluation at step 54000: mean reward 2.38\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 2.44     |\n",
            "| time/              |          |\n",
            "|    fps             | 727      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 304      |\n",
            "|    total_timesteps | 221184   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=224000, episode_reward=2.31 +/- 4.45\n",
            "Episode length: 138.40 +/- 46.85\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 138         |\n",
            "|    mean_reward          | 2.31        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 224000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021233657 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.181       |\n",
            "|    explained_variance   | 0.932       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0438      |\n",
            "|    n_updates            | 3230        |\n",
            "|    policy_gradient_loss | -0.0038     |\n",
            "|    std                  | 0.222       |\n",
            "|    value_loss           | 0.0476      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 56000: mean reward 2.31\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.13     |\n",
            "| time/              |          |\n",
            "|    fps             | 727      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 315      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=232000, episode_reward=4.66 +/- 5.48\n",
            "Episode length: 188.60 +/- 65.75\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 189        |\n",
            "|    mean_reward          | 4.66       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 232000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02106855 |\n",
            "|    clip_fraction        | 0.216      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.184      |\n",
            "|    explained_variance   | 0.929      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.00971   |\n",
            "|    n_updates            | 3240       |\n",
            "|    policy_gradient_loss | -0.00464   |\n",
            "|    std                  | 0.221      |\n",
            "|    value_loss           | 0.0487     |\n",
            "----------------------------------------\n",
            "Evaluation at step 58000: mean reward 4.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 159      |\n",
            "|    ep_rew_mean     | 3.05     |\n",
            "| time/              |          |\n",
            "|    fps             | 726      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 327      |\n",
            "|    total_timesteps | 237568   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=2.75 +/- 2.11\n",
            "Episode length: 147.00 +/- 53.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 147         |\n",
            "|    mean_reward          | 2.75        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 240000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022018213 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.196       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00972    |\n",
            "|    n_updates            | 3250        |\n",
            "|    policy_gradient_loss | -0.00613    |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 0.0507      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 60000: mean reward 2.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 155      |\n",
            "|    ep_rew_mean     | 2.8      |\n",
            "| time/              |          |\n",
            "|    fps             | 725      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 338      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=248000, episode_reward=2.74 +/- 4.20\n",
            "Episode length: 138.40 +/- 54.55\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 138         |\n",
            "|    mean_reward          | 2.74        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 248000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018935274 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.216       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0076      |\n",
            "|    n_updates            | 3260        |\n",
            "|    policy_gradient_loss | -0.00351    |\n",
            "|    std                  | 0.217       |\n",
            "|    value_loss           | 0.043       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 62000: mean reward 2.74\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 2.63     |\n",
            "| time/              |          |\n",
            "|    fps             | 726      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 349      |\n",
            "|    total_timesteps | 253952   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=256000, episode_reward=2.17 +/- 3.31\n",
            "Episode length: 125.40 +/- 40.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 125         |\n",
            "|    mean_reward          | 2.17        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 256000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020831637 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.218       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00394    |\n",
            "|    n_updates            | 3270        |\n",
            "|    policy_gradient_loss | -0.00525    |\n",
            "|    std                  | 0.218       |\n",
            "|    value_loss           | 0.0415      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 64000: mean reward 2.17\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 157      |\n",
            "|    ep_rew_mean     | 2.79     |\n",
            "| time/              |          |\n",
            "|    fps             | 725      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 361      |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=264000, episode_reward=1.84 +/- 2.91\n",
            "Episode length: 119.40 +/- 34.47\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 119         |\n",
            "|    mean_reward          | 1.84        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 264000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021126898 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.223       |\n",
            "|    explained_variance   | 0.894       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0236      |\n",
            "|    n_updates            | 3280        |\n",
            "|    policy_gradient_loss | -0.00496    |\n",
            "|    std                  | 0.217       |\n",
            "|    value_loss           | 0.0395      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 66000: mean reward 1.84\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 164      |\n",
            "|    ep_rew_mean     | 3.17     |\n",
            "| time/              |          |\n",
            "|    fps             | 725      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 372      |\n",
            "|    total_timesteps | 270336   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=272000, episode_reward=3.58 +/- 4.70\n",
            "Episode length: 170.00 +/- 61.88\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 170         |\n",
            "|    mean_reward          | 3.58        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 272000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022738732 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.228       |\n",
            "|    explained_variance   | 0.906       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0189      |\n",
            "|    n_updates            | 3290        |\n",
            "|    policy_gradient_loss | -0.0056     |\n",
            "|    std                  | 0.216       |\n",
            "|    value_loss           | 0.0417      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 68000: mean reward 3.58\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.25     |\n",
            "| time/              |          |\n",
            "|    fps             | 726      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 383      |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=4.07 +/- 4.34\n",
            "Episode length: 166.20 +/- 70.55\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 166         |\n",
            "|    mean_reward          | 4.07        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 280000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021659479 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.253       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0244      |\n",
            "|    n_updates            | 3300        |\n",
            "|    policy_gradient_loss | -0.00606    |\n",
            "|    std                  | 0.213       |\n",
            "|    value_loss           | 0.0519      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 70000: mean reward 4.07\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 3.24     |\n",
            "| time/              |          |\n",
            "|    fps             | 726      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 394      |\n",
            "|    total_timesteps | 286720   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=288000, episode_reward=3.20 +/- 5.42\n",
            "Episode length: 141.80 +/- 86.96\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 142         |\n",
            "|    mean_reward          | 3.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 288000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023572408 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.269       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0126     |\n",
            "|    n_updates            | 3310        |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    std                  | 0.212       |\n",
            "|    value_loss           | 0.0498      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 72000: mean reward 3.20\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.03     |\n",
            "| time/              |          |\n",
            "|    fps             | 725      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 406      |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=296000, episode_reward=4.55 +/- 4.82\n",
            "Episode length: 197.20 +/- 73.81\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 197         |\n",
            "|    mean_reward          | 4.55        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 296000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022524035 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.259       |\n",
            "|    explained_variance   | 0.902       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00673     |\n",
            "|    n_updates            | 3320        |\n",
            "|    policy_gradient_loss | -0.00259    |\n",
            "|    std                  | 0.214       |\n",
            "|    value_loss           | 0.0477      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 74000: mean reward 4.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 144      |\n",
            "|    ep_rew_mean     | 2.51     |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 418      |\n",
            "|    total_timesteps | 303104   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=304000, episode_reward=1.55 +/- 3.35\n",
            "Episode length: 134.20 +/- 34.41\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 134         |\n",
            "|    mean_reward          | 1.55        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 304000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024379719 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.262       |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0439      |\n",
            "|    n_updates            | 3330        |\n",
            "|    policy_gradient_loss | -0.00587    |\n",
            "|    std                  | 0.213       |\n",
            "|    value_loss           | 0.041       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 76000: mean reward 1.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 134      |\n",
            "|    ep_rew_mean     | 3.22     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 429      |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=312000, episode_reward=6.06 +/- 7.60\n",
            "Episode length: 180.60 +/- 121.31\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 181         |\n",
            "|    mean_reward          | 6.06        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 312000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024438173 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.276       |\n",
            "|    explained_variance   | 0.893       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0192      |\n",
            "|    n_updates            | 3340        |\n",
            "|    policy_gradient_loss | -0.00209    |\n",
            "|    std                  | 0.21        |\n",
            "|    value_loss           | 0.057       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 78000: mean reward 6.06\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 139      |\n",
            "|    ep_rew_mean     | 3.36     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 441      |\n",
            "|    total_timesteps | 319488   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=4.42 +/- 6.52\n",
            "Episode length: 154.20 +/- 94.93\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 154        |\n",
            "|    mean_reward          | 4.42       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 320000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02225709 |\n",
            "|    clip_fraction        | 0.222      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.307      |\n",
            "|    explained_variance   | 0.925      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.00882   |\n",
            "|    n_updates            | 3350       |\n",
            "|    policy_gradient_loss | -0.00506   |\n",
            "|    std                  | 0.207      |\n",
            "|    value_loss           | 0.0449     |\n",
            "----------------------------------------\n",
            "Evaluation at step 80000: mean reward 4.42\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 143      |\n",
            "|    ep_rew_mean     | 2.45     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 452      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=328000, episode_reward=5.20 +/- 7.28\n",
            "Episode length: 192.40 +/- 96.93\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 192         |\n",
            "|    mean_reward          | 5.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 328000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021960514 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.318       |\n",
            "|    explained_variance   | 0.922       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.018       |\n",
            "|    n_updates            | 3360        |\n",
            "|    policy_gradient_loss | -0.00364    |\n",
            "|    std                  | 0.208       |\n",
            "|    value_loss           | 0.0499      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 82000: mean reward 5.20\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 140      |\n",
            "|    ep_rew_mean     | 2.08     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 464      |\n",
            "|    total_timesteps | 335872   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=336000, episode_reward=3.48 +/- 2.58\n",
            "Episode length: 151.60 +/- 30.47\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 152         |\n",
            "|    mean_reward          | 3.48        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 336000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019585041 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.308       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0125      |\n",
            "|    n_updates            | 3370        |\n",
            "|    policy_gradient_loss | -0.00289    |\n",
            "|    std                  | 0.209       |\n",
            "|    value_loss           | 0.0544      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 84000: mean reward 3.48\n",
            "Eval num_timesteps=344000, episode_reward=1.81 +/- 3.90\n",
            "Episode length: 117.60 +/- 47.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 118      |\n",
            "|    mean_reward     | 1.81     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 344000   |\n",
            "---------------------------------\n",
            "Evaluation at step 86000: mean reward 1.81\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 2.86     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 476      |\n",
            "|    total_timesteps | 344064   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=352000, episode_reward=1.87 +/- 3.58\n",
            "Episode length: 108.00 +/- 47.87\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 108         |\n",
            "|    mean_reward          | 1.87        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 352000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021284759 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.302       |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0238      |\n",
            "|    n_updates            | 3380        |\n",
            "|    policy_gradient_loss | -0.00579    |\n",
            "|    std                  | 0.209       |\n",
            "|    value_loss           | 0.0417      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 88000: mean reward 1.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 2.93     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 487      |\n",
            "|    total_timesteps | 352256   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=2.06 +/- 2.93\n",
            "Episode length: 107.80 +/- 45.40\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 108        |\n",
            "|    mean_reward          | 2.06       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 360000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02346477 |\n",
            "|    clip_fraction        | 0.237      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.308      |\n",
            "|    explained_variance   | 0.918      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0181     |\n",
            "|    n_updates            | 3390       |\n",
            "|    policy_gradient_loss | -0.00785   |\n",
            "|    std                  | 0.208      |\n",
            "|    value_loss           | 0.0481     |\n",
            "----------------------------------------\n",
            "Evaluation at step 90000: mean reward 2.06\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 2.7      |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 498      |\n",
            "|    total_timesteps | 360448   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=368000, episode_reward=1.87 +/- 4.46\n",
            "Episode length: 141.00 +/- 52.74\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 141         |\n",
            "|    mean_reward          | 1.87        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 368000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023217872 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.325       |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00295    |\n",
            "|    n_updates            | 3400        |\n",
            "|    policy_gradient_loss | -0.00383    |\n",
            "|    std                  | 0.206       |\n",
            "|    value_loss           | 0.044       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 92000: mean reward 1.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.32     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 509      |\n",
            "|    total_timesteps | 368640   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=376000, episode_reward=3.55 +/- 1.98\n",
            "Episode length: 164.80 +/- 40.47\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 165        |\n",
            "|    mean_reward          | 3.55       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 376000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02689055 |\n",
            "|    clip_fraction        | 0.233      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.346      |\n",
            "|    explained_variance   | 0.93       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.00795   |\n",
            "|    n_updates            | 3410       |\n",
            "|    policy_gradient_loss | -0.00516   |\n",
            "|    std                  | 0.203      |\n",
            "|    value_loss           | 0.037      |\n",
            "----------------------------------------\n",
            "Evaluation at step 94000: mean reward 3.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.56     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 520      |\n",
            "|    total_timesteps | 376832   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=384000, episode_reward=3.82 +/- 4.61\n",
            "Episode length: 162.60 +/- 77.91\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 163         |\n",
            "|    mean_reward          | 3.82        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 384000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023919497 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.363       |\n",
            "|    explained_variance   | 0.941       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0179      |\n",
            "|    n_updates            | 3420        |\n",
            "|    policy_gradient_loss | -0.00113    |\n",
            "|    std                  | 0.203       |\n",
            "|    value_loss           | 0.0354      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 96000: mean reward 3.82\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 140      |\n",
            "|    ep_rew_mean     | 3.25     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 532      |\n",
            "|    total_timesteps | 385024   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=392000, episode_reward=1.29 +/- 2.81\n",
            "Episode length: 101.00 +/- 34.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 101         |\n",
            "|    mean_reward          | 1.29        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 392000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022125741 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.377       |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0086     |\n",
            "|    n_updates            | 3430        |\n",
            "|    policy_gradient_loss | -0.0028     |\n",
            "|    std                  | 0.2         |\n",
            "|    value_loss           | 0.0599      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 98000: mean reward 1.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.48     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 543      |\n",
            "|    total_timesteps | 393216   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=2.81 +/- 3.36\n",
            "Episode length: 135.60 +/- 45.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 136         |\n",
            "|    mean_reward          | 2.81        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 400000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022221256 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.396       |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0184     |\n",
            "|    n_updates            | 3440        |\n",
            "|    policy_gradient_loss | -0.00336    |\n",
            "|    std                  | 0.198       |\n",
            "|    value_loss           | 0.0403      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 100000: mean reward 2.81\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 162      |\n",
            "|    ep_rew_mean     | 3.04     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 554      |\n",
            "|    total_timesteps | 401408   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=408000, episode_reward=2.04 +/- 3.34\n",
            "Episode length: 127.80 +/- 35.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 128         |\n",
            "|    mean_reward          | 2.04        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 408000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025927123 |\n",
            "|    clip_fraction        | 0.238       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.407       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0197     |\n",
            "|    n_updates            | 3450        |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    std                  | 0.198       |\n",
            "|    value_loss           | 0.0471      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 102000: mean reward 2.04\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 162      |\n",
            "|    ep_rew_mean     | 3.08     |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 50       |\n",
            "|    time_elapsed    | 565      |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=416000, episode_reward=1.93 +/- 3.15\n",
            "Episode length: 138.60 +/- 37.21\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 139        |\n",
            "|    mean_reward          | 1.93       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 416000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02393063 |\n",
            "|    clip_fraction        | 0.229      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.404      |\n",
            "|    explained_variance   | 0.921      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0149     |\n",
            "|    n_updates            | 3460       |\n",
            "|    policy_gradient_loss | -0.00304   |\n",
            "|    std                  | 0.198      |\n",
            "|    value_loss           | 0.0453     |\n",
            "----------------------------------------\n",
            "Evaluation at step 104000: mean reward 1.93\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | 3.01     |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 51       |\n",
            "|    time_elapsed    | 576      |\n",
            "|    total_timesteps | 417792   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=424000, episode_reward=5.32 +/- 5.62\n",
            "Episode length: 167.00 +/- 89.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 167         |\n",
            "|    mean_reward          | 5.32        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 424000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024048612 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.427       |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0126     |\n",
            "|    n_updates            | 3470        |\n",
            "|    policy_gradient_loss | -0.00484    |\n",
            "|    std                  | 0.195       |\n",
            "|    value_loss           | 0.0551      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 106000: mean reward 5.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 3.04     |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 52       |\n",
            "|    time_elapsed    | 588      |\n",
            "|    total_timesteps | 425984   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=432000, episode_reward=3.94 +/- 3.73\n",
            "Episode length: 180.80 +/- 56.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 181         |\n",
            "|    mean_reward          | 3.94        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 432000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022180118 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.449       |\n",
            "|    explained_variance   | 0.928       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0213      |\n",
            "|    n_updates            | 3480        |\n",
            "|    policy_gradient_loss | -0.00336    |\n",
            "|    std                  | 0.194       |\n",
            "|    value_loss           | 0.0396      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 108000: mean reward 3.94\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 2.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 53       |\n",
            "|    time_elapsed    | 599      |\n",
            "|    total_timesteps | 434176   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=7.58 +/- 5.50\n",
            "Episode length: 236.60 +/- 119.61\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 237        |\n",
            "|    mean_reward          | 7.58       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 440000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02306585 |\n",
            "|    clip_fraction        | 0.247      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.455      |\n",
            "|    explained_variance   | 0.92       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0151    |\n",
            "|    n_updates            | 3490       |\n",
            "|    policy_gradient_loss | -0.00277   |\n",
            "|    std                  | 0.193      |\n",
            "|    value_loss           | 0.0455     |\n",
            "----------------------------------------\n",
            "Evaluation at step 110000: mean reward 7.58\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 159      |\n",
            "|    ep_rew_mean     | 3.07     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 54       |\n",
            "|    time_elapsed    | 611      |\n",
            "|    total_timesteps | 442368   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=448000, episode_reward=2.82 +/- 3.30\n",
            "Episode length: 161.80 +/- 11.39\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 162         |\n",
            "|    mean_reward          | 2.82        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 448000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026304945 |\n",
            "|    clip_fraction        | 0.239       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.478       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0362      |\n",
            "|    n_updates            | 3500        |\n",
            "|    policy_gradient_loss | -0.00527    |\n",
            "|    std                  | 0.19        |\n",
            "|    value_loss           | 0.0642      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 112000: mean reward 2.82\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 156      |\n",
            "|    ep_rew_mean     | 4.03     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 55       |\n",
            "|    time_elapsed    | 623      |\n",
            "|    total_timesteps | 450560   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=456000, episode_reward=8.13 +/- 10.58\n",
            "Episode length: 222.20 +/- 126.61\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 222         |\n",
            "|    mean_reward          | 8.13        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 456000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023096245 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.496       |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0111     |\n",
            "|    n_updates            | 3510        |\n",
            "|    policy_gradient_loss | -0.00342    |\n",
            "|    std                  | 0.19        |\n",
            "|    value_loss           | 0.0549      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 114000: mean reward 8.13\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.47     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 56       |\n",
            "|    time_elapsed    | 634      |\n",
            "|    total_timesteps | 458752   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=464000, episode_reward=2.58 +/- 2.37\n",
            "Episode length: 127.00 +/- 49.16\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 127         |\n",
            "|    mean_reward          | 2.58        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 464000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023676543 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.51        |\n",
            "|    explained_variance   | 0.914       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0208      |\n",
            "|    n_updates            | 3520        |\n",
            "|    policy_gradient_loss | -0.00268    |\n",
            "|    std                  | 0.188       |\n",
            "|    value_loss           | 0.0611      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 116000: mean reward 2.58\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.45     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 57       |\n",
            "|    time_elapsed    | 646      |\n",
            "|    total_timesteps | 466944   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=472000, episode_reward=7.40 +/- 4.27\n",
            "Episode length: 235.00 +/- 96.95\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 235         |\n",
            "|    mean_reward          | 7.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 472000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022626169 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.524       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0208      |\n",
            "|    n_updates            | 3530        |\n",
            "|    policy_gradient_loss | -0.00402    |\n",
            "|    std                  | 0.187       |\n",
            "|    value_loss           | 0.0401      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 118000: mean reward 7.40\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 135      |\n",
            "|    ep_rew_mean     | 1.92     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 58       |\n",
            "|    time_elapsed    | 658      |\n",
            "|    total_timesteps | 475136   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=2.48 +/- 3.00\n",
            "Episode length: 117.00 +/- 46.22\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 117         |\n",
            "|    mean_reward          | 2.48        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 480000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025470167 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.541       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00171     |\n",
            "|    n_updates            | 3540        |\n",
            "|    policy_gradient_loss | -0.00444    |\n",
            "|    std                  | 0.185       |\n",
            "|    value_loss           | 0.0402      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 120000: mean reward 2.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 140      |\n",
            "|    ep_rew_mean     | 2.21     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 669      |\n",
            "|    total_timesteps | 483328   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=488000, episode_reward=3.98 +/- 5.81\n",
            "Episode length: 159.60 +/- 81.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 160         |\n",
            "|    mean_reward          | 3.98        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 488000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029591285 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.571       |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0497      |\n",
            "|    n_updates            | 3550        |\n",
            "|    policy_gradient_loss | -0.0055     |\n",
            "|    std                  | 0.182       |\n",
            "|    value_loss           | 0.0458      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 122000: mean reward 3.98\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 2.61     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 60       |\n",
            "|    time_elapsed    | 680      |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=496000, episode_reward=4.68 +/- 4.36\n",
            "Episode length: 183.00 +/- 68.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 183         |\n",
            "|    mean_reward          | 4.68        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 496000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026561454 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.564       |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0341      |\n",
            "|    n_updates            | 3560        |\n",
            "|    policy_gradient_loss | -0.00471    |\n",
            "|    std                  | 0.183       |\n",
            "|    value_loss           | 0.0428      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 124000: mean reward 4.68\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.74     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 61       |\n",
            "|    time_elapsed    | 691      |\n",
            "|    total_timesteps | 499712   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=504000, episode_reward=2.71 +/- 3.94\n",
            "Episode length: 141.40 +/- 49.79\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 141        |\n",
            "|    mean_reward          | 2.71       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 504000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02548424 |\n",
            "|    clip_fraction        | 0.253      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.56       |\n",
            "|    explained_variance   | 0.868      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0094    |\n",
            "|    n_updates            | 3570       |\n",
            "|    policy_gradient_loss | -0.00301   |\n",
            "|    std                  | 0.184      |\n",
            "|    value_loss           | 0.0609     |\n",
            "----------------------------------------\n",
            "Evaluation at step 126000: mean reward 2.71\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 3.94     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 62       |\n",
            "|    time_elapsed    | 703      |\n",
            "|    total_timesteps | 507904   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=512000, episode_reward=2.59 +/- 3.32\n",
            "Episode length: 138.80 +/- 46.65\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 139         |\n",
            "|    mean_reward          | 2.59        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 512000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025858635 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.562       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00506    |\n",
            "|    n_updates            | 3580        |\n",
            "|    policy_gradient_loss | -0.00381    |\n",
            "|    std                  | 0.183       |\n",
            "|    value_loss           | 0.0477      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 128000: mean reward 2.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 3.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 63       |\n",
            "|    time_elapsed    | 714      |\n",
            "|    total_timesteps | 516096   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=2.57 +/- 2.72\n",
            "Episode length: 118.40 +/- 49.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 118         |\n",
            "|    mean_reward          | 2.57        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 520000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025771141 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.58        |\n",
            "|    explained_variance   | 0.898       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0168      |\n",
            "|    n_updates            | 3590        |\n",
            "|    policy_gradient_loss | -0.00241    |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 0.0944      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 130000: mean reward 2.57\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 3.62     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 64       |\n",
            "|    time_elapsed    | 725      |\n",
            "|    total_timesteps | 524288   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=528000, episode_reward=4.03 +/- 1.58\n",
            "Episode length: 173.40 +/- 48.57\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 173         |\n",
            "|    mean_reward          | 4.03        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 528000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024959162 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.598       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0191      |\n",
            "|    n_updates            | 3600        |\n",
            "|    policy_gradient_loss | -0.000276   |\n",
            "|    std                  | 0.18        |\n",
            "|    value_loss           | 0.0446      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 132000: mean reward 4.03\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.48     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 65       |\n",
            "|    time_elapsed    | 736      |\n",
            "|    total_timesteps | 532480   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=536000, episode_reward=3.08 +/- 3.94\n",
            "Episode length: 136.00 +/- 71.71\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 136         |\n",
            "|    mean_reward          | 3.08        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 536000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026337363 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.61        |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0514      |\n",
            "|    n_updates            | 3610        |\n",
            "|    policy_gradient_loss | -0.000939   |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0501      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 134000: mean reward 3.08\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 2.47     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 66       |\n",
            "|    time_elapsed    | 747      |\n",
            "|    total_timesteps | 540672   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=544000, episode_reward=4.95 +/- 5.87\n",
            "Episode length: 178.80 +/- 95.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 179         |\n",
            "|    mean_reward          | 4.95        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 544000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023807898 |\n",
            "|    clip_fraction        | 0.243       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.607       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0118      |\n",
            "|    n_updates            | 3620        |\n",
            "|    policy_gradient_loss | -0.00332    |\n",
            "|    std                  | 0.18        |\n",
            "|    value_loss           | 0.0542      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 136000: mean reward 4.95\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 2.81     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 67       |\n",
            "|    time_elapsed    | 759      |\n",
            "|    total_timesteps | 548864   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=552000, episode_reward=4.73 +/- 5.23\n",
            "Episode length: 176.00 +/- 79.43\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 176        |\n",
            "|    mean_reward          | 4.73       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 552000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02140025 |\n",
            "|    clip_fraction        | 0.225      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.595      |\n",
            "|    explained_variance   | 0.92       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0353     |\n",
            "|    n_updates            | 3630       |\n",
            "|    policy_gradient_loss | -0.00285   |\n",
            "|    std                  | 0.18       |\n",
            "|    value_loss           | 0.0401     |\n",
            "----------------------------------------\n",
            "Evaluation at step 138000: mean reward 4.73\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 2.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 68       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 557056   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=3.76 +/- 3.28\n",
            "Episode length: 180.80 +/- 65.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 181         |\n",
            "|    mean_reward          | 3.76        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 560000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027183477 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.598       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00923     |\n",
            "|    n_updates            | 3640        |\n",
            "|    policy_gradient_loss | -0.0037     |\n",
            "|    std                  | 0.18        |\n",
            "|    value_loss           | 0.0391      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 140000: mean reward 3.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 144      |\n",
            "|    ep_rew_mean     | 2.3      |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 69       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 565248   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=568000, episode_reward=3.69 +/- 5.50\n",
            "Episode length: 154.80 +/- 82.43\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 155        |\n",
            "|    mean_reward          | 3.69       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 568000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02833433 |\n",
            "|    clip_fraction        | 0.256      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.614      |\n",
            "|    explained_variance   | 0.932      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0103     |\n",
            "|    n_updates            | 3650       |\n",
            "|    policy_gradient_loss | -0.00157   |\n",
            "|    std                  | 0.178      |\n",
            "|    value_loss           | 0.0404     |\n",
            "----------------------------------------\n",
            "Evaluation at step 142000: mean reward 3.69\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 2.53     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 70       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=576000, episode_reward=3.21 +/- 3.51\n",
            "Episode length: 138.20 +/- 59.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 138         |\n",
            "|    mean_reward          | 3.21        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 576000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026921764 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.616       |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.021       |\n",
            "|    n_updates            | 3660        |\n",
            "|    policy_gradient_loss | -0.00437    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0394      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 144000: mean reward 3.21\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 2.67     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 71       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 581632   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=584000, episode_reward=4.16 +/- 4.40\n",
            "Episode length: 165.60 +/- 77.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 166         |\n",
            "|    mean_reward          | 4.16        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 584000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028580584 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.62        |\n",
            "|    explained_variance   | 0.922       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00202     |\n",
            "|    n_updates            | 3670        |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    std                  | 0.178       |\n",
            "|    value_loss           | 0.0325      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 146000: mean reward 4.16\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 2.71     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 72       |\n",
            "|    time_elapsed    | 817      |\n",
            "|    total_timesteps | 589824   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=592000, episode_reward=3.55 +/- 4.45\n",
            "Episode length: 169.20 +/- 56.77\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 169        |\n",
            "|    mean_reward          | 3.55       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 592000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02996543 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.617      |\n",
            "|    explained_variance   | 0.914      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0107     |\n",
            "|    n_updates            | 3680       |\n",
            "|    policy_gradient_loss | -0.00134   |\n",
            "|    std                  | 0.179      |\n",
            "|    value_loss           | 0.0304     |\n",
            "----------------------------------------\n",
            "Evaluation at step 148000: mean reward 3.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 2.74     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 73       |\n",
            "|    time_elapsed    | 827      |\n",
            "|    total_timesteps | 598016   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=2.53 +/- 4.34\n",
            "Episode length: 127.20 +/- 65.29\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 127         |\n",
            "|    mean_reward          | 2.53        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 600000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026489485 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.609       |\n",
            "|    explained_variance   | 0.905       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0318      |\n",
            "|    n_updates            | 3690        |\n",
            "|    policy_gradient_loss | -0.000171   |\n",
            "|    std                  | 0.18        |\n",
            "|    value_loss           | 0.0408      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 150000: mean reward 2.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.55     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 74       |\n",
            "|    time_elapsed    | 839      |\n",
            "|    total_timesteps | 606208   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=608000, episode_reward=5.23 +/- 7.85\n",
            "Episode length: 184.40 +/- 106.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 184         |\n",
            "|    mean_reward          | 5.23        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 608000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027161997 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.613       |\n",
            "|    explained_variance   | 0.922       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00846     |\n",
            "|    n_updates            | 3700        |\n",
            "|    policy_gradient_loss | -0.00304    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0407      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 152000: mean reward 5.23\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 155      |\n",
            "|    ep_rew_mean     | 2.72     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 75       |\n",
            "|    time_elapsed    | 850      |\n",
            "|    total_timesteps | 614400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=616000, episode_reward=1.88 +/- 2.98\n",
            "Episode length: 127.60 +/- 37.74\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 128         |\n",
            "|    mean_reward          | 1.88        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 616000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026582677 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.614       |\n",
            "|    explained_variance   | 0.902       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0154      |\n",
            "|    n_updates            | 3710        |\n",
            "|    policy_gradient_loss | -0.00316    |\n",
            "|    std                  | 0.178       |\n",
            "|    value_loss           | 0.0474      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 154000: mean reward 1.88\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 2.62     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 76       |\n",
            "|    time_elapsed    | 862      |\n",
            "|    total_timesteps | 622592   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=624000, episode_reward=2.60 +/- 4.70\n",
            "Episode length: 129.40 +/- 71.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 129         |\n",
            "|    mean_reward          | 2.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 624000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025027849 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.616       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00346    |\n",
            "|    n_updates            | 3720        |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0384      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 156000: mean reward 2.60\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 3.71     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 77       |\n",
            "|    time_elapsed    | 873      |\n",
            "|    total_timesteps | 630784   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=632000, episode_reward=4.03 +/- 5.46\n",
            "Episode length: 164.20 +/- 78.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 164         |\n",
            "|    mean_reward          | 4.03        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 632000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027518706 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.628       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0147      |\n",
            "|    n_updates            | 3730        |\n",
            "|    policy_gradient_loss | -0.00313    |\n",
            "|    std                  | 0.177       |\n",
            "|    value_loss           | 0.0606      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 158000: mean reward 4.03\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.71     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 78       |\n",
            "|    time_elapsed    | 886      |\n",
            "|    total_timesteps | 638976   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=3.07 +/- 5.32\n",
            "Episode length: 154.80 +/- 75.40\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 155        |\n",
            "|    mean_reward          | 3.07       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 640000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02573383 |\n",
            "|    clip_fraction        | 0.252      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.631      |\n",
            "|    explained_variance   | 0.881      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0162    |\n",
            "|    n_updates            | 3740       |\n",
            "|    policy_gradient_loss | -0.0043    |\n",
            "|    std                  | 0.178      |\n",
            "|    value_loss           | 0.0404     |\n",
            "----------------------------------------\n",
            "Evaluation at step 160000: mean reward 3.07\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 143      |\n",
            "|    ep_rew_mean     | 2.4      |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 79       |\n",
            "|    time_elapsed    | 897      |\n",
            "|    total_timesteps | 647168   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=648000, episode_reward=5.66 +/- 7.74\n",
            "Episode length: 197.00 +/- 101.12\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 197         |\n",
            "|    mean_reward          | 5.66        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 648000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025168335 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.636       |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00127     |\n",
            "|    n_updates            | 3750        |\n",
            "|    policy_gradient_loss | -0.00377    |\n",
            "|    std                  | 0.177       |\n",
            "|    value_loss           | 0.0345      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 162000: mean reward 5.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 2.52     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 80       |\n",
            "|    time_elapsed    | 908      |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=656000, episode_reward=3.89 +/- 3.92\n",
            "Episode length: 147.00 +/- 70.29\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 147        |\n",
            "|    mean_reward          | 3.89       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 656000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03083222 |\n",
            "|    clip_fraction        | 0.249      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.64       |\n",
            "|    explained_variance   | 0.891      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0273     |\n",
            "|    n_updates            | 3760       |\n",
            "|    policy_gradient_loss | -0.00467   |\n",
            "|    std                  | 0.177      |\n",
            "|    value_loss           | 0.0396     |\n",
            "----------------------------------------\n",
            "Evaluation at step 164000: mean reward 3.89\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 2.46     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 81       |\n",
            "|    time_elapsed    | 919      |\n",
            "|    total_timesteps | 663552   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=664000, episode_reward=3.48 +/- 2.54\n",
            "Episode length: 161.40 +/- 54.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 161         |\n",
            "|    mean_reward          | 3.48        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 664000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024680095 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.635       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0297      |\n",
            "|    n_updates            | 3770        |\n",
            "|    policy_gradient_loss | -0.00344    |\n",
            "|    std                  | 0.177       |\n",
            "|    value_loss           | 0.0408      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 166000: mean reward 3.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 2.66     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 82       |\n",
            "|    time_elapsed    | 931      |\n",
            "|    total_timesteps | 671744   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=672000, episode_reward=4.07 +/- 3.80\n",
            "Episode length: 161.20 +/- 63.06\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 161         |\n",
            "|    mean_reward          | 4.07        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 672000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025425896 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.629       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0179      |\n",
            "|    n_updates            | 3780        |\n",
            "|    policy_gradient_loss | -0.00229    |\n",
            "|    std                  | 0.178       |\n",
            "|    value_loss           | 0.0469      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 168000: mean reward 4.07\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 3.63     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 83       |\n",
            "|    time_elapsed    | 942      |\n",
            "|    total_timesteps | 679936   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=2.41 +/- 3.45\n",
            "Episode length: 157.80 +/- 56.41\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 158         |\n",
            "|    mean_reward          | 2.41        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026508953 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.643       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0456      |\n",
            "|    n_updates            | 3790        |\n",
            "|    policy_gradient_loss | -0.00276    |\n",
            "|    std                  | 0.176       |\n",
            "|    value_loss           | 0.0628      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 170000: mean reward 2.41\n",
            "Eval num_timesteps=688000, episode_reward=1.68 +/- 3.94\n",
            "Episode length: 131.00 +/- 49.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 131      |\n",
            "|    mean_reward     | 1.68     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 688000   |\n",
            "---------------------------------\n",
            "Evaluation at step 172000: mean reward 1.68\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | 4.34     |\n",
            "| time/              |          |\n",
            "|    fps             | 720      |\n",
            "|    iterations      | 84       |\n",
            "|    time_elapsed    | 954      |\n",
            "|    total_timesteps | 688128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=696000, episode_reward=2.35 +/- 2.28\n",
            "Episode length: 108.80 +/- 44.16\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 109         |\n",
            "|    mean_reward          | 2.35        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 696000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027492948 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.65        |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0202      |\n",
            "|    n_updates            | 3800        |\n",
            "|    policy_gradient_loss | -0.00263    |\n",
            "|    std                  | 0.176       |\n",
            "|    value_loss           | 0.0421      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 174000: mean reward 2.35\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 2.84     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 85       |\n",
            "|    time_elapsed    | 965      |\n",
            "|    total_timesteps | 696320   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=704000, episode_reward=1.94 +/- 2.58\n",
            "Episode length: 129.00 +/- 38.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 129         |\n",
            "|    mean_reward          | 1.94        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 704000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025050528 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.643       |\n",
            "|    explained_variance   | 0.89        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0169      |\n",
            "|    n_updates            | 3810        |\n",
            "|    policy_gradient_loss | -0.00122    |\n",
            "|    std                  | 0.177       |\n",
            "|    value_loss           | 0.0382      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 176000: mean reward 1.94\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 138      |\n",
            "|    ep_rew_mean     | 2.28     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 86       |\n",
            "|    time_elapsed    | 977      |\n",
            "|    total_timesteps | 704512   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=712000, episode_reward=1.83 +/- 3.24\n",
            "Episode length: 128.80 +/- 37.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 129         |\n",
            "|    mean_reward          | 1.83        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 712000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024669342 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.641       |\n",
            "|    explained_variance   | 0.926       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0182      |\n",
            "|    n_updates            | 3820        |\n",
            "|    policy_gradient_loss | -0.00322    |\n",
            "|    std                  | 0.176       |\n",
            "|    value_loss           | 0.0419      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 178000: mean reward 1.83\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 3.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 87       |\n",
            "|    time_elapsed    | 988      |\n",
            "|    total_timesteps | 712704   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=3.72 +/- 4.63\n",
            "Episode length: 160.00 +/- 65.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 160         |\n",
            "|    mean_reward          | 3.72        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 720000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026133027 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.621       |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0237      |\n",
            "|    n_updates            | 3830        |\n",
            "|    policy_gradient_loss | -0.0028     |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0595      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 180000: mean reward 3.72\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 3.76     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 88       |\n",
            "|    time_elapsed    | 999      |\n",
            "|    total_timesteps | 720896   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=728000, episode_reward=2.48 +/- 3.96\n",
            "Episode length: 147.80 +/- 48.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 148         |\n",
            "|    mean_reward          | 2.48        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 728000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028646225 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.612       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00497    |\n",
            "|    n_updates            | 3840        |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0385      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 182000: mean reward 2.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.54     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 89       |\n",
            "|    time_elapsed    | 1010     |\n",
            "|    total_timesteps | 729088   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=736000, episode_reward=3.50 +/- 3.92\n",
            "Episode length: 169.20 +/- 61.78\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 169         |\n",
            "|    mean_reward          | 3.5         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 736000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027180292 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.618       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0227     |\n",
            "|    n_updates            | 3850        |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0409      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 184000: mean reward 3.50\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 2.67     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 90       |\n",
            "|    time_elapsed    | 1021     |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=744000, episode_reward=4.47 +/- 5.90\n",
            "Episode length: 177.80 +/- 79.70\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 178       |\n",
            "|    mean_reward          | 4.47      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 744000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0256246 |\n",
            "|    clip_fraction        | 0.247     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.622     |\n",
            "|    explained_variance   | 0.888     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.00318  |\n",
            "|    n_updates            | 3860      |\n",
            "|    policy_gradient_loss | -0.0031   |\n",
            "|    std                  | 0.178     |\n",
            "|    value_loss           | 0.0376    |\n",
            "---------------------------------------\n",
            "Evaluation at step 186000: mean reward 4.47\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 3.12     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 91       |\n",
            "|    time_elapsed    | 1033     |\n",
            "|    total_timesteps | 745472   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=752000, episode_reward=9.74 +/- 12.02\n",
            "Episode length: 244.60 +/- 135.59\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 245         |\n",
            "|    mean_reward          | 9.74        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 752000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027228829 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.619       |\n",
            "|    explained_variance   | 0.899       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0306     |\n",
            "|    n_updates            | 3870        |\n",
            "|    policy_gradient_loss | -0.00224    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0422      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 188000: mean reward 9.74\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 155      |\n",
            "|    ep_rew_mean     | 3.01     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 92       |\n",
            "|    time_elapsed    | 1044     |\n",
            "|    total_timesteps | 753664   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=4.68 +/- 2.70\n",
            "Episode length: 194.40 +/- 57.28\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 194         |\n",
            "|    mean_reward          | 4.68        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 760000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026580548 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.621       |\n",
            "|    explained_variance   | 0.917       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0431      |\n",
            "|    n_updates            | 3880        |\n",
            "|    policy_gradient_loss | -0.00315    |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 0.0476      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 190000: mean reward 4.68\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | 2.68     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 93       |\n",
            "|    time_elapsed    | 1056     |\n",
            "|    total_timesteps | 761856   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=768000, episode_reward=6.32 +/- 6.94\n",
            "Episode length: 216.20 +/- 75.96\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 216         |\n",
            "|    mean_reward          | 6.32        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 768000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024415854 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.625       |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.000701    |\n",
            "|    n_updates            | 3890        |\n",
            "|    policy_gradient_loss | -0.002      |\n",
            "|    std                  | 0.178       |\n",
            "|    value_loss           | 0.0411      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 192000: mean reward 6.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 161      |\n",
            "|    ep_rew_mean     | 3.57     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 94       |\n",
            "|    time_elapsed    | 1067     |\n",
            "|    total_timesteps | 770048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=776000, episode_reward=1.77 +/- 3.72\n",
            "Episode length: 122.00 +/- 45.36\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 122        |\n",
            "|    mean_reward          | 1.77       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 776000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02291905 |\n",
            "|    clip_fraction        | 0.237      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.617      |\n",
            "|    explained_variance   | 0.892      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0145    |\n",
            "|    n_updates            | 3900       |\n",
            "|    policy_gradient_loss | -0.00144   |\n",
            "|    std                  | 0.18       |\n",
            "|    value_loss           | 0.057      |\n",
            "----------------------------------------\n",
            "Evaluation at step 194000: mean reward 1.77\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 163      |\n",
            "|    ep_rew_mean     | 3.37     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 95       |\n",
            "|    time_elapsed    | 1078     |\n",
            "|    total_timesteps | 778240   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=784000, episode_reward=3.27 +/- 2.64\n",
            "Episode length: 160.80 +/- 43.92\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 161         |\n",
            "|    mean_reward          | 3.27        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 784000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026893415 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.602       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00437    |\n",
            "|    n_updates            | 3910        |\n",
            "|    policy_gradient_loss | -0.00546    |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 0.0448      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 196000: mean reward 3.27\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 159      |\n",
            "|    ep_rew_mean     | 3.1      |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 96       |\n",
            "|    time_elapsed    | 1089     |\n",
            "|    total_timesteps | 786432   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=792000, episode_reward=3.80 +/- 4.39\n",
            "Episode length: 162.00 +/- 60.41\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 162         |\n",
            "|    mean_reward          | 3.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 792000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027180035 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.6         |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0258     |\n",
            "|    n_updates            | 3920        |\n",
            "|    policy_gradient_loss | -0.00349    |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 0.0419      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 198000: mean reward 3.80\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 155      |\n",
            "|    ep_rew_mean     | 2.92     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 97       |\n",
            "|    time_elapsed    | 1101     |\n",
            "|    total_timesteps | 794624   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=2.87 +/- 4.12\n",
            "Episode length: 139.80 +/- 65.16\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 140         |\n",
            "|    mean_reward          | 2.87        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 800000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024285365 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.617       |\n",
            "|    explained_variance   | 0.905       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00691     |\n",
            "|    n_updates            | 3930        |\n",
            "|    policy_gradient_loss | -0.00334    |\n",
            "|    std                  | 0.178       |\n",
            "|    value_loss           | 0.045       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 200000: mean reward 2.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 3.84     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 98       |\n",
            "|    time_elapsed    | 1113     |\n",
            "|    total_timesteps | 802816   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=808000, episode_reward=2.44 +/- 4.00\n",
            "Episode length: 138.40 +/- 49.50\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 138         |\n",
            "|    mean_reward          | 2.44        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 808000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028980605 |\n",
            "|    clip_fraction        | 0.27        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.642       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0234      |\n",
            "|    n_updates            | 3940        |\n",
            "|    policy_gradient_loss | -0.0048     |\n",
            "|    std                  | 0.176       |\n",
            "|    value_loss           | 0.0519      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 202000: mean reward 2.44\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | 5.25     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 99       |\n",
            "|    time_elapsed    | 1124     |\n",
            "|    total_timesteps | 811008   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=816000, episode_reward=5.45 +/- 6.03\n",
            "Episode length: 203.00 +/- 68.97\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 203         |\n",
            "|    mean_reward          | 5.45        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 816000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028495224 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.668       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.000412    |\n",
            "|    n_updates            | 3950        |\n",
            "|    policy_gradient_loss | -0.00282    |\n",
            "|    std                  | 0.173       |\n",
            "|    value_loss           | 0.059       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 204000: mean reward 5.45\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 166      |\n",
            "|    ep_rew_mean     | 4.41     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 100      |\n",
            "|    time_elapsed    | 1136     |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=824000, episode_reward=3.59 +/- 3.83\n",
            "Episode length: 148.80 +/- 63.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 149         |\n",
            "|    mean_reward          | 3.59        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 824000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026796555 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.675       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00283    |\n",
            "|    n_updates            | 3960        |\n",
            "|    policy_gradient_loss | -0.00358    |\n",
            "|    std                  | 0.174       |\n",
            "|    value_loss           | 0.0549      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 206000: mean reward 3.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 143      |\n",
            "|    ep_rew_mean     | 2.42     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 101      |\n",
            "|    time_elapsed    | 1147     |\n",
            "|    total_timesteps | 827392   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=832000, episode_reward=1.63 +/- 4.21\n",
            "Episode length: 125.80 +/- 45.64\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 126         |\n",
            "|    mean_reward          | 1.63        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 832000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025352878 |\n",
            "|    clip_fraction        | 0.243       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.673       |\n",
            "|    explained_variance   | 0.887       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0162      |\n",
            "|    n_updates            | 3970        |\n",
            "|    policy_gradient_loss | -0.00375    |\n",
            "|    std                  | 0.174       |\n",
            "|    value_loss           | 0.0523      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 208000: mean reward 1.63\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 142      |\n",
            "|    ep_rew_mean     | 2.45     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 102      |\n",
            "|    time_elapsed    | 1158     |\n",
            "|    total_timesteps | 835584   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=5.28 +/- 5.61\n",
            "Episode length: 177.40 +/- 98.08\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 177        |\n",
            "|    mean_reward          | 5.28       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 840000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02967608 |\n",
            "|    clip_fraction        | 0.249      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.671      |\n",
            "|    explained_variance   | 0.915      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0169     |\n",
            "|    n_updates            | 3980       |\n",
            "|    policy_gradient_loss | -0.00311   |\n",
            "|    std                  | 0.174      |\n",
            "|    value_loss           | 0.0444     |\n",
            "----------------------------------------\n",
            "Evaluation at step 210000: mean reward 5.28\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 2.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 103      |\n",
            "|    time_elapsed    | 1169     |\n",
            "|    total_timesteps | 843776   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=848000, episode_reward=2.76 +/- 3.69\n",
            "Episode length: 165.20 +/- 28.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 165         |\n",
            "|    mean_reward          | 2.76        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 848000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027157307 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.672       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00872     |\n",
            "|    n_updates            | 3990        |\n",
            "|    policy_gradient_loss | -0.00216    |\n",
            "|    std                  | 0.174       |\n",
            "|    value_loss           | 0.0463      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 212000: mean reward 2.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 152      |\n",
            "|    ep_rew_mean     | 2.77     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 104      |\n",
            "|    time_elapsed    | 1180     |\n",
            "|    total_timesteps | 851968   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=856000, episode_reward=1.65 +/- 2.99\n",
            "Episode length: 105.00 +/- 42.62\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 105         |\n",
            "|    mean_reward          | 1.65        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 856000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027352553 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.672       |\n",
            "|    explained_variance   | 0.898       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0318      |\n",
            "|    n_updates            | 4000        |\n",
            "|    policy_gradient_loss | -0.00494    |\n",
            "|    std                  | 0.173       |\n",
            "|    value_loss           | 0.05        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 214000: mean reward 1.65\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 4.11     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 105      |\n",
            "|    time_elapsed    | 1191     |\n",
            "|    total_timesteps | 860160   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=864000, episode_reward=3.75 +/- 5.66\n",
            "Episode length: 163.40 +/- 75.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 163         |\n",
            "|    mean_reward          | 3.75        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 864000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026514422 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.707       |\n",
            "|    explained_variance   | 0.854       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.000475    |\n",
            "|    n_updates            | 4010        |\n",
            "|    policy_gradient_loss | -0.00296    |\n",
            "|    std                  | 0.17        |\n",
            "|    value_loss           | 0.0577      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 216000: mean reward 3.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 165      |\n",
            "|    ep_rew_mean     | 4.32     |\n",
            "| time/              |          |\n",
            "|    fps             | 721      |\n",
            "|    iterations      | 106      |\n",
            "|    time_elapsed    | 1202     |\n",
            "|    total_timesteps | 868352   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=872000, episode_reward=2.14 +/- 1.96\n",
            "Episode length: 114.20 +/- 41.98\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 114         |\n",
            "|    mean_reward          | 2.14        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 872000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027116122 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.704       |\n",
            "|    explained_variance   | 0.9         |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0153     |\n",
            "|    n_updates            | 4020        |\n",
            "|    policy_gradient_loss | -0.00122    |\n",
            "|    std                  | 0.171       |\n",
            "|    value_loss           | 0.0458      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 218000: mean reward 2.14\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 2.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 107      |\n",
            "|    time_elapsed    | 1213     |\n",
            "|    total_timesteps | 876544   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=2.65 +/- 3.02\n",
            "Episode length: 145.80 +/- 38.98\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 146         |\n",
            "|    mean_reward          | 2.65        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 880000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029141165 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.715       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0249      |\n",
            "|    n_updates            | 4030        |\n",
            "|    policy_gradient_loss | -0.00188    |\n",
            "|    std                  | 0.169       |\n",
            "|    value_loss           | 0.0388      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 220000: mean reward 2.65\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 144      |\n",
            "|    ep_rew_mean     | 2.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 108      |\n",
            "|    time_elapsed    | 1225     |\n",
            "|    total_timesteps | 884736   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=888000, episode_reward=2.53 +/- 4.82\n",
            "Episode length: 139.20 +/- 66.84\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 139         |\n",
            "|    mean_reward          | 2.53        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 888000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028193776 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.736       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0167     |\n",
            "|    n_updates            | 4040        |\n",
            "|    policy_gradient_loss | -0.00349    |\n",
            "|    std                  | 0.167       |\n",
            "|    value_loss           | 0.0414      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 222000: mean reward 2.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 2.84     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 109      |\n",
            "|    time_elapsed    | 1236     |\n",
            "|    total_timesteps | 892928   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=896000, episode_reward=2.76 +/- 3.65\n",
            "Episode length: 141.60 +/- 41.67\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 142        |\n",
            "|    mean_reward          | 2.76       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 896000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02909296 |\n",
            "|    clip_fraction        | 0.262      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.754      |\n",
            "|    explained_variance   | 0.917      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0319     |\n",
            "|    n_updates            | 4050       |\n",
            "|    policy_gradient_loss | -0.00281   |\n",
            "|    std                  | 0.166      |\n",
            "|    value_loss           | 0.0443     |\n",
            "----------------------------------------\n",
            "Evaluation at step 224000: mean reward 2.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 162      |\n",
            "|    ep_rew_mean     | 3.39     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 110      |\n",
            "|    time_elapsed    | 1246     |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=904000, episode_reward=1.60 +/- 2.75\n",
            "Episode length: 125.20 +/- 32.73\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 125       |\n",
            "|    mean_reward          | 1.6       |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 904000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0263864 |\n",
            "|    clip_fraction        | 0.254     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.77      |\n",
            "|    explained_variance   | 0.896     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.0225   |\n",
            "|    n_updates            | 4060      |\n",
            "|    policy_gradient_loss | -0.00399  |\n",
            "|    std                  | 0.165     |\n",
            "|    value_loss           | 0.0343    |\n",
            "---------------------------------------\n",
            "Evaluation at step 226000: mean reward 1.60\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | 3.63     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 111      |\n",
            "|    time_elapsed    | 1257     |\n",
            "|    total_timesteps | 909312   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=912000, episode_reward=4.12 +/- 4.58\n",
            "Episode length: 193.40 +/- 49.25\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 193         |\n",
            "|    mean_reward          | 4.12        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 912000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029748447 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.777       |\n",
            "|    explained_variance   | 0.906       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0103      |\n",
            "|    n_updates            | 4070        |\n",
            "|    policy_gradient_loss | -0.00138    |\n",
            "|    std                  | 0.164       |\n",
            "|    value_loss           | 0.0464      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 228000: mean reward 4.12\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 162      |\n",
            "|    ep_rew_mean     | 4.79     |\n",
            "| time/              |          |\n",
            "|    fps             | 722      |\n",
            "|    iterations      | 112      |\n",
            "|    time_elapsed    | 1269     |\n",
            "|    total_timesteps | 917504   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=2.52 +/- 4.51\n",
            "Episode length: 144.40 +/- 65.76\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 144         |\n",
            "|    mean_reward          | 2.52        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 920000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029360961 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.781       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.18        |\n",
            "|    n_updates            | 4080        |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    std                  | 0.164       |\n",
            "|    value_loss           | 0.0898      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 230000: mean reward 2.52\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 156      |\n",
            "|    ep_rew_mean     | 4.43     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 113      |\n",
            "|    time_elapsed    | 1280     |\n",
            "|    total_timesteps | 925696   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=928000, episode_reward=1.81 +/- 3.16\n",
            "Episode length: 118.80 +/- 43.91\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 119         |\n",
            "|    mean_reward          | 1.81        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 928000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025294632 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.774       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0224      |\n",
            "|    n_updates            | 4090        |\n",
            "|    policy_gradient_loss | -0.00108    |\n",
            "|    std                  | 0.166       |\n",
            "|    value_loss           | 0.0542      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 232000: mean reward 1.81\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 156      |\n",
            "|    ep_rew_mean     | 4.29     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 114      |\n",
            "|    time_elapsed    | 1291     |\n",
            "|    total_timesteps | 933888   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=936000, episode_reward=1.69 +/- 2.30\n",
            "Episode length: 114.80 +/- 31.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 115         |\n",
            "|    mean_reward          | 1.69        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 936000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029673092 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.771       |\n",
            "|    explained_variance   | 0.893       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0153      |\n",
            "|    n_updates            | 4100        |\n",
            "|    policy_gradient_loss | -0.000909   |\n",
            "|    std                  | 0.165       |\n",
            "|    value_loss           | 0.0611      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 234000: mean reward 1.69\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 3.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 115      |\n",
            "|    time_elapsed    | 1302     |\n",
            "|    total_timesteps | 942080   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=944000, episode_reward=5.88 +/- 7.87\n",
            "Episode length: 185.80 +/- 119.67\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 186         |\n",
            "|    mean_reward          | 5.88        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 944000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028866448 |\n",
            "|    clip_fraction        | 0.26        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.764       |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0335      |\n",
            "|    n_updates            | 4110        |\n",
            "|    policy_gradient_loss | -0.00368    |\n",
            "|    std                  | 0.166       |\n",
            "|    value_loss           | 0.0457      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 236000: mean reward 5.88\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 3.56     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 116      |\n",
            "|    time_elapsed    | 1314     |\n",
            "|    total_timesteps | 950272   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=952000, episode_reward=2.44 +/- 3.76\n",
            "Episode length: 143.00 +/- 45.56\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 143         |\n",
            "|    mean_reward          | 2.44        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 952000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031056874 |\n",
            "|    clip_fraction        | 0.268       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.786       |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0813      |\n",
            "|    n_updates            | 4120        |\n",
            "|    policy_gradient_loss | -0.00336    |\n",
            "|    std                  | 0.163       |\n",
            "|    value_loss           | 0.0639      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 238000: mean reward 2.44\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 4.36     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 117      |\n",
            "|    time_elapsed    | 1324     |\n",
            "|    total_timesteps | 958464   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=25.47 +/- 48.57\n",
            "Episode length: 170.60 +/- 124.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 171         |\n",
            "|    mean_reward          | 25.5        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 960000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024925452 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.804       |\n",
            "|    explained_variance   | 0.935       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0268      |\n",
            "|    n_updates            | 4130        |\n",
            "|    policy_gradient_loss | -0.000781   |\n",
            "|    std                  | 0.162       |\n",
            "|    value_loss           | 0.0512      |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 240000: mean reward 25.47\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 162      |\n",
            "|    ep_rew_mean     | 3.48     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 118      |\n",
            "|    time_elapsed    | 1335     |\n",
            "|    total_timesteps | 966656   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=968000, episode_reward=4.77 +/- 6.81\n",
            "Episode length: 170.60 +/- 106.03\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 171         |\n",
            "|    mean_reward          | 4.77        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 968000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027528524 |\n",
            "|    clip_fraction        | 0.264       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.806       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0202      |\n",
            "|    n_updates            | 4140        |\n",
            "|    policy_gradient_loss | -0.000123   |\n",
            "|    std                  | 0.163       |\n",
            "|    value_loss           | 0.0446      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 242000: mean reward 4.77\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | 4.25     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 119      |\n",
            "|    time_elapsed    | 1348     |\n",
            "|    total_timesteps | 974848   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=976000, episode_reward=3.16 +/- 3.00\n",
            "Episode length: 170.60 +/- 54.79\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 171        |\n",
            "|    mean_reward          | 3.16       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 976000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02898268 |\n",
            "|    clip_fraction        | 0.277      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.81       |\n",
            "|    explained_variance   | 0.889      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0065     |\n",
            "|    n_updates            | 4150       |\n",
            "|    policy_gradient_loss | -0.00212   |\n",
            "|    std                  | 0.162      |\n",
            "|    value_loss           | 0.0679     |\n",
            "----------------------------------------\n",
            "Evaluation at step 244000: mean reward 3.16\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 153      |\n",
            "|    ep_rew_mean     | 5.03     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 120      |\n",
            "|    time_elapsed    | 1359     |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=984000, episode_reward=2.35 +/- 3.22\n",
            "Episode length: 123.00 +/- 45.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 123         |\n",
            "|    mean_reward          | 2.35        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 984000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029179577 |\n",
            "|    clip_fraction        | 0.268       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.803       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0309      |\n",
            "|    n_updates            | 4160        |\n",
            "|    policy_gradient_loss | -0.00306    |\n",
            "|    std                  | 0.163       |\n",
            "|    value_loss           | 0.068       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 246000: mean reward 2.35\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 147      |\n",
            "|    ep_rew_mean     | 3.32     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 121      |\n",
            "|    time_elapsed    | 1370     |\n",
            "|    total_timesteps | 991232   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=992000, episode_reward=3.57 +/- 2.86\n",
            "Episode length: 148.00 +/- 43.42\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 148        |\n",
            "|    mean_reward          | 3.57       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 992000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03148947 |\n",
            "|    clip_fraction        | 0.258      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.791      |\n",
            "|    explained_variance   | 0.908      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0208     |\n",
            "|    n_updates            | 4170       |\n",
            "|    policy_gradient_loss | -0.00191   |\n",
            "|    std                  | 0.164      |\n",
            "|    value_loss           | 0.0461     |\n",
            "----------------------------------------\n",
            "Evaluation at step 248000: mean reward 3.57\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 3.59     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 122      |\n",
            "|    time_elapsed    | 1381     |\n",
            "|    total_timesteps | 999424   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=2.37 +/- 3.32\n",
            "Episode length: 127.80 +/- 39.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 128         |\n",
            "|    mean_reward          | 2.37        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 1000000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028805632 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.814       |\n",
            "|    explained_variance   | 0.894       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0351      |\n",
            "|    n_updates            | 4180        |\n",
            "|    policy_gradient_loss | -0.00167    |\n",
            "|    std                  | 0.161       |\n",
            "|    value_loss           | 0.0758      |\n",
            "-----------------------------------------\n",
            "Evaluation at step 250000: mean reward 2.37\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 3.88     |\n",
            "| time/              |          |\n",
            "|    fps             | 723      |\n",
            "|    iterations      | 123      |\n",
            "|    time_elapsed    | 1392     |\n",
            "|    total_timesteps | 1007616  |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "9142aaf6-b1f0-4e8b-c8dc-f263eacc127e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 36.81911273412406 +/- 62.40618049660436\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "0fbc67ed-195f-4712-9f0e-1549ffafd93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serve\n",
            "hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.game_state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.game_state` for environment variables or `env.get_wrapper_attr('game_state')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'server': 'Player1', 'positions_reversed': True, 'opponent': 'Player1', 'player': 'Player2', 'Player1': {'x': 1185, 'y': 355, 'dy': 0, 'width': 15, 'height': 180, 'colour': 'blue'}, 'Player2': {'x': 0, 'y': 355, 'dy': 0, 'width': 15, 'height': 90, 'colour': 'red'}, 'ball': {'x': 1173, 'y': 445.0, 'dx': 10, 'dy': 10, 'radius': 12, 'speed': 10, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0}, 'stats': {'rally_length': 0, 'serve_speed': 10, 'server': 'Player1'}}\n",
            "serve\n",
            "hit\n",
            "hit\n",
            "hit\n",
            "{'server': 'Player1', 'positions_reversed': False, 'opponent': 'Player1', 'player': 'Player2', 'Player1': {'x': 0, 'y': 355, 'dy': 0, 'width': 15, 'height': 180, 'colour': 'blue'}, 'Player2': {'x': 1185, 'y': 355, 'dy': 0, 'width': 15, 'height': 90, 'colour': 'red'}, 'ball': {'x': 27, 'y': 445.0, 'dx': 10, 'dy': 10, 'radius': 12, 'speed': 10, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0}, 'stats': {'rally_length': 0, 'serve_speed': 10, 'server': 'Player1'}}\n",
            "serve\n",
            "hit\n",
            "{'server': 'Player1', 'positions_reversed': True, 'opponent': 'Player1', 'player': 'Player2', 'Player1': {'x': 1185, 'y': 355, 'dy': 0, 'width': 15, 'height': 180, 'colour': 'blue'}, 'Player2': {'x': 0, 'y': 355, 'dy': 0, 'width': 15, 'height': 90, 'colour': 'red'}, 'ball': {'x': 1173, 'y': 445.0, 'dx': 10, 'dy': 10, 'radius': 12, 'speed': 10, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0}, 'stats': {'rally_length': 0, 'serve_speed': 10, 'server': 'Player1'}}\n",
            "serve\n",
            "hit\n",
            "hit\n",
            "hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'server': 'Player1', 'positions_reversed': False, 'opponent': 'Player1', 'player': 'Player2', 'Player1': {'x': 0, 'y': 355, 'dy': 0, 'width': 15, 'height': 180, 'colour': 'blue'}, 'Player2': {'x': 1185, 'y': 355, 'dy': 0, 'width': 15, 'height': 90, 'colour': 'red'}, 'ball': {'x': 27, 'y': 445.0, 'dx': 10, 'dy': 10, 'radius': 12, 'speed': 10, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0}, 'stats': {'rally_length': 0, 'serve_speed': 10, 'server': 'Player1'}}\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "# model = PPO.load(\"ppo_custom_pong_1\")\n",
        "model = PPO.load(\"./logs/best_model/best_model\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "eval_env.norm_reward = False  # Disable reward normalization for evaluation\n",
        "\n",
        "# Load the normalization statistics\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "train_env = VecNormalize.load(\"vecnormalize.pkl\", train_env)\n",
        "\n",
        "# Sync the observation normalization statistics\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(eval_env.envs[0].game_state)\n",
        "      obs = eval_env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgjmNUdl1WcA",
        "outputId": "d1d7a22b-bbc4-4e08-eee4-1def3205f09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Load the trained PPO model\n",
        "model = PPO.load(f\"{path}/ppo_custom_pong_{name}.{iteration+1}\")\n",
        "\n",
        "# Extract the PyTorch model's weights\n",
        "policy_weights = model.policy.state_dict()\n",
        "\n",
        "# Define the TensorFlow model\n",
        "class TfPPOPolicy(tf.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TfPPOPolicy, self).__init__()\n",
        "        self.fc1 = layers.Dense(64, activation='relu', input_shape=(input_dim,))\n",
        "        self.fc2 = layers.Dense(64, activation='relu')\n",
        "        self.action_head = layers.Dense(output_dim)\n",
        "        self.value_head = layers.Dense(1)\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 8], dtype=tf.float32)])\n",
        "    def __call__(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        action_logits = self.action_head(x)\n",
        "        value = self.value_head(x)\n",
        "        return action_logits, value\n",
        "\n",
        "    def predict(self, x):\n",
        "        action_logits, value = self.__call__(x)\n",
        "        # Apply sigmoid to the buttonPressed logits to constrain between 0 and 1\n",
        "        buttonPressed = tf.sigmoid(action_logits[:, 0])\n",
        "        paddleDirection = action_logits[:, 1]\n",
        "        return buttonPressed, paddleDirection\n",
        "\n",
        "\n",
        "# Create the TensorFlow model\n",
        "input_dim = 8  # Your observation space dimension\n",
        "output_dim = 2  # Number of actions\n",
        "tf_policy = TfPPOPolicy(input_dim, output_dim)\n",
        "\n",
        "# Load weights into the TensorFlow model\n",
        "tf_policy.fc1.build((None, input_dim))\n",
        "tf_policy.fc1.set_weights([\n",
        "    policy_weights['mlp_extractor.policy_net.0.weight'].cpu().numpy().T,\n",
        "    policy_weights['mlp_extractor.policy_net.0.bias'].cpu().numpy()\n",
        "])\n",
        "\n",
        "tf_policy.fc2.build((None, 64))\n",
        "tf_policy.fc2.set_weights([\n",
        "    policy_weights['mlp_extractor.policy_net.2.weight'].cpu().numpy().T,\n",
        "    policy_weights['mlp_extractor.policy_net.2.bias'].cpu().numpy()\n",
        "])\n",
        "\n",
        "tf_policy.action_head.build((None, 64))\n",
        "tf_policy.action_head.set_weights([\n",
        "    policy_weights['action_net.weight'].cpu().numpy().T,\n",
        "    policy_weights['action_net.bias'].cpu().numpy()\n",
        "])\n",
        "\n",
        "tf_policy.value_head.build((None, 64))\n",
        "tf_policy.value_head.set_weights([\n",
        "    policy_weights['value_net.weight'].cpu().numpy().T,\n",
        "    policy_weights['value_net.bias'].cpu().numpy()\n",
        "])\n",
        "\n",
        "# Save the wrapped policy as a TensorFlow SavedModel\n",
        "saved_model_path = \"./saved_model\"\n",
        "tf.saved_model.save(tf_policy, saved_model_path)\n",
        "\n",
        "# Convert the SavedModel to TensorFlow.js format using the command line\n",
        "# !tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model ./saved_model ./tfjs_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = 4\n",
        "model = PPO.load(f\"{path}/ppo_custom_pong_{name}.{iteration+1}\")\n",
        "\n",
        "# Extract PyTorch model's weights\n",
        "policy_weights = model.policy.state_dict()\n",
        "\n",
        "# Define the TensorFlow model\n",
        "class TfPPOPolicy(tf.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TfPPOPolicy, self).__init__()\n",
        "        self.fc1_policy = layers.Dense(64, activation='tanh', name='fc1_policy')\n",
        "        self.fc2_policy = layers.Dense(64, activation='tanh', name='fc2_policy')\n",
        "        self.fc1_value = layers.Dense(64, activation='tanh', name='fc1_value')\n",
        "        self.fc2_value = layers.Dense(64, activation='tanh', name='fc2_value')\n",
        "        self.action_head = layers.Dense(output_dim, name='action_head')\n",
        "        self.value_head = layers.Dense(1, name='value_head')\n",
        "\n",
        "        # Build the layers by running a dummy input through the network\n",
        "        dummy_input = tf.random.uniform((1, input_dim))\n",
        "        self.__call__(dummy_input)\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 8], dtype=tf.float32)])\n",
        "    def __call__(self, x):\n",
        "        policy_x = self.fc1_policy(x)\n",
        "        policy_x = self.fc2_policy(policy_x)\n",
        "        action_logits = self.action_head(policy_x)\n",
        "\n",
        "        value_x = self.fc1_value(x)\n",
        "        value_x = self.fc2_value(value_x)\n",
        "        value = self.value_head(value_x)\n",
        "\n",
        "        return action_logits, value\n",
        "\n",
        "    def predict(self, x):\n",
        "        action_logits, value = self.__call__(x)\n",
        "        buttonPressed = tf.sigmoid(action_logits[:, 0])\n",
        "        paddleDirection = action_logits[:, 1]\n",
        "        return buttonPressed, paddleDirection\n",
        "\n",
        "# Instantiate the TensorFlow model\n",
        "input_dim = 8  # Adjust based on your actual input dimension\n",
        "output_dim = 2  # Adjust based on your actual output dimension\n",
        "tf_policy = TfPPOPolicy(input_dim, output_dim)\n",
        "\n",
        "# Helper function to assign weights\n",
        "def assign_weights(tf_model, torch_weights):\n",
        "    # Assign policy network weights\n",
        "    tf_model.fc1_policy.kernel.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.policy_net.0.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.fc1_policy.bias.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.policy_net.0.bias'].numpy(), dtype=tf.float32))\n",
        "    tf_model.fc2_policy.kernel.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.policy_net.2.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.fc2_policy.bias.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.policy_net.2.bias'].numpy(), dtype=tf.float32))\n",
        "\n",
        "    # Assign value network weights\n",
        "    tf_model.fc1_value.kernel.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.value_net.0.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.fc1_value.bias.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.value_net.0.bias'].numpy(), dtype=tf.float32))\n",
        "    tf_model.fc2_value.kernel.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.value_net.2.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.fc2_value.bias.assign(tf.convert_to_tensor(torch_weights['mlp_extractor.value_net.2.bias'].numpy(), dtype=tf.float32))\n",
        "\n",
        "    # Assign action and value head weights\n",
        "    tf_model.action_head.kernel.assign(tf.convert_to_tensor(torch_weights['action_net.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.action_head.bias.assign(tf.convert_to_tensor(torch_weights['action_net.bias'].numpy(), dtype=tf.float32))\n",
        "    tf_model.value_head.kernel.assign(tf.convert_to_tensor(torch_weights['value_net.weight'].t().numpy(), dtype=tf.float32))\n",
        "    tf_model.value_head.bias.assign(tf.convert_to_tensor(torch_weights['value_net.bias'].numpy(), dtype=tf.float32))\n",
        "\n",
        "# Assign the weights\n",
        "assign_weights(tf_policy, model.policy.state_dict())\n",
        "\n",
        "# Get intermediate outputs from the PyTorch model\n",
        "def get_pytorch_intermediate_outputs(model, input_data):\n",
        "    with th.no_grad():\n",
        "        x = input_data\n",
        "        x1 = model.policy.mlp_extractor.policy_net[0](x)\n",
        "        x2 = model.policy.mlp_extractor.policy_net[2](x1)\n",
        "        action_logits = model.policy.action_net(x2)\n",
        "        value_x1 = model.policy.mlp_extractor.value_net[0](x)\n",
        "        value_x2 = model.policy.mlp_extractor.value_net[2](value_x1)\n",
        "        value = model.policy.value_net(value_x2)\n",
        "    return x1, x2, action_logits, value_x1, value_x2, value\n",
        "\n",
        "# Get intermediate outputs from the TensorFlow model\n",
        "def get_tf_intermediate_outputs(model, input_data):\n",
        "    policy_x1 = model.fc1_policy(input_data)\n",
        "    policy_x2 = model.fc2_policy(policy_x1)\n",
        "    action_logits = model.action_head(policy_x2)\n",
        "    value_x1 = model.fc1_value(input_data)\n",
        "    value_x2 = model.fc2_value(value_x1)\n",
        "    value = model.value_head(value_x2)\n",
        "    return policy_x1, policy_x2, action_logits, value_x1, value_x2, value\n",
        "\n",
        "# Test the conversion with a small input\n",
        "test_input = th.rand(1, input_dim)  # Use a fixed test input\n",
        "torch_policy_x1, torch_policy_x2, torch_action_logits, torch_value_x1, torch_value_x2, torch_value = get_pytorch_intermediate_outputs(model, test_input)\n",
        "\n",
        "# Ensure the input to TensorFlow is identical\n",
        "tf_input = tf.convert_to_tensor(test_input.numpy(), dtype=tf.float32)\n",
        "tf_policy_x1, tf_policy_x2, tf_action_logits, tf_value_x1, tf_value_x2, tf_value = get_tf_intermediate_outputs(tf_policy, tf_input)\n",
        "\n",
        "# Print intermediate outputs\n",
        "print(\"PyTorch policy fc1 output:\", torch_policy_x1.detach().numpy())\n",
        "print(\"TensorFlow policy fc1 output:\", tf_policy_x1.numpy())\n",
        "\n",
        "print(\"PyTorch policy fc2 output:\", torch_policy_x2.detach().numpy())\n",
        "print(\"TensorFlow policy fc2 output:\", tf_policy_x2.numpy())\n",
        "\n",
        "print(\"PyTorch action logits:\", torch_action_logits.detach().numpy())\n",
        "print(\"TensorFlow action logits:\", tf_action_logits.numpy())\n",
        "\n",
        "print(\"PyTorch value fc1 output:\", torch_value_x1.detach().numpy())\n",
        "print(\"TensorFlow value fc1 output:\", tf_value_x1.numpy())\n",
        "\n",
        "print(\"PyTorch value fc2 output:\", torch_value_x2.detach().numpy())\n",
        "print(\"TensorFlow value fc2 output:\", tf_value_x2.numpy())\n",
        "\n",
        "print(\"PyTorch value:\", torch_value.detach().numpy())\n",
        "print(\"TensorFlow value:\", tf_value.numpy())\n",
        "\n",
        "saved_model_path = \"./saved_model_new\"\n",
        "tf.saved_model.save(tf_policy, saved_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxmvM-HflF9X",
        "outputId": "47353da5-4de2-40b2-e096-0c1a97be335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch policy fc1 output: [[-0.15904823 -0.0670957   0.10368206  0.30483988  0.01337119 -0.24508096\n",
            "  -0.3546461  -0.48025256  0.03984199  0.63393235 -0.09844951  0.41337955\n",
            "  -0.11019313  0.45303807  0.11283946  0.28443953 -0.56069803  0.34210873\n",
            "   0.23990984  0.22165376  0.20915824 -0.1377928  -0.06990146  0.16214462\n",
            "  -0.00795686 -0.3767324   0.02442205  0.01465188  0.05508695  0.1515808\n",
            "   0.09872608  0.12319711 -0.11831853 -0.47226366 -0.24919662  0.0546505\n",
            "  -0.16687143  0.12732671 -0.2569833  -0.1844132   0.19296348 -0.44674465\n",
            "   0.30742025  0.02844924  0.28813526 -0.13057217 -0.12696053 -0.05529656\n",
            "  -0.624382   -0.11825649  0.5915864  -0.21161465  0.24253242  0.25089306\n",
            "   0.26347867  0.00536796  0.13035037  0.09258169  0.02575868  0.28844428\n",
            "  -0.4807719   0.38800618 -0.3156076   0.21997526]]\n",
            "TensorFlow policy fc1 output: [[-0.15772057 -0.06699519  0.10331212  0.29573548  0.0133704  -0.24028914\n",
            "  -0.34048945 -0.44644582  0.03982091  0.560754   -0.09813269  0.39133838\n",
            "  -0.10974926  0.4243931   0.11236295  0.27700895 -0.50849515  0.32935867\n",
            "   0.2354106   0.21809368  0.20616062 -0.13692726 -0.06978783  0.1607384\n",
            "  -0.0079567  -0.35986638  0.0244172   0.01465081  0.05503129  0.15043041\n",
            "   0.09840656  0.12257758 -0.11776947 -0.44002646 -0.24416329  0.05459615\n",
            "  -0.16533956  0.12664306 -0.25147173 -0.1823507   0.19060363 -0.41921943\n",
            "   0.29808837  0.02844158  0.28041756 -0.12983514 -0.12628272 -0.05524025\n",
            "  -0.5541717  -0.11770827  0.5310356  -0.20851147  0.23788628  0.24575794\n",
            "   0.25754634  0.00536792  0.12961708  0.09231807  0.02575299  0.28070232\n",
            "  -0.4468616   0.36964008 -0.3055299   0.21649447]]\n",
            "PyTorch policy fc2 output: [[ 0.2580748   0.35961708  0.79846567  0.00922559  0.01817649  0.30674368\n",
            "   0.18265246 -0.92196685  0.03930359 -0.10490738 -0.8269539  -0.14007714\n",
            "   0.639525    0.43950984 -0.11992251  0.52155083  0.35799447 -0.9686339\n",
            "   0.53367007 -0.4186283  -0.9787291   0.4076707   0.48253754 -0.52516675\n",
            "  -0.75939614 -0.6455525  -0.26208976 -0.26244202  0.00843102 -0.73489\n",
            "  -0.3686754  -0.27404973  0.6555939  -0.48622587  0.5459755  -0.0335986\n",
            "  -0.29758233  0.06046808 -0.90973884  0.17154229 -0.5355959   0.59310573\n",
            "  -0.9109549   0.224879    0.10024659  0.13302805 -0.7215535  -0.68927634\n",
            "   0.4023895   0.13953723  0.64177346  0.41072994  0.20503883 -0.5347915\n",
            "  -0.44041976 -0.57368046 -0.50232524 -0.26915783 -0.92936677  0.39247403\n",
            "   0.02194716 -0.6760395  -0.24187298 -0.9414554 ]]\n",
            "TensorFlow policy fc2 output: [[ 0.24393584  0.32435977  0.63934124 -0.00488053  0.0079509   0.296116\n",
            "   0.17834142 -0.6925586   0.04286323 -0.10359357 -0.6523718  -0.10117047\n",
            "   0.5436927   0.39548707 -0.10701001  0.4604681   0.32514682 -0.7183251\n",
            "   0.46035838 -0.3685432  -0.7203359   0.36457637  0.44025108 -0.46881142\n",
            "  -0.6229156  -0.54608345 -0.23073186 -0.23654167 -0.00535676 -0.594685\n",
            "  -0.34250167 -0.2585312   0.5305902  -0.40447313  0.45061582 -0.00403009\n",
            "  -0.28960022  0.0403762  -0.6916822   0.15735978 -0.48520464  0.5022033\n",
            "  -0.708647    0.20722154  0.10305924  0.13694416 -0.594488   -0.5470527\n",
            "   0.34724894  0.14174727  0.51997185  0.3680116   0.15604188 -0.46607265\n",
            "  -0.39335066 -0.50249016 -0.46575364 -0.24310172 -0.70581114  0.3625975\n",
            "   0.01186695 -0.5565037  -0.22590269 -0.7180713 ]]\n",
            "PyTorch action logits: [[ 3.3907397  -0.76871794]]\n",
            "TensorFlow action logits: [[ 2.8357568 -0.7079013]]\n",
            "PyTorch value fc1 output: [[ 0.25013858  0.52730304 -0.11026082  0.6144301   0.066351    0.4472005\n",
            "  -0.505573    0.6166473  -0.5621375   0.33948815 -1.1604046  -0.23091036\n",
            "   0.30203593  0.0725731  -0.635337    4.7972784   0.8640883   0.303644\n",
            "   0.07197234 -1.0487679   0.02833357 -0.01196579 -0.91335005  0.32950622\n",
            "  -0.7891722   0.3583299  -0.3759547   0.22720857  0.4741523  -0.46054238\n",
            "  -0.13286358 -0.20280026 -0.19921532 -0.8326294  -0.05478342  0.42130312\n",
            "   0.42745298  0.15030624  0.41557273 -0.7077857   1.3917019   0.2991379\n",
            "   0.10512996  0.49273685 -0.09425604  1.1861508  -0.24738634 -0.23231472\n",
            "   0.16122843  0.5948034  -0.87920254 -0.41122717  0.17039219  0.26438093\n",
            "  -0.08759536 -0.21747485  3.5157447   0.47026908  1.0766728  -0.39269522\n",
            "  -0.36358416 -0.07807507 -0.1365075  -1.88003   ]]\n",
            "TensorFlow value fc1 output: [[ 0.24504894  0.48331678 -0.10981613  0.54723805  0.0662538   0.41959506\n",
            "  -0.4664887   0.5487894  -0.5095617   0.32702038 -0.8211717  -0.226892\n",
            "   0.29317462  0.07244593 -0.5617162   0.99986374  0.69835806  0.29464376\n",
            "   0.07184832 -0.7813269   0.02832597 -0.01196523 -0.7227363   0.318077\n",
            "  -0.65793985  0.3437421  -0.35918924  0.22337787  0.4415482  -0.43052614\n",
            "  -0.13208726 -0.20006497 -0.1966211  -0.68188536 -0.05472869  0.39802763\n",
            "   0.40319046  0.14918447  0.3931942  -0.6092865   0.88354486  0.2905235\n",
            "   0.10474434  0.4563859  -0.09397786  0.8293813  -0.2424602  -0.22822367\n",
            "   0.15984577  0.53334147 -0.70601964 -0.38951418  0.16876212  0.25838858\n",
            "  -0.08737203 -0.21410997  0.9982344   0.43841675  0.791962   -0.37368143\n",
            "  -0.34836712 -0.07791682 -0.13566585 -0.9544948 ]]\n",
            "PyTorch value fc2 output: [[ 11.3046       3.020113   -10.154844     7.2899475    7.450326\n",
            "   -6.015837     4.199188     8.130712     6.8452573    2.4769845\n",
            "   -4.7033186   -5.036308    -7.9722424   -4.4968877    5.2572765\n",
            "    6.500271    10.156302    -3.0073059    6.856681    -5.2172427\n",
            "    0.4614238  -11.973866    -3.5081766    8.445072    -5.642384\n",
            "    2.5342095    9.629617    -9.644254    -3.3437939   -4.897865\n",
            "    5.09489     -9.123881     8.993333   -10.084673     5.7927275\n",
            "   -4.9303665   -3.3660018    5.888387    -7.060141     6.007693\n",
            "   -3.097494    10.523493     5.190138     4.0096464  -11.483518\n",
            "   -6.192313    -4.7696147    0.73663056  -3.6840646    9.217075\n",
            "   -9.43288      8.568207    -7.0673876   -3.608019    -6.152904\n",
            "    9.349822     3.8994138    4.761756    -1.5383017   -5.8954306\n",
            "   -4.1070676    5.9390473   -9.058568    -1.4752965 ]]\n",
            "TensorFlow value fc2 output: [[ 0.99880713  0.17634863 -0.99955213  0.9258599   0.9858959  -0.7337584\n",
            "  -0.853001    0.9631653   0.99954355 -0.9177774   0.5163262  -0.05382967\n",
            "  -0.99757856  0.49152875 -0.54339314  0.978013    0.9903061  -0.9766907\n",
            "   0.97541493 -0.09554671 -0.73056173 -0.99975497 -0.12796944  0.98858476\n",
            "   0.4664988  -0.44383836  0.99751043 -0.9977553  -0.5579129  -0.9902064\n",
            "  -0.4626998  -0.9909408   0.9993342  -0.99926215 -0.00921412  0.22577764\n",
            "   0.5723289  -0.18870667 -0.9965706  -0.504261   -0.9164681   0.9966628\n",
            "   0.7942445  -0.75719005 -0.9986909  -0.83118725  0.52902126  0.89332855\n",
            "  -0.08345071  0.9976431  -0.9953273   0.9695512  -0.5666921  -0.36290288\n",
            "  -0.14055876  0.9969447   0.51977086 -0.7824171   0.35802096 -0.99243397\n",
            "  -0.871433    0.3490689  -0.9933708  -0.924408  ]]\n",
            "PyTorch value: [[-552.69366]]\n",
            "TensorFlow value: [[-23.621513]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeLcH84ysrNd",
        "outputId": "d33a6066-238c-463a-b717-9f12a45061a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-04 15:18:50.093334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Convert the SavedModel to TensorFlow.js format using the command line\n",
        "!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model ./saved_model_new ./tfjs_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pygame\n",
        "%pip install moviepy"
      ],
      "metadata": {
        "id": "OkfFEBE-Gqtx",
        "outputId": "7f96a9b1-6613-4071-d834-5670e2954a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}