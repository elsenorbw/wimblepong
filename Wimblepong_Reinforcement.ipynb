{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "5f720a71-dd7f-4970-d3d2-5bcc0003bf97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.16.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Requirement already satisfied: shimmy[atari]~=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=1.3.0->stable-baselines3[extra]) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]) (12.5.40)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.10/dist-packages (4.20.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.16.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.9.1)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.2)\n",
            "Requirement already satisfied: onnx2tf in /usr/local/lib/python3.10/dist-packages (1.22.6)\n",
            "Collecting onnx==1.15.0\n",
            "  Using cached onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "Requirement already satisfied: onnxruntime==1.17.1 in /usr/local/lib/python3.10/dist-packages (1.17.1)\n",
            "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (3.20.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.37.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime==1.17.1) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.17.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n",
            "Installing collected packages: onnx\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.16.1\n",
            "    Uninstalling onnx-1.16.1:\n",
            "      Successfully uninstalled onnx-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "onnxscript 0.1.0.dev20240627 requires onnx>=1.16, but you have onnx 1.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "onnx"
                ]
              },
              "id": "3133e235718a40798819223dba7c84be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx_graphsurgeon in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.25.2)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.14.0->onnx_graphsurgeon) (3.20.3)\n",
            "Requirement already satisfied: sng4onnx in /usr/local/lib/python3.10/dist-packages (1.0.4)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20240627)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.25.2)\n",
            "Collecting onnx>=1.16 (from onnxscript)\n",
            "  Using cached onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.3.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.15.0\n",
            "    Uninstalling onnx-1.15.0:\n",
            "      Successfully uninstalled onnx-1.15.0\n",
            "Successfully installed onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "onnx"
                ]
              },
              "id": "48f4c486f9184c9e8d42c8e4ae0bc467"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install tensorflowjs\n",
        "%pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
        "\n",
        "# onnx2tf deps:\n",
        "%pip install onnx_graphsurgeon\n",
        "%pip install sng4onnx\n",
        "\n",
        "%pip install onnxscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "6cd85edf-41ad-49d7-f6f6-9631e0da9f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"26-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "513e0a3f-7cb0-4bca-b4f4-1455d084197b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "import onnx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'red', 'Player2': 'blue'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    \"serve\": lambda ball_speed: ball_speed,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "def bounded_value(value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N932taVhk14O"
      },
      "outputs": [],
      "source": [
        "class ComputerPlayer:\n",
        "    def __init__(self):\n",
        "        self.reset(serve_delay=50, initial_direction=15, offset=0, max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "\n",
        "    def reset(self, serve_delay, initial_direction, offset, max_speed):\n",
        "        # print(\"ComputerPlayer reset\")\n",
        "        self.serve_delay = serve_delay\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = initial_direction\n",
        "        self.offset = initial_direction\n",
        "        self.max_speed = max_speed\n",
        "\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2  ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardSystem:\n",
        "    def __init__(self, rewarded_player):\n",
        "        self.rewarded_player = rewarded_player\n",
        "        self.total_reward = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.step_count += 1\n",
        "\n",
        "    def pre_serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            self.total_reward -= 0.01\n",
        "            # if self.step_count > 150:\n",
        "            #   self.total_reward -= self.step_count * 0.02\n",
        "            # else:\n",
        "            #   ball = game_state['ball']\n",
        "            #   reward = (abs(ball['dy']) * ((1000 - self.step_count) / 1000)) / 10\n",
        "            #   self.total_reward += reward\n",
        "\n",
        "    def serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            ball = game_state['ball']\n",
        "            reward = abs(ball['dy']) * abs(ball['dy'])\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def hit_paddle_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 50\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def conceed_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = (-abs(game_state['ball']['y'] - game_state[player]['y']) / max(game_state['stats']['rally_length'], 1))\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def score_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 100\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def paddle_movement_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            paddle = game_state[player]\n",
        "            reward = 0\n",
        "            self.total_reward += reward\n"
      ],
      "metadata": {
        "id": "b3MKATB64e3l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iIpdvdwMk14O"
      },
      "outputs": [],
      "source": [
        "class PongGame:\n",
        "    def __init__(self, server, positions_reversed, player, opponent):\n",
        "        self.game_state = {\n",
        "        'server': server,\n",
        "        'positions_reversed': positions_reversed,\n",
        "        'player': player,\n",
        "        'opponent': opponent,\n",
        "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def update_game_state(self, actions, delta_time, reward_system):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            return True\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "\n",
        "            reward_system.pre_serve_reward(server, game_state)\n",
        "\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "\n",
        "                reward_system.serve_reward(server, game_state)\n",
        "\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if not player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "\n",
        "        reward_system.paddle_movement_reward(self.game_state['player'], game_state)\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self, computer_player):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "        self.starting_state_index = 0\n",
        "\n",
        "        self.computer_player = computer_player\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"CustomPongEnv reset\")\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "        # print(self.starting_state_index)\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        player = starting_state['player']\n",
        "        opponent = starting_state['opponent']\n",
        "        self.computer_player.reset(serve_delay=random.randint(10, 10), initial_direction=random.randint(-60, 60), offset=random.randint(-PADDLE_HEIGHT/2, PADDLE_HEIGHT/2), max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
        "        self.reward_system = RewardSystem(rewarded_player=player)\n",
        "        self.step_count = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        self.reward_system.reset()\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 60}\n",
        "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
        "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
        "        terminated = self.game.update_game_state(actions, 2.5, self.reward_system)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        truncated = False\n",
        "        if self.step_count > 1000:\n",
        "            terminated = True\n",
        "        reward = self.reward_system.total_reward\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x'] / COURT_WIDTH),\n",
        "            float(state['ball']['y'] / COURT_HEIGHT),\n",
        "            float(state['ball']['dx'] / 40),\n",
        "            float(state['ball']['dy'] / 40),\n",
        "            float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
        "            float(paddle['y'] / COURT_HEIGHT),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(int(is_server)),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c286d90c-20fe-4e60-ac83-b8f82cca64d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: [[0.9691667 0.55625   0.25      0.25      1.        0.44375   1.\n",
            "  1.       ]]\n",
            "Action taken: [ 0.15970048 -0.43001705]\n",
            "Observation: [[0.9691667 0.55625   0.25      0.        1.        0.3631218 1.\n",
            "  1.       ]]\n",
            "Reward: [10.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [0.9213258 0.6158111]\n",
            "Observation: [[ 0.9691667   0.542812   -0.25       -0.10750426  1.          0.47858638\n",
            "   0.          1.        ]]\n",
            "Reward: [2.0143578]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [ 0.92651963 -0.37926456]\n",
            "Observation: [[ 0.9483333   0.52937394 -0.25       -0.10750426  1.          0.40747428\n",
            "   0.          1.        ]]\n",
            "Reward: [0.93832016]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [0.04438894 0.39902732]\n",
            "Observation: [[ 0.9275      0.5159359  -0.25       -0.10750426  1.          0.4822919\n",
            "   0.          1.        ]]\n",
            "Reward: [0.7932152]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [ 0.7562352  -0.08482796]\n",
            "Observation: [[ 0.9066667   0.50249785 -0.25       -0.10750426  1.          0.46638665\n",
            "   0.          1.        ]]\n",
            "Reward: [0.16054651]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [ 0.94306153 -0.9265007 ]\n",
            "Observation: [[ 0.8858333   0.48905984 -0.25       -0.10750426  1.          0.29266778\n",
            "   0.          1.        ]]\n",
            "Reward: [1.2733748]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [0.3615513 0.6175205]\n",
            "Observation: [[ 0.865       0.4756218  -0.25       -0.10750426  1.          0.40845287\n",
            "   0.          1.        ]]\n",
            "Reward: [0.6682535]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [0.43734843 0.00066789]\n",
            "Observation: [[ 0.8441667   0.46218377 -0.25       -0.10750426  1.          0.4085781\n",
            "   0.          1.        ]]\n",
            "Reward: [0.00066833]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [ 0.8426002 -0.6683715]\n",
            "Observation: [[ 0.8233333   0.44874573 -0.25       -0.10750426  1.          0.28325844\n",
            "   0.          1.        ]]\n",
            "Reward: [0.59358615]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [ 0.16022083 -0.73154426]\n",
            "Observation: [[ 0.8025      0.4353077  -0.25       -0.10750426  1.          0.1460939\n",
            "   0.          1.        ]]\n",
            "Reward: [0.56524676]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [0.25651035 0.75540185]\n",
            "Observation: [[ 0.7816667   0.42186967 -0.25       -0.10750426  1.          0.28773174\n",
            "   0.          1.        ]]\n",
            "Reward: [0.50721]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [0.13558096 0.9623809 ]\n",
            "Observation: [[ 0.7608333   0.40843165 -0.25       -0.10750426  1.          0.46817815\n",
            "   0.          1.        ]]\n",
            "Reward: [0.55622536]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [ 0.85750633 -0.4124424 ]\n",
            "Observation: [[ 0.74        0.3949936  -0.25       -0.10750426  1.          0.3908452\n",
            "   0.          1.        ]]\n",
            "Reward: [0.21490034]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [ 0.06811951 -0.12498761]\n",
            "Observation: [[ 0.7191667   0.38155556 -0.25       -0.10750426  1.          0.36741003\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06114268]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [ 0.6820588  -0.01625351]\n",
            "Observation: [[ 0.6983333   0.36811754 -0.25       -0.10750426  1.          0.3643625\n",
            "   0.          1.        ]]\n",
            "Reward: [0.00766711]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [ 0.10766368 -0.38569123]\n",
            "Observation: [[ 0.6775      0.3546795  -0.25       -0.10750426  1.          0.2920454\n",
            "   0.          1.        ]]\n",
            "Reward: [0.17578582]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [0.8295189 0.6427156]\n",
            "Observation: [[ 0.6566667   0.34124148 -0.25       -0.10750426  1.          0.41255456\n",
            "   0.          1.        ]]\n",
            "Reward: [0.2808077]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [ 0.19267593 -0.08911251]\n",
            "Observation: [[ 0.6358333   0.32780343 -0.25       -0.10750426  1.          0.39584598\n",
            "   0.          1.        ]]\n",
            "Reward: [0.03783754]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [0.36301064 0.94303143]\n",
            "Observation: [[ 0.615       0.31436542 -0.25       -0.10750426  1.          0.5726644\n",
            "   0.          1.        ]]\n",
            "Reward: [0.38257203]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [ 0.6626239  -0.42638573]\n",
            "Observation: [[ 0.5941667   0.30092737 -0.25       -0.10750426  1.          0.49271706\n",
            "   0.          1.        ]]\n",
            "Reward: [0.16590561]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [ 0.8778073  -0.07953335]\n",
            "Observation: [[ 0.5733333   0.28748935 -0.25       -0.10750426  1.          0.47780454\n",
            "   0.          1.        ]]\n",
            "Reward: [0.03004926]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [ 0.63465387 -0.26171914]\n",
            "Observation: [[ 0.5525      0.2740513  -0.25       -0.10750426  1.          0.42873222\n",
            "   0.          1.        ]]\n",
            "Reward: [0.096434]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [0.46783575 0.9438399 ]\n",
            "Observation: [[ 0.5316667   0.26061326 -0.25       -0.10750426  1.          0.60570216\n",
            "   0.          1.        ]]\n",
            "Reward: [0.33502254]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [0.31484592 0.70494175]\n",
            "Observation: [[ 0.5108333   0.24717525 -0.25       -0.10750426  1.          0.73787874\n",
            "   0.          1.        ]]\n",
            "Reward: [0.2401219]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [0.2607073  0.63350075]\n",
            "Observation: [[ 0.49        0.23373722 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.20689674]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [0.996793   0.85567296]\n",
            "Observation: [[ 0.46916667  0.22029917 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.26665962]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [ 0.05235558 -0.9276111 ]\n",
            "Observation: [[ 0.44833332  0.20686114 -0.25       -0.10750426  1.          0.6010729\n",
            "   0.          1.        ]]\n",
            "Reward: [0.27462086]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [0.34067217 0.78387713]\n",
            "Observation: [[ 0.4275      0.1934231  -0.25       -0.10750426  1.          0.74804986\n",
            "   0.          1.        ]]\n",
            "Reward: [0.22050303]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [ 0.5640795  -0.15495431]\n",
            "Observation: [[ 0.40666667  0.17998508 -0.25       -0.10750426  1.          0.7189959\n",
            "   0.          1.        ]]\n",
            "Reward: [0.04189067]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [ 0.36765784 -0.01100439]\n",
            "Observation: [[ 0.38583332  0.16654705 -0.25       -0.10750426  1.          0.7169326\n",
            "   0.          1.        ]]\n",
            "Reward: [0.00288859]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [ 0.9255653 -0.9187112]\n",
            "Observation: [[ 0.365       0.15310901 -0.25       -0.10750426  1.          0.5446743\n",
            "   0.          1.        ]]\n",
            "Reward: [0.23312993]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [0.67687935 0.7585601 ]\n",
            "Observation: [[ 0.34416667  0.13967098 -0.25       -0.10750426  1.          0.6869043\n",
            "   0.          1.        ]]\n",
            "Reward: [0.18587317]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [0.76754725 0.7984204 ]\n",
            "Observation: [[ 0.32333332  0.12623295 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.18870512]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [0.31502423 0.64980656]\n",
            "Observation: [[ 0.3025      0.11279491 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.14831498]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [0.5553809  0.21414886]\n",
            "Observation: [[ 0.28166667  0.09935688 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.04748409]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [0.75947005 0.23426072]\n",
            "Observation: [[ 0.26083332  0.08591884 -0.25       -0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0506871]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [ 0.05441369 -0.3110936 ]\n",
            "Observation: [[ 0.24        0.07248081 -0.25       -0.10750426  1.          0.71667\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06587288]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [0.1916514  0.13297859]\n",
            "Observation: [[ 0.21916667  0.05904278 -0.25       -0.10750426  1.          0.74160343\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02766244]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [ 0.72799313 -0.30691415]\n",
            "Observation: [[ 0.19833334  0.04560475 -0.25       -0.10750426  1.          0.68405706\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06283516]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [ 0.4173611 -0.7499393]\n",
            "Observation: [[ 0.1775      0.03216672 -0.25       -0.10750426  1.          0.5434434\n",
            "   0.          1.        ]]\n",
            "Reward: [0.15084329]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [0.19442452 0.7630964 ]\n",
            "Observation: [[ 0.15666667  0.01872868 -0.25       -0.10750426  1.          0.686524\n",
            "   0.          1.        ]]\n",
            "Reward: [0.150561]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [0.1634511  0.28618917]\n",
            "Observation: [[ 0.13583334  0.015      -0.25        0.10750426  1.          0.7401844\n",
            "   0.          1.        ]]\n",
            "Reward: [0.05550828]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [ 0.6894472  -0.93064374]\n",
            "Observation: [[ 0.115       0.02843803 -0.25        0.10750426  1.          0.5656887\n",
            "   0.          1.        ]]\n",
            "Reward: [0.17698596]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [0.11507872 0.4544057 ]\n",
            "Observation: [[ 0.09416667  0.04187607 -0.25        0.10750426  1.          0.6508898\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0848244]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [0.46420476 0.83444446]\n",
            "Observation: [[ 0.07333333  0.0553141  -0.25        0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.15266486]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [0.42413974 0.04587522]\n",
            "Observation: [[ 0.0525      0.06875213 -0.25        0.10750426  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.00825665]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [ 0.32295036 -0.16554937]\n",
            "Observation: [[ 0.03166667  0.08219016 -0.25        0.10750426  1.          0.7439595\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02938166]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [0.4008624 0.3208219]\n",
            "Observation: [[ 0.03083333  0.09562819  0.23130512 -0.19724268  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0562155]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [0.8273337  0.25237888]\n",
            "Observation: [[ 0.05010876  0.07097286  0.23130512 -0.19724268  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.04372149]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [0.8259819  0.05134119]\n",
            "Observation: [[ 0.06938419  0.04631753  0.23130512 -0.19724268  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.00881249]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [0.04214998 0.35008442]\n",
            "Observation: [[ 0.08865961  0.02166219  0.23130512 -0.19724268  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.05956898]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [ 0.8864499 -0.3502962]\n",
            "Observation: [[0.10793504 0.015      0.23130512 0.19724268 1.         0.7093195\n",
            "  0.         1.        ]]\n",
            "Reward: [0.05911525]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [0.81366456 0.944929  ]\n",
            "Observation: [[0.12721047 0.03965534 0.23130512 0.19724268 1.         0.775\n",
            "  0.         1.        ]]\n",
            "Reward: [0.15781182]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [ 0.7358108 -0.5874079]\n",
            "Observation: [[0.1464859  0.06431067 0.23130512 0.19724268 1.         0.664861\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09704725]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [0.1992552 0.5066729]\n",
            "Observation: [[0.16576132 0.088966   0.23130512 0.19724268 1.         0.7598622\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08280933]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [ 0.62021524 -0.24255487]\n",
            "Observation: [[0.18503675 0.11362134 0.23130512 0.19724268 1.         0.7143831\n",
            "  0.         1.        ]]\n",
            "Reward: [0.03926084]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [0.5012674 0.5603971]\n",
            "Observation: [[0.20431218 0.13827668 0.23130512 0.19724268 1.         0.775\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08981048]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [ 0.8357058  -0.00255917]\n",
            "Observation: [[0.2235876  0.16293201 0.23130512 0.19724268 1.         0.77452016\n",
            "  0.         1.        ]]\n",
            "Reward: [0.00040686]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [0.523596   0.76652586]\n",
            "Observation: [[0.24286303 0.18758735 0.23130512 0.19724268 1.         0.775\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12075585]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [0.50499696 0.21594146]\n",
            "Observation: [[0.26213846 0.21224268 0.23130512 0.19724268 1.         0.775\n",
            "  0.         1.        ]]\n",
            "Reward: [0.03374337]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [ 0.25814325 -0.7686585 ]\n",
            "Observation: [[0.28141388 0.23689802 0.23130512 0.19724268 1.         0.63087654\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11901387]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [0.23853633 0.7598518 ]\n",
            "Observation: [[0.3006893  0.26155335 0.23130512 0.19724268 1.         0.77334875\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11646581]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [ 0.89965016 -0.83528113]\n",
            "Observation: [[0.31996474 0.2862087  0.23130512 0.19724268 1.         0.61673355\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12659603]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.00972419 -0.53106576]\n",
            "Observation: [[0.33924016 0.31086403 0.23130512 0.19724268 1.         0.5171587\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07959647]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [0.7301025 0.2613758]\n",
            "Observation: [[0.3585156  0.33551937 0.23130512 0.19724268 1.         0.56616664\n",
            "  0.         1.        ]]\n",
            "Reward: [0.03878097]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [ 0.3506057  -0.10926045]\n",
            "Observation: [[0.37779102 0.3601747  0.23130512 0.19724268 1.         0.54568034\n",
            "  0.         1.        ]]\n",
            "Reward: [0.01607087]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [0.0632096  0.40803266]\n",
            "Observation: [[0.39706644 0.38483003 0.23130512 0.19724268 1.         0.6221865\n",
            "  0.         1.        ]]\n",
            "Reward: [0.05951618]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [ 0.53994876 -0.8612739 ]\n",
            "Observation: [[0.41634187 0.40948537 0.23130512 0.19724268 1.         0.4606976\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12444141]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [0.9175492 0.0821913]\n",
            "Observation: [[0.4356173  0.4341407  0.23130512 0.19724268 1.         0.47610846\n",
            "  0.         1.        ]]\n",
            "Reward: [0.01177973]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [ 0.45387685 -0.5022901 ]\n",
            "Observation: [[0.45489272 0.45879605 0.23130512 0.19724268 1.         0.38192907\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07140956]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [ 0.58647734 -0.5422295 ]\n",
            "Observation: [[0.47416815 0.48345137 0.23130512 0.19724268 1.         0.28026104\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07646058]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [0.6822443  0.19880328]\n",
            "Observation: [[0.49344358 0.5081067  0.23130512 0.19724268 1.         0.31753665\n",
            "  0.         1.        ]]\n",
            "Reward: [0.02783014]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [ 0.927424   -0.15808521]\n",
            "Observation: [[0.51271904 0.53276205 0.23130512 0.19724268 1.         0.28789568\n",
            "  0.         1.        ]]\n",
            "Reward: [0.02198935]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [0.82349163 0.17015311]\n",
            "Observation: [[0.53199446 0.5574174  0.23130512 0.19724268 1.         0.3197994\n",
            "  0.         1.        ]]\n",
            "Reward: [0.02353579]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [ 0.43236825 -0.07072271]\n",
            "Observation: [[0.5512699  0.58207273 0.23130512 0.19724268 1.         0.30653888\n",
            "  0.         1.        ]]\n",
            "Reward: [0.00973696]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [ 0.6300458  -0.33119935]\n",
            "Observation: [[0.5705453  0.6067281  0.23130512 0.19724268 1.         0.244439\n",
            "  0.         1.        ]]\n",
            "Reward: [0.04539802]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [0.21521369 0.71920705]\n",
            "Observation: [[0.58982074 0.63138336 0.23130512 0.19724268 1.         0.3792903\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09808382]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [ 0.9704269 -0.9291268]\n",
            "Observation: [[0.60909617 0.6560387  0.23130512 0.19724268 1.         0.20507905\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12592891]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [0.2927714 0.2933152]\n",
            "Observation: [[0.6283716  0.68069404 0.23130512 0.19724268 1.         0.26007566\n",
            "  0.         1.        ]]\n",
            "Reward: [0.03952629]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [0.8358242  0.21437702]\n",
            "Observation: [[0.647647   0.7053494  0.23130512 0.19724268 1.         0.30027133\n",
            "  0.         1.        ]]\n",
            "Reward: [0.02873998]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [ 0.13248496 -0.38854086]\n",
            "Observation: [[0.66692245 0.7300047  0.23130512 0.19724268 1.         0.22741993\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0518287]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [0.06963552 0.34815633]\n",
            "Observation: [[0.6861979  0.75466007 0.23130512 0.19724268 1.         0.29269925\n",
            "  0.         1.        ]]\n",
            "Reward: [0.04622065]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [ 0.75318104 -0.50540626]\n",
            "Observation: [[0.7054733  0.7793154  0.23130512 0.19724268 1.         0.19793557\n",
            "  0.         1.        ]]\n",
            "Reward: [0.06677021]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [0.7753281  0.94960403]\n",
            "Observation: [[0.72474873 0.80397075 0.23130512 0.19724268 1.         0.37598634\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12471219]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [ 0.08740322 -0.9705449 ]\n",
            "Observation: [[0.74402416 0.8286261  0.23130512 0.19724268 1.         0.19400916\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12657458]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 0.07700402 -0.02810077]\n",
            "Observation: [[0.7632996  0.8532814  0.23130512 0.19724268 1.         0.18874027\n",
            "  0.         1.        ]]\n",
            "Reward: [0.00364315]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [ 0.9941448  -0.10567989]\n",
            "Observation: [[0.782575   0.8779367  0.23130512 0.19724268 1.         0.16892529\n",
            "  0.         1.        ]]\n",
            "Reward: [0.01363098]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [0.36113623 0.44994843]\n",
            "Observation: [[0.80185044 0.90259206 0.23130512 0.19724268 1.         0.25329062\n",
            "  0.         1.        ]]\n",
            "Reward: [0.05774226]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [0.6665205 0.4335429]\n",
            "Observation: [[0.82112586 0.9272474  0.23130512 0.19724268 1.         0.3345799\n",
            "  0.         1.        ]]\n",
            "Reward: [0.05535986]\n",
            "iteration: 88\n",
            "Done: [False]\n",
            "Action taken: [0.13815068 0.87129587]\n",
            "Observation: [[0.8404013  0.95190275 0.23130512 0.19724268 1.         0.49794787\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11061725]\n",
            "iteration: 89\n",
            "Done: [False]\n",
            "Action taken: [ 0.25869653 -0.43778318]\n",
            "Observation: [[0.8596767  0.9765581  0.23130512 0.19724268 1.         0.41586354\n",
            "  0.         1.        ]]\n",
            "Reward: [0.05526666]\n",
            "iteration: 90\n",
            "Done: [False]\n",
            "Action taken: [ 0.9904678 -0.7359917]\n",
            "Observation: [[ 0.87895215  0.985       0.23130512 -0.19724268  1.          0.27786508\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09234705]\n",
            "iteration: 91\n",
            "Done: [False]\n",
            "Action taken: [ 0.6008218  -0.37192056]\n",
            "Observation: [[ 0.8982276   0.9603447   0.23130512 -0.19724268  1.          0.20812999\n",
            "   0.          1.        ]]\n",
            "Reward: [0.04639443]\n",
            "iteration: 92\n",
            "Done: [False]\n",
            "Action taken: [0.3901248 0.8183712]\n",
            "Observation: [[ 0.917503    0.93568933  0.23130512 -0.19724268  1.          0.3615746\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10143187]\n",
            "iteration: 93\n",
            "Done: [False]\n",
            "Action taken: [0.8736566  0.72546315]\n",
            "Observation: [[ 0.9367784   0.911034    0.23130512 -0.19724268  1.          0.49759892\n",
            "   0.          1.        ]]\n",
            "Reward: [0.08930669]\n",
            "iteration: 94\n",
            "Done: [False]\n",
            "Action taken: [ 0.3051584  -0.97950953]\n",
            "Observation: [[ 0.95605385  0.88637865  0.23130512 -0.19724268  1.          0.31394088\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11966135]\n",
            "iteration: 95\n",
            "Done: [False]\n",
            "Action taken: [0.74737144 0.33617318]\n",
            "Observation: [[ 0.9753293   0.8617233   0.23130512 -0.19724268  1.          0.37697336\n",
            "   0.          1.        ]]\n",
            "Reward: [0.04077408]\n",
            "iteration: 96\n",
            "Done: [False]\n",
            "Action taken: [ 0.68025476 -0.8095531 ]\n",
            "Observation: [[ 0.9946047   0.83706796  0.23130512 -0.19724268  1.          0.22518216\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09343421]\n",
            "iteration: 97\n",
            "Done: [False]\n",
            "Action taken: [0.53036875 0.43832052]\n",
            "Observation: [[0.9691667 0.55625   0.25      0.25      1.        0.44375   1.\n",
            "  1.       ]]\n",
            "Reward: [0.]\n",
            "iteration: 98\n",
            "Done: [ True]\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R172XbXX5Am5",
        "outputId": "df9d5637-9c12-4932-bdc1-fcb367fa004a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([0.9691667, 0.55625  , 0.25     , 0.25     , 1.       , 0.44375  ,\n",
            "       1.       , 1.       ], dtype=float32), {})\n",
            "Action taken: [ 0.9963445  -0.11009466]\n",
            "Observation: [ 0.9691667   0.55625    -0.25        0.          1.          0.42310724\n",
            "  0.          1.        ]\n",
            "Reward: 2.01\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.41906592 -0.79476184]\n",
            "Observation: [ 0.9483333  0.55625   -0.25       0.         1.         0.2740894\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [0.9478894  0.87098277]\n",
            "Observation: [ 0.9275      0.55625    -0.25        0.          1.          0.43739867\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [0.93524736 0.6148688 ]\n",
            "Observation: [ 0.9066667  0.55625   -0.25       0.         1.         0.5526866\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [ 0.4522378 -0.5251924]\n",
            "Observation: [ 0.8858333  0.55625   -0.25       0.         1.         0.454213\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [0.03803585 0.8478745 ]\n",
            "Observation: [ 0.865       0.55625    -0.25        0.          1.          0.61318946\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [0.42516726 0.6377705 ]\n",
            "Observation: [ 0.8441667   0.55625    -0.25        0.          1.          0.73277146\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [ 0.4881051  -0.11752756]\n",
            "Observation: [ 0.8233333  0.55625   -0.25       0.         1.         0.710735\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.31020772 -0.33025002]\n",
            "Observation: [ 0.8025     0.55625   -0.25       0.         1.         0.6488131\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [ 0.7774263  -0.10741559]\n",
            "Observation: [ 0.7816667  0.55625   -0.25       0.         1.         0.6286727\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [ 0.53772575 -0.87638557]\n",
            "Observation: [ 0.7608333   0.55625    -0.25        0.          1.          0.46435043\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.08503608 -0.7140222 ]\n",
            "Observation: [ 0.74        0.55625    -0.25        0.          1.          0.33047128\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [ 0.7387859  -0.48030448]\n",
            "Observation: [ 0.7191667   0.55625    -0.25        0.          1.          0.24041417\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [ 0.6035119  -0.52194566]\n",
            "Observation: [ 0.6983333   0.55625    -0.25        0.          1.          0.14254937\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [0.31587932 0.17943428]\n",
            "Observation: [ 0.6775     0.55625   -0.25       0.         1.         0.1761933\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [ 0.1019162 -0.9242682]\n",
            "Observation: [ 0.6566667   0.55625    -0.25        0.          1.          0.00289301\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.36609307 -0.43404204]\n",
            "Observation: [ 0.6358333  0.55625   -0.25       0.         1.         0.\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [ 0.5347734  -0.38692835]\n",
            "Observation: [ 0.615    0.55625 -0.25     0.       1.       0.       0.       1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [0.91601294 0.15265109]\n",
            "Observation: [ 0.5941667   0.55625    -0.25        0.          1.          0.02862208\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [0.398003   0.07964855]\n",
            "Observation: [ 0.5733333   0.55625    -0.25        0.          1.          0.04355618\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [0.39916646 0.1292825 ]\n",
            "Observation: [ 0.5525      0.55625    -0.25        0.          1.          0.06779665\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [0.7093783  0.25447145]\n",
            "Observation: [ 0.5316667   0.55625    -0.25        0.          1.          0.11551005\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [0.14736481 0.68784934]\n",
            "Observation: [ 0.5108333  0.55625   -0.25       0.         1.         0.2444818\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [0.23216777 0.83784705]\n",
            "Observation: [ 0.49        0.55625    -0.25        0.          1.          0.40157813\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [0.24508575 0.45326853]\n",
            "Observation: [ 0.46916667  0.55625    -0.25        0.          1.          0.48656598\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.6082919 -0.7518596]\n",
            "Observation: [ 0.44833332  0.55625    -0.25        0.          1.          0.3455923\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [ 0.32505852 -0.38478336]\n",
            "Observation: [ 0.4275      0.55625    -0.25        0.          1.          0.27344543\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [0.6040326  0.60282147]\n",
            "Observation: [ 0.40666667  0.55625    -0.25        0.          1.          0.38647443\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [0.98602897 0.17018402]\n",
            "Observation: [ 0.38583332  0.55625    -0.25        0.          1.          0.41838396\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [ 0.63128126 -0.5204453 ]\n",
            "Observation: [ 0.365       0.55625    -0.25        0.          1.          0.32080045\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [ 0.28990972 -0.5961987 ]\n",
            "Observation: [ 0.34416667  0.55625    -0.25        0.          1.          0.20901321\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [ 0.44520256 -0.8035426 ]\n",
            "Observation: [ 0.32333332  0.55625    -0.25        0.          1.          0.05834896\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [0.0721717  0.16888203]\n",
            "Observation: [ 0.3025      0.55625    -0.25        0.          1.          0.09001434\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [0.45970505 0.57884866]\n",
            "Observation: [ 0.28166667  0.55625    -0.25        0.          1.          0.19854847\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [0.07472283 0.72632   ]\n",
            "Observation: [ 0.26083332  0.55625    -0.25        0.          1.          0.3347335\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [0.10324377 0.44873205]\n",
            "Observation: [ 0.24        0.55625    -0.25        0.          1.          0.41887072\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [0.41043875 0.23036084]\n",
            "Observation: [ 0.21916667  0.55625    -0.25        0.          1.          0.46206337\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [ 0.6343284  -0.90636045]\n",
            "Observation: [ 0.19833334  0.55625    -0.25        0.          1.          0.2921208\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [0.4116054  0.68321633]\n",
            "Observation: [ 0.1775      0.55625    -0.25        0.          1.          0.42022386\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.6908958 -0.6039947]\n",
            "Observation: [ 0.15666667  0.55625    -0.25        0.          1.          0.30697486\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [ 0.2680218  -0.60129976]\n",
            "Observation: [ 0.13583334  0.55625    -0.25        0.          1.          0.19423115\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [0.34244713 0.7959932 ]\n",
            "Observation: [ 0.115       0.55625    -0.25        0.          1.          0.34347987\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [0.01061163 0.8057704 ]\n",
            "Observation: [ 0.09416667  0.55625    -0.25        0.          1.          0.49456182\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [0.5313101 0.7677056]\n",
            "Observation: [ 0.07333333  0.55625    -0.25        0.          1.          0.63850665\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [ 0.49532962 -0.02822956]\n",
            "Observation: [ 0.0525     0.55625   -0.25       0.         1.         0.6332136\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [ 0.7130848  -0.56966096]\n",
            "Observation: [ 0.03166667  0.55625    -0.25        0.          1.          0.5264022\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [ 0.39415446 -0.2545316 ]\n",
            "Observation: [0.03083333 0.55625    0.30158833 0.08185805 1.         0.47867748\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [ 0.31554475 -0.4851148 ]\n",
            "Observation: [0.05596569 0.56648225 0.30158833 0.08185805 1.         0.38771847\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.79618365 -0.8987087 ]\n",
            "Observation: [0.08109805 0.5767145  0.30158833 0.08185805 1.         0.21921058\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [0.3837831  0.48649183]\n",
            "Observation: [0.10623041 0.5869468  0.30158833 0.08185805 1.         0.31042778\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [0.3254838  0.59345925]\n",
            "Observation: [0.13136277 0.597179   0.30158833 0.08185805 1.         0.4217014\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [0.503824  0.3171709]\n",
            "Observation: [0.15649512 0.60741127 0.30158833 0.08185805 1.         0.48117095\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [0.0511052 0.6269883]\n",
            "Observation: [0.1816275  0.61764354 0.30158833 0.08185805 1.         0.5987312\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [0.14672232 0.6065616 ]\n",
            "Observation: [0.20675986 0.6278758  0.30158833 0.08185805 1.         0.71246153\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.70171237 -0.15253532]\n",
            "Observation: [0.23189221 0.638108   0.30158833 0.08185805 1.         0.6838612\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [0.3414206  0.14297664]\n",
            "Observation: [0.25702456 0.6483403  0.30158833 0.08185805 1.         0.7106693\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [0.59261584 0.9951077 ]\n",
            "Observation: [0.2821569  0.65857255 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [0.9902432  0.00300835]\n",
            "Observation: [0.30728927 0.6688048  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [ 0.45307377 -0.7779095 ]\n",
            "Observation: [0.33242166 0.6790371  0.30158833 0.08185805 1.         0.629142\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [0.00548865 0.60628283]\n",
            "Observation: [0.35755402 0.6892693  0.30158833 0.08185805 1.         0.74282\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [0.9139218 0.7221263]\n",
            "Observation: [0.38268638 0.6995016  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [0.91367465 0.2923604 ]\n",
            "Observation: [0.40781873 0.70973384 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [0.07837347 0.47194695]\n",
            "Observation: [0.4329511  0.7199661  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [0.40832415 0.6957447 ]\n",
            "Observation: [0.45808345 0.7301983  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [0.9809106  0.74865687]\n",
            "Observation: [0.4832158  0.7404306  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [0.02487038 0.00705958]\n",
            "Observation: [0.50834817 0.75066286 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [ 0.641034   -0.59737563]\n",
            "Observation: [0.5334805  0.76089513 0.30158833 0.08185805 1.         0.66299206\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [0.37973177 0.59209466]\n",
            "Observation: [0.5586129  0.77112734 0.30158833 0.08185805 1.         0.7740098\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [ 0.5926745 -0.8222752]\n",
            "Observation: [0.58374524 0.7813596  0.30158833 0.08185805 1.         0.61983323\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [0.44425917 0.92654103]\n",
            "Observation: [0.6088776  0.7915919  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [0.47490937 0.3081336 ]\n",
            "Observation: [0.63400996 0.80182415 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [0.10382925 0.697541  ]\n",
            "Observation: [0.6591423  0.81205636 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 71\n",
            "Done: False\n",
            "Action taken: [ 0.7169733 -0.8100465]\n",
            "Observation: [0.6842747  0.82228863 0.30158833 0.08185805 1.         0.62311625\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 72\n",
            "Done: False\n",
            "Action taken: [0.7920861  0.38169476]\n",
            "Observation: [0.70940703 0.8325209  0.30158833 0.08185805 1.         0.694684\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 73\n",
            "Done: False\n",
            "Action taken: [ 0.5698501  -0.07655038]\n",
            "Observation: [0.7345394  0.8427532  0.30158833 0.08185805 1.         0.6803309\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 74\n",
            "Done: False\n",
            "Action taken: [ 0.8767663  -0.11470309]\n",
            "Observation: [0.75967175 0.8529854  0.30158833 0.08185805 1.         0.658824\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 75\n",
            "Done: False\n",
            "Action taken: [0.47087753 0.31492713]\n",
            "Observation: [0.7848041  0.86321765 0.30158833 0.08185805 1.         0.71787286\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 76\n",
            "Done: False\n",
            "Action taken: [0.97458285 0.8020344 ]\n",
            "Observation: [0.80993646 0.8734499  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 77\n",
            "Done: False\n",
            "Action taken: [ 0.56678224 -0.10397571]\n",
            "Observation: [0.8350688  0.8836822  0.30158833 0.08185805 1.         0.75550455\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 78\n",
            "Done: False\n",
            "Action taken: [0.10787839 0.2550911 ]\n",
            "Observation: [0.8602012  0.8939144  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 79\n",
            "Done: False\n",
            "Action taken: [ 0.9372104  -0.29584152]\n",
            "Observation: [0.88533354 0.9041467  0.30158833 0.08185805 1.         0.7195297\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 80\n",
            "Done: False\n",
            "Action taken: [ 0.38820815 -0.0216769 ]\n",
            "Observation: [0.9104659  0.91437894 0.30158833 0.08185805 1.         0.7154653\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 81\n",
            "Done: False\n",
            "Action taken: [ 0.9353664 -0.7766226]\n",
            "Observation: [0.93559825 0.9246112  0.30158833 0.08185805 1.         0.56984854\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 82\n",
            "Done: False\n",
            "Action taken: [ 0.29275155 -0.21960439]\n",
            "Observation: [0.9607306  0.9348435  0.30158833 0.08185805 1.         0.52867275\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 83\n",
            "Done: False\n",
            "Action taken: [0.09222496 0.6252975 ]\n",
            "Observation: [0.985863   0.9450757  0.30158833 0.08185805 1.         0.64591604\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 84\n",
            "Done: False\n",
            "Action taken: [ 0.7645796 -0.9610336]\n",
            "Observation: [1.0109954  0.95530796 0.30158833 0.08185805 1.         0.46572223\n",
            " 0.         1.        ]\n",
            "Reward: -123.7367789238741\n",
            "iteration: 85\n",
            "Done: False\n",
            "Action taken: [0.85728043 0.9293642 ]\n",
            "Observation: [1.0109954  0.95530796 0.30158833 0.08185805 1.         0.46572223\n",
            " 0.         1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 86\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_sjyFuJcGT-w",
        "outputId": "10fc5e66-d81b-4e4e-b35b-e3e0fb4efbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Eval num_timesteps=8000, episode_reward=-10.01 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -10      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -10.01\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 99.8     |\n",
            "|    ep_rew_mean     | 2.68     |\n",
            "| time/              |          |\n",
            "|    fps             | 1155     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-10.01 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -10         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031522557 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3          |\n",
            "|    explained_variance   | -0.0202     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.205      |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.00176     |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 0.428       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 4000: mean reward -10.01\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 103      |\n",
            "|    ep_rew_mean     | 36.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 813      |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 20       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-10.01 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -10         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022588652 |\n",
            "|    clip_fraction        | 0.0919      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.23       |\n",
            "|    explained_variance   | 0.434       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.266      |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | 0.00428     |\n",
            "|    std                  | 1.26        |\n",
            "|    value_loss           | 0.225       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 6000: mean reward -10.01\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | 26.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 740      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-10.01 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -10         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023240501 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.44       |\n",
            "|    explained_variance   | 0.401       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.314      |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | 0.00296     |\n",
            "|    std                  | 1.41        |\n",
            "|    value_loss           | 0.235       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 8000: mean reward -10.01\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 112      |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 711      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=85.91 +/- 103.73\n",
            "Episode length: 133.80 +/- 40.95\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 134         |\n",
            "|    mean_reward          | 85.9        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021230001 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.65       |\n",
            "|    explained_variance   | 0.403       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.134      |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | 0.00156     |\n",
            "|    std                  | 1.56        |\n",
            "|    value_loss           | 0.271       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 10000: mean reward 85.91\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | 48       |\n",
            "| time/              |          |\n",
            "|    fps             | 739      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 55       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=79.38 +/- 61.00\n",
            "Episode length: 108.80 +/- 27.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 109         |\n",
            "|    mean_reward          | 79.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024401188 |\n",
            "|    clip_fraction        | 0.0955      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.85       |\n",
            "|    explained_variance   | 0.451       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.288      |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | 0.00244     |\n",
            "|    std                  | 1.74        |\n",
            "|    value_loss           | 0.227       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 12000: mean reward 79.38\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | 60       |\n",
            "| time/              |          |\n",
            "|    fps             | 761      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 64       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=59.43 +/- 74.89\n",
            "Episode length: 98.00 +/- 20.19\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 98         |\n",
            "|    mean_reward          | 59.4       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 56000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02044063 |\n",
            "|    clip_fraction        | 0.101      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.06      |\n",
            "|    explained_variance   | 0.404      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.32      |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | 0.000908   |\n",
            "|    std                  | 1.91       |\n",
            "|    value_loss           | 0.213      |\n",
            "----------------------------------------\n",
            "Evaluation at step 14000: mean reward 59.43\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 124      |\n",
            "|    ep_rew_mean     | 61.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 777      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=25.11 +/- 35.30\n",
            "Episode length: 109.40 +/- 36.05\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 109         |\n",
            "|    mean_reward          | 25.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017062433 |\n",
            "|    clip_fraction        | 0.0875      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.23       |\n",
            "|    explained_variance   | 0.445       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.416      |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | 0.00205     |\n",
            "|    std                  | 2.07        |\n",
            "|    value_loss           | 0.216       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 16000: mean reward 25.11\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | 57.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 789      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 83       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=96.61 +/- 92.24\n",
            "Episode length: 115.60 +/- 38.95\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 116         |\n",
            "|    mean_reward          | 96.6        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 72000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020346772 |\n",
            "|    clip_fraction        | 0.0974      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.41       |\n",
            "|    explained_variance   | 0.477       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.341      |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | 0.00174     |\n",
            "|    std                  | 2.28        |\n",
            "|    value_loss           | 0.176       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 96.61\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | 74.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 798      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 92       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=132.69 +/- 88.07\n",
            "Episode length: 133.80 +/- 29.73\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 134         |\n",
            "|    mean_reward          | 133         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023993615 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.61       |\n",
            "|    explained_variance   | 0.479       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.416      |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | 0.00239     |\n",
            "|    std                  | 2.53        |\n",
            "|    value_loss           | 0.154       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 20000: mean reward 132.69\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | 71.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 804      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 101      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=58.89 +/- 79.97\n",
            "Episode length: 107.40 +/- 25.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 107         |\n",
            "|    mean_reward          | 58.9        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 88000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017794508 |\n",
            "|    clip_fraction        | 0.087       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.81       |\n",
            "|    explained_variance   | 0.46        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.443      |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | 0.00402     |\n",
            "|    std                  | 2.77        |\n",
            "|    value_loss           | 0.185       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 22000: mean reward 58.89\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 136      |\n",
            "|    ep_rew_mean     | 103      |\n",
            "| time/              |          |\n",
            "|    fps             | 811      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 111      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=84.94 +/- 59.38\n",
            "Episode length: 135.40 +/- 61.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 135         |\n",
            "|    mean_reward          | 84.9        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016773356 |\n",
            "|    clip_fraction        | 0.0906      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.97       |\n",
            "|    explained_variance   | 0.514       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.424      |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | 0.00325     |\n",
            "|    std                  | 3           |\n",
            "|    value_loss           | 0.149       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 24000: mean reward 84.94\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 135      |\n",
            "|    ep_rew_mean     | 96.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 816      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 120      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=127.85 +/- 89.97\n",
            "Episode length: 172.60 +/- 54.30\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 173        |\n",
            "|    mean_reward          | 128        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 104000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01857271 |\n",
            "|    clip_fraction        | 0.0913     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.14      |\n",
            "|    explained_variance   | 0.509      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.462     |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | 0.000628   |\n",
            "|    std                  | 3.27       |\n",
            "|    value_loss           | 0.113      |\n",
            "----------------------------------------\n",
            "Evaluation at step 26000: mean reward 127.85\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 133      |\n",
            "|    ep_rew_mean     | 112      |\n",
            "| time/              |          |\n",
            "|    fps             | 819      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create and train\n",
        "ent_coef=0.1\n",
        "eval_freq=2000\n",
        "gamma=0.95\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef, gamma=gamma)\n",
        "model.learn(total_timesteps=100000, callback=eval_callback, progress_bar=False)\n",
        "\n",
        "# Save the model\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mL_LYMHeejpF",
        "outputId": "f449f78d-9b29-46b9-ff2b-ee1a705a98f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=60.90 +/- 177.47\n",
            "Episode length: 91.00 +/- 51.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91       |\n",
            "|    mean_reward     | 60.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward 60.90\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 88.9     |\n",
            "|    ep_rew_mean     | -155     |\n",
            "| time/              |          |\n",
            "|    fps             | 2511     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-36.31 +/- 375.27\n",
            "Episode length: 80.60 +/- 24.43\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 80.6         |\n",
            "|    mean_reward          | -36.3        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 16000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026602554 |\n",
            "|    clip_fraction        | 0.0154       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -31.3        |\n",
            "|    explained_variance   | 0.644        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.25        |\n",
            "|    n_updates            | 1670         |\n",
            "|    policy_gradient_loss | 0.000866     |\n",
            "|    std                  | 1.66e+06     |\n",
            "|    value_loss           | 1.05         |\n",
            "------------------------------------------\n",
            "Evaluation at step 4000: mean reward -36.31\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 90.7     |\n",
            "|    ep_rew_mean     | -182     |\n",
            "| time/              |          |\n",
            "|    fps             | 1294     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 12       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-132.53 +/- 251.19\n",
            "Episode length: 110.40 +/- 55.24\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 110          |\n",
            "|    mean_reward          | -133         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 24000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034748223 |\n",
            "|    clip_fraction        | 0.0204       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -31.4        |\n",
            "|    explained_variance   | 0.643        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.817       |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | 0.000718     |\n",
            "|    std                  | 1.74e+06     |\n",
            "|    value_loss           | 1            |\n",
            "------------------------------------------\n",
            "Evaluation at step 6000: mean reward -132.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 87.4     |\n",
            "|    ep_rew_mean     | -162     |\n",
            "| time/              |          |\n",
            "|    fps             | 1107     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 22       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-79.53 +/- 179.36\n",
            "Episode length: 75.40 +/- 24.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 75.4         |\n",
            "|    mean_reward          | -79.5        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 32000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041819187 |\n",
            "|    clip_fraction        | 0.0293       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -31.5        |\n",
            "|    explained_variance   | 0.685        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.13        |\n",
            "|    n_updates            | 1690         |\n",
            "|    policy_gradient_loss | -8.67e-06    |\n",
            "|    std                  | 1.82e+06     |\n",
            "|    value_loss           | 0.869        |\n",
            "------------------------------------------\n",
            "Evaluation at step 8000: mean reward -79.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 82.9     |\n",
            "|    ep_rew_mean     | -138     |\n",
            "| time/              |          |\n",
            "|    fps             | 1037     |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-161.25 +/- 304.57\n",
            "Episode length: 81.00 +/- 22.78\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 81          |\n",
            "|    mean_reward          | -161        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003354952 |\n",
            "|    clip_fraction        | 0.0209      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -31.6       |\n",
            "|    explained_variance   | 0.69        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.26       |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | 0.000685    |\n",
            "|    std                  | 1.9e+06     |\n",
            "|    value_loss           | 1           |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward -161.25\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91       |\n",
            "|    ep_rew_mean     | -83.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 998      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 41       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-83.64 +/- 280.54\n",
            "Episode length: 91.80 +/- 53.55\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 91.8         |\n",
            "|    mean_reward          | -83.6        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035257784 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -31.6        |\n",
            "|    explained_variance   | 0.673        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.22        |\n",
            "|    n_updates            | 1710         |\n",
            "|    policy_gradient_loss | 0.00086      |\n",
            "|    std                  | 1.98e+06     |\n",
            "|    value_loss           | 0.764        |\n",
            "------------------------------------------\n",
            "Evaluation at step 12000: mean reward -83.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 86.1     |\n",
            "|    ep_rew_mean     | -132     |\n",
            "| time/              |          |\n",
            "|    fps             | 971      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 50       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=7.79 +/- 239.77\n",
            "Episode length: 108.00 +/- 105.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 108         |\n",
            "|    mean_reward          | 7.79        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004568506 |\n",
            "|    clip_fraction        | 0.0348      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -31.7       |\n",
            "|    explained_variance   | 0.596       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.27       |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | -6.14e-05   |\n",
            "|    std                  | 2.09e+06    |\n",
            "|    value_loss           | 0.752       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward 7.79\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91.6     |\n",
            "|    ep_rew_mean     | -90.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 953      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 60       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-82.85 +/- 190.41\n",
            "Episode length: 118.00 +/- 49.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 118         |\n",
            "|    mean_reward          | -82.9       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004118869 |\n",
            "|    clip_fraction        | 0.0246      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -31.8       |\n",
            "|    explained_variance   | 0.644       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.16       |\n",
            "|    n_updates            | 1730        |\n",
            "|    policy_gradient_loss | 0.00048     |\n",
            "|    std                  | 2.18e+06    |\n",
            "|    value_loss           | 0.783       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 16000: mean reward -82.85\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91.3     |\n",
            "|    ep_rew_mean     | -127     |\n",
            "| time/              |          |\n",
            "|    fps             | 939      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 69       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=73.38 +/- 163.69\n",
            "Episode length: 68.40 +/- 26.11\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 68.4         |\n",
            "|    mean_reward          | 73.4         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 72000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048172483 |\n",
            "|    clip_fraction        | 0.0288       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -31.9        |\n",
            "|    explained_variance   | 0.622        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.34        |\n",
            "|    n_updates            | 1740         |\n",
            "|    policy_gradient_loss | 0.000178     |\n",
            "|    std                  | 2.28e+06     |\n",
            "|    value_loss           | 0.976        |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 73.38\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96.2     |\n",
            "|    ep_rew_mean     | -131     |\n",
            "| time/              |          |\n",
            "|    fps             | 931      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 79       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-15.66 +/- 195.94\n",
            "Episode length: 120.60 +/- 101.19\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 121          |\n",
            "|    mean_reward          | -15.7        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022998152 |\n",
            "|    clip_fraction        | 0.0128       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32          |\n",
            "|    explained_variance   | 0.653        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.847       |\n",
            "|    n_updates            | 1750         |\n",
            "|    policy_gradient_loss | 0.00125      |\n",
            "|    std                  | 2.36e+06     |\n",
            "|    value_loss           | 0.822        |\n",
            "------------------------------------------\n",
            "Evaluation at step 20000: mean reward -15.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 93.2     |\n",
            "|    ep_rew_mean     | -118     |\n",
            "| time/              |          |\n",
            "|    fps             | 922      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-153.08 +/- 231.60\n",
            "Episode length: 82.20 +/- 18.47\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 82.2        |\n",
            "|    mean_reward          | -153        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 88000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004510157 |\n",
            "|    clip_fraction        | 0.0278      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.1       |\n",
            "|    explained_variance   | 0.682       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.24       |\n",
            "|    n_updates            | 1760        |\n",
            "|    policy_gradient_loss | 0.000409    |\n",
            "|    std                  | 2.48e+06    |\n",
            "|    value_loss           | 0.936       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 22000: mean reward -153.08\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 88       |\n",
            "|    ep_rew_mean     | -154     |\n",
            "| time/              |          |\n",
            "|    fps             | 917      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-40.66 +/- 171.20\n",
            "Episode length: 94.20 +/- 21.63\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 94.2         |\n",
            "|    mean_reward          | -40.7        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 96000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028161216 |\n",
            "|    clip_fraction        | 0.016        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.2        |\n",
            "|    explained_variance   | 0.69         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.19        |\n",
            "|    n_updates            | 1770         |\n",
            "|    policy_gradient_loss | 0.00116      |\n",
            "|    std                  | 2.58e+06     |\n",
            "|    value_loss           | 0.845        |\n",
            "------------------------------------------\n",
            "Evaluation at step 24000: mean reward -40.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 93.1     |\n",
            "|    ep_rew_mean     | -136     |\n",
            "| time/              |          |\n",
            "|    fps             | 911      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 107      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=32.15 +/- 288.28\n",
            "Episode length: 71.60 +/- 22.54\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 71.6         |\n",
            "|    mean_reward          | 32.1         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034445212 |\n",
            "|    clip_fraction        | 0.0206       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.3        |\n",
            "|    explained_variance   | 0.67         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.27        |\n",
            "|    n_updates            | 1780         |\n",
            "|    policy_gradient_loss | 0.00064      |\n",
            "|    std                  | 2.7e+06      |\n",
            "|    value_loss           | 0.827        |\n",
            "------------------------------------------\n",
            "Evaluation at step 26000: mean reward 32.15\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 98.1     |\n",
            "|    ep_rew_mean     | -120     |\n",
            "| time/              |          |\n",
            "|    fps             | 908      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 117      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "gamma=0.999\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "# model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef, gamma=gamma)\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\", env=train_env, ent_coef=ent_coef, gamma=gamma)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "71c7d9ee-a0bc-4a9a-81d3-3515602701b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: -78.79866129999999 +/- 133.82650038538142\n"
          ]
        }
      ],
      "source": [
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "66361794-9545-4082-95dc-8c3c1900febc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n",
            "reset\n",
            "reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# model = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer())])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    # print(obs, action)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "      env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-hPY2KxRxsE",
        "outputId": "eab0d0ac-61fc-4c16-9188-19f3a634870d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Actions: [[-15.044194 370.17538 ]]\n",
            "ONNX Values: [[-6.150612]]\n",
            "ONNX Log Prob: [-31.393408]\n",
            "PyTorch Actions: [[-15.044194 370.17535 ]]\n",
            "PyTorch Values: [[-6.150613]]\n",
            "PyTorch Log Prob: [-31.393408]\n",
            "Actions match: True\n",
            "Values match: True\n",
            "Log prob match: True\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        # Run the policy in deterministic mode\n",
        "        actions, values, log_prob = self.policy(observation, deterministic=True)\n",
        "        return actions, values, log_prob\n",
        "\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\"\n",
        "model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "\n",
        "# Define dummy input based on the observation space shape\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_dynamo.onnx\"\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Create an ONNX runtime session\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "ort_inputs = {\"input\": observation}\n",
        "ort_outputs = ort_sess.run(None, ort_inputs)\n",
        "\n",
        "# Output from ONNX\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_outputs\n",
        "\n",
        "# Print ONNX outputs\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "# Check that the predictions are the same in PyTorch\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "\n",
        "# Print PyTorch outputs\n",
        "print(\"PyTorch Actions:\", pytorch_outputs[0].numpy())\n",
        "print(\"PyTorch Values:\", pytorch_outputs[1].numpy())\n",
        "print(\"PyTorch Log Prob:\", pytorch_outputs[2].numpy())\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = [output.numpy() for output in pytorch_outputs]\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "\n",
        "    actions_match = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match: {actions_match}\")\n",
        "    print(f\"Values match: {values_match}\")\n",
        "    print(f\"Log prob match: {log_prob_match}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(pytorch_outputs, ort_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx2tf import convert\n",
        "\n",
        "# Define paths\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Convert ONNX to TensorFlow SavedModel\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPSegjRq1cOI",
        "outputId": "6e388ea9-a671-4fd4-91b3-531c529265b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnx2tf/onnx2tf.py\", line 631, in convert\n",
            "    result = subprocess.check_output(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
            "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n",
            "\n",
            "\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Cast\u001b[35m onnx_op_name\u001b[0m: wa/policy/Cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.dtype\u001b[0m: \u001b[34mname\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Exp\u001b[35m onnx_op_name\u001b[0m: wa/policy/Exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: policy.log_std \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/policy/features_extractor/flatten/Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 8] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape/Reshape:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Constant_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_1/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_2/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_3/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_2/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_2/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_3/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_3/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/value_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.value_net.weight \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.value_net.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: values \u001b[36mshape\u001b[0m: [1, 1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_4/AddV2:0 \u001b[34mshape\u001b[0m: (1, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_5/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_5/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape/wa/policy/Shape:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ConstantOfShape\u001b[35m onnx_op_name\u001b[0m: wa/policy/ConstantOfShape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: constant\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.value\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.fill/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.fill/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_5/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_1/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m27 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_2/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m28 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_5/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_1/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_8/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m29 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_2/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_9/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m30 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_9/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m31 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Log\u001b[35m onnx_op_name\u001b[0m: wa/policy/Log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_9/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m32 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_8/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_8/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m33 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_5_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_8/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_1/Reshape:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m34 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_2_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_1/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m35 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_3_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_12/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m36 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Neg\u001b[35m onnx_op_name\u001b[0m: wa/policy/Neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_1/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m37 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/policy/Div\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_12/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m38 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_1/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m39 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_4_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_1/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_2/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m40 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceSum\u001b[35m onnx_op_name\u001b[0m: wa/policy/ReduceSum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: log_prob \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_sum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_2/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_sum_1/Sum:0 \u001b[34mshape\u001b[0m: (None,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.engine.functional.Functional at 0x7c1ed051c490>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnaMw0pAURsM",
        "outputId": "b27160df-3692-4e9a-89e8-692b9f9939d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow.js model saved at: /content/drive/MyDrive/wimblepong/26-thursday/ppo_custom_pong_1_tfjs_2\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tfjs_2\"\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow.js\n",
        "subprocess.run([\n",
        "    'tensorflowjs_converter',\n",
        "    '--input_format', 'tf_saved_model',\n",
        "    '--output_format', 'tfjs_graph_model',\n",
        "    \"--signature_name\", \"serving_default\",\n",
        "    tf_model_path,\n",
        "    tfjs_model_path\n",
        "])\n",
        "\n",
        "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}