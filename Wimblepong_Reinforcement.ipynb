{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "7c997a2e-4347-428c-b0e4-abec234f5aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sng4onnx\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: sng4onnx\n",
            "Successfully installed sng4onnx-1.0.4\n"
          ]
        }
      ],
      "source": [
        "# %pip install gym\n",
        "# %pip install stable-baselines3[extra]\n",
        "# %pip install tensorflowjs\n",
        "# %pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
        "\n",
        "# onnx2tf deps:\n",
        "# %pip install onnx_graphsurgeon\n",
        "%pip install sng4onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "94bed29d-4148-40aa-b52b-16c6deb354fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"24-monday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "2c201199-03b5-4523-85a6-3296c3386fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "import onnx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'blue', 'Player2': 'red'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    \"serve\": lambda ball_speed: ball_speed / 10,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "def bounded_value(value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N932taVhk14O"
      },
      "outputs": [],
      "source": [
        "class ComputerPlayer:\n",
        "    def __init__(self):\n",
        "        self.serve_delay = 50\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 15\n",
        "\n",
        "    def reset(self, serve_delay, initial_direction):\n",
        "        self.serve_delay = serve_delay\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = initial_direction\n",
        "\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4 ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iIpdvdwMk14O"
      },
      "outputs": [],
      "source": [
        "class PongGame:\n",
        "    def __init__(self, server, positions_reversed, player, opponent):\n",
        "        self.game_state = {\n",
        "        'server': server,\n",
        "        'positions_reversed': positions_reversed,\n",
        "        'player': player,\n",
        "        'opponent': opponent,\n",
        "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def update_game_state(self, actions, delta_time):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            return 0.01, True\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if player_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not player_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if player_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward, False\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self, computer_player):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "        self.starting_state_index = 0\n",
        "\n",
        "        self.computer_player = computer_player\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"Environment reset\")\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        player = starting_state['player']\n",
        "        opponent = starting_state['opponent']\n",
        "        self.computer_player.reset(serve_delay=random.randint(10, 10), initial_direction=random.randint(-60, 60))\n",
        "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 60}\n",
        "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
        "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
        "        reward, terminated = self.game.update_game_state(actions, 2.5)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x'] / COURT_WIDTH),\n",
        "            float(state['ball']['y'] / COURT_HEIGHT),\n",
        "            float(state['ball']['dx'] / 40),\n",
        "            float(state['ball']['dy'] / 40),\n",
        "            float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
        "            float(paddle['y'] / COURT_HEIGHT),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(int(is_server)),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w4LjxARS9zCF"
      },
      "outputs": [],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R172XbXX5Am5",
        "outputId": "a406dcc7-885d-4b00-e785-edd223bf3b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([0.9691667, 0.55625  , 0.25     , 0.25     , 0.       , 0.44375  ,\n",
            "       1.       , 0.       ], dtype=float32), {})\n",
            "Action taken: [ 0.31435114 -0.39158967]\n",
            "Observation: [0.9691667  0.55625    0.25       0.         0.         0.51717305\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [0.631885  0.5119813]\n",
            "Observation: [0.9691667  0.5697917  0.25       0.10833333 0.         0.42117658\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [ 0.9398458  -0.40603262]\n",
            "Observation: [0.9691667  0.5946181  0.25       0.19861111 0.         0.4973077\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [ 0.62816495 -0.1900346 ]\n",
            "Observation: [0.9691667 0.6288484 0.25      0.2738426 0.        0.5329392 1.\n",
            " 0.       ]\n",
            "Reward: 0.0\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [0.35966715 0.598566  ]\n",
            "Observation: [0.9691667  0.6709153  0.25       0.33653548 0.         0.42070806\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [0.883644  0.3263361]\n",
            "Observation: [0.9691667  0.7070128  0.25       0.2887796  0.         0.35952002\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [0.18020436 0.398753  ]\n",
            "Observation: [0.9691667  0.7235523  0.25       0.13231632 0.         0.28475386\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [0.10692368 0.6978647 ]\n",
            "Observation: [0.9691667  0.72379357 0.25       0.00193026 0.         0.15390421\n",
            " 1.         0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.17338207 -0.08278251]\n",
            "Observation: [ 0.9691667   0.710453    0.25       -0.10672478  0.          0.16942593\n",
            "  1.          0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [0.66083163 0.18592171]\n",
            "Observation: [ 0.9691667   0.6857942   0.25       -0.19727065  0.          0.1345656\n",
            "  1.          0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [ 0.21405058 -0.4241278 ]\n",
            "Observation: [ 0.9691667   0.6517035   0.25       -0.27272555  0.          0.21408957\n",
            "  1.          0.        ]\n",
            "Reward: 0.0\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.24881688 -0.9725436 ]\n",
            "Observation: [ 0.9691667  0.6097529 -0.25      -0.3356046  0.         0.3964415\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [0.11542077 0.04785924]\n",
            "Observation: [ 0.9483333  0.5678023 -0.25      -0.3356046  0.         0.3874679\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [ 0.0678439 -0.7087591]\n",
            "Observation: [ 0.9275      0.5258517  -0.25       -0.3356046   0.          0.52036023\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [ 0.8138022  -0.30327487]\n",
            "Observation: [ 0.9066667   0.48390114 -0.25       -0.3356046   0.          0.57722425\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [0.0462429  0.25615084]\n",
            "Observation: [ 0.8858333   0.4419506  -0.25       -0.3356046   0.          0.52919596\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.7446618  -0.95398474]\n",
            "Observation: [ 0.865       0.4        -0.25       -0.3356046   0.          0.70806813\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [0.423311   0.55055314]\n",
            "Observation: [ 0.8441667   0.35804942 -0.25       -0.3356046   0.          0.6048394\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [0.6197901  0.19397123]\n",
            "Observation: [ 0.8233333   0.31609884 -0.25       -0.3356046   0.          0.5684698\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [ 0.309317   -0.29379243]\n",
            "Observation: [ 0.8025      0.27414826 -0.25       -0.3356046   0.          0.6235559\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [ 0.40787923 -0.50060797]\n",
            "Observation: [ 0.7816667   0.23219769 -0.25       -0.3356046   0.          0.71741986\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [0.23928948 0.88508886]\n",
            "Observation: [ 0.7608333   0.19024712 -0.25       -0.3356046   0.          0.5514657\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [ 2.2981858e-05 -5.3539282e-01]\n",
            "Observation: [ 0.74        0.14829654 -0.25       -0.3356046   0.          0.6518519\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [ 0.93988407 -0.15071134]\n",
            "Observation: [ 0.7191667   0.10634596 -0.25       -0.3356046   0.          0.6801102\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [0.49683505 0.2959862 ]\n",
            "Observation: [ 0.6983333   0.06439538 -0.25       -0.3356046   0.          0.6246128\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [0.10470655 0.30197117]\n",
            "Observation: [ 0.6775     0.0224448 -0.25      -0.3356046  0.         0.5679932\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [ 0.61929446 -0.5938715 ]\n",
            "Observation: [ 0.6566667  0.015     -0.25       0.3356046  0.         0.6793441\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [ 0.6002527  -0.64613366]\n",
            "Observation: [ 0.6358333   0.05695058 -0.25        0.3356046   0.          0.8004942\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [0.78008366 0.97071195]\n",
            "Observation: [ 0.615       0.09890115 -0.25        0.3356046   0.          0.6184857\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [0.11806319 0.5028328 ]\n",
            "Observation: [ 0.5941667   0.14085174 -0.25        0.3356046   0.          0.52420455\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [ 0.22567944 -0.70332396]\n",
            "Observation: [ 0.5733333  0.1828023 -0.25       0.3356046  0.         0.6560778\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [0.67606866 0.4371393 ]\n",
            "Observation: [ 0.5525      0.22475289 -0.25        0.3356046   0.          0.5741142\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [0.61531955 0.4117124 ]\n",
            "Observation: [ 0.5316667   0.26670346 -0.25        0.3356046   0.          0.4969181\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [0.41635603 0.656852  ]\n",
            "Observation: [ 0.5108333   0.30865404 -0.25        0.3356046   0.          0.37375835\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [ 0.990893  -0.7085565]\n",
            "Observation: [ 0.49        0.35060462 -0.25        0.3356046   0.          0.5066127\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [0.30315435 0.91400623]\n",
            "Observation: [ 0.46916667  0.3925552  -0.25        0.3356046   0.          0.33523652\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [ 0.98545355 -0.3441743 ]\n",
            "Observation: [ 0.44833332  0.43450576 -0.25        0.3356046   0.          0.39976922\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [0.30548495 0.12864889]\n",
            "Observation: [ 0.4275      0.47645634 -0.25        0.3356046   0.          0.37564754\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [ 0.2650839 -0.175055 ]\n",
            "Observation: [ 0.40666667  0.5184069  -0.25        0.3356046   0.          0.40847036\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.30544308 -0.3997531 ]\n",
            "Observation: [ 0.38583332  0.5603575  -0.25        0.3356046   0.          0.48342407\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [0.26079696 0.9943896 ]\n",
            "Observation: [ 0.365      0.6023081 -0.25       0.3356046  0.         0.296976\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [ 0.40335834 -0.84651905]\n",
            "Observation: [ 0.34416667  0.6442587  -0.25        0.3356046   0.          0.45569834\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [ 0.99655795 -0.9998778 ]\n",
            "Observation: [ 0.32333332  0.68620926 -0.25        0.3356046   0.          0.6431754\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [ 0.56711173 -0.25020748]\n",
            "Observation: [ 0.3025      0.7281598  -0.25        0.3356046   0.          0.69008934\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [0.02120896 0.91596043]\n",
            "Observation: [ 0.28166667  0.77011037 -0.25        0.3356046   0.          0.5183467\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [ 0.6549797  -0.15304123]\n",
            "Observation: [ 0.26083332  0.81206095 -0.25        0.3356046   0.          0.54704195\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [0.4119727  0.03565068]\n",
            "Observation: [ 0.24        0.85401154 -0.25        0.3356046   0.          0.5403575\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [ 0.713957   -0.85238284]\n",
            "Observation: [ 0.21916667  0.8959621  -0.25        0.3356046   0.          0.7001793\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.05552301 -0.5992059 ]\n",
            "Observation: [ 0.19833334  0.9379127  -0.25        0.3356046   0.          0.81253034\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [ 0.69524384 -0.73586494]\n",
            "Observation: [ 0.1775     0.9798633 -0.25       0.3356046  0.         0.8875\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [0.38518468 0.5927618 ]\n",
            "Observation: [ 0.15666667  0.985      -0.25       -0.3356046   0.          0.7763572\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [ 0.84046054 -0.44715762]\n",
            "Observation: [ 0.13583334  0.94304943 -0.25       -0.3356046   0.          0.8601992\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [0.28057003 0.6084329 ]\n",
            "Observation: [ 0.115       0.90109885 -0.25       -0.3356046   0.          0.74611807\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [ 0.7085699 -0.6865312]\n",
            "Observation: [ 0.09416667  0.85914826 -0.25       -0.3356046   0.          0.87484264\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [0.53606874 0.7676332 ]\n",
            "Observation: [ 0.07333333  0.8171977  -0.25       -0.3356046   0.          0.73091143\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [ 0.2780863  -0.47961938]\n",
            "Observation: [ 0.0525      0.7752471  -0.25       -0.3356046   0.          0.82084006\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [0.18727104 0.26567146]\n",
            "Observation: [ 0.03166667  0.7332965  -0.25       -0.3356046   0.          0.7710267\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [ 0.92460644 -0.37993655]\n",
            "Observation: [ 0.01083333  0.69134593 -0.25       -0.3356046   0.          0.8422648\n",
            "  0.          0.        ]\n",
            "Reward: 0.01\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [ 0.41303945 -0.9989623 ]\n",
            "Observation: [-0.01       0.6493954 -0.25      -0.3356046  0.         0.8875\n",
            "  0.         0.       ]\n",
            "Reward: -154.28550081673825\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [ 0.27817032 -0.05381011]\n",
            "Observation: [-0.01       0.6493954 -0.25      -0.3356046  0.         0.8875\n",
            "  0.         0.       ]\n",
            "Reward: 0.01\n",
            "iteration: 59\n",
            "Done: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "_sjyFuJcGT-w",
        "outputId": "3a195b14-acd8-4d08-9ca9-a0215d2eaf38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Eval num_timesteps=8000, episode_reward=-125.14 +/- 75.85\n",
            "Episode length: 94.00 +/- 41.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | -125     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -125.14\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 79.3     |\n",
            "|    ep_rew_mean     | -264     |\n",
            "| time/              |          |\n",
            "|    fps             | 2977     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-291.64 +/- 109.39\n",
            "Episode length: 60.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 60          |\n",
            "|    mean_reward          | -292        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020729527 |\n",
            "|    clip_fraction        | 0.0967      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.97       |\n",
            "|    explained_variance   | -0.00599    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.27        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.000817    |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 2.44        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 4000: mean reward -291.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 84       |\n",
            "|    ep_rew_mean     | -216     |\n",
            "| time/              |          |\n",
            "|    fps             | 1234     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-195.66 +/- 88.03\n",
            "Episode length: 60.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 60         |\n",
            "|    mean_reward          | -196       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 24000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01763801 |\n",
            "|    clip_fraction        | 0.0828     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.16      |\n",
            "|    explained_variance   | 0.353      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.583      |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | 0.00225    |\n",
            "|    std                  | 1.21       |\n",
            "|    value_loss           | 1.69       |\n",
            "----------------------------------------\n",
            "Evaluation at step 6000: mean reward -195.66\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 77.8     |\n",
            "|    ep_rew_mean     | -223     |\n",
            "| time/              |          |\n",
            "|    fps             | 1056     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-129.63 +/- 121.49\n",
            "Episode length: 91.40 +/- 62.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 91.4        |\n",
            "|    mean_reward          | -130        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018756587 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.33       |\n",
            "|    explained_variance   | 0.321       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.15        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00125    |\n",
            "|    std                  | 1.32        |\n",
            "|    value_loss           | 1.95        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 8000: mean reward -129.63\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 84.9     |\n",
            "|    ep_rew_mean     | -178     |\n",
            "| time/              |          |\n",
            "|    fps             | 996      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1.29 +/- 87.28\n",
            "Episode length: 85.60 +/- 27.05\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 85.6        |\n",
            "|    mean_reward          | -1.29       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013368627 |\n",
            "|    clip_fraction        | 0.0898      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.47       |\n",
            "|    explained_variance   | 0.245       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.17        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00103    |\n",
            "|    std                  | 1.39        |\n",
            "|    value_loss           | 1.63        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 10000: mean reward -1.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 88.9     |\n",
            "|    ep_rew_mean     | -127     |\n",
            "| time/              |          |\n",
            "|    fps             | 969      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 42       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-102.45 +/- 48.28\n",
            "Episode length: 86.00 +/- 31.87\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 86          |\n",
            "|    mean_reward          | -102        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014105537 |\n",
            "|    clip_fraction        | 0.0961      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.59       |\n",
            "|    explained_variance   | 0.267       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0885      |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | 0.00018     |\n",
            "|    std                  | 1.49        |\n",
            "|    value_loss           | 1.13        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 12000: mean reward -102.45\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96.8     |\n",
            "|    ep_rew_mean     | -89.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 945      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 51       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-50.42 +/- 86.27\n",
            "Episode length: 94.00 +/- 29.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 94          |\n",
            "|    mean_reward          | -50.4       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016128149 |\n",
            "|    clip_fraction        | 0.082       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.74       |\n",
            "|    explained_variance   | 0.23        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.322       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | 0.00294     |\n",
            "|    std                  | 1.61        |\n",
            "|    value_loss           | 1.02        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward -50.42\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 110      |\n",
            "|    ep_rew_mean     | -87      |\n",
            "| time/              |          |\n",
            "|    fps             | 951      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 60       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-44.19 +/- 84.66\n",
            "Episode length: 123.60 +/- 116.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 124         |\n",
            "|    mean_reward          | -44.2       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015507045 |\n",
            "|    clip_fraction        | 0.0873      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.9        |\n",
            "|    explained_variance   | 0.163       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.247       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | 0.00173     |\n",
            "|    std                  | 1.75        |\n",
            "|    value_loss           | 0.926       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 16000: mean reward -44.19\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 84.3     |\n",
            "|    ep_rew_mean     | -109     |\n",
            "| time/              |          |\n",
            "|    fps             | 937      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 69       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-34.48 +/- 85.50\n",
            "Episode length: 117.80 +/- 61.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 118         |\n",
            "|    mean_reward          | -34.5       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 72000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016720489 |\n",
            "|    clip_fraction        | 0.0955      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.06       |\n",
            "|    explained_variance   | 0.225       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.278       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.000509   |\n",
            "|    std                  | 1.9         |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 18000: mean reward -34.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96       |\n",
            "|    ep_rew_mean     | -69.2    |\n",
            "| time/              |          |\n",
            "|    fps             | 925      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 79       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-74.67 +/- 93.95\n",
            "Episode length: 67.40 +/- 14.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 67.4        |\n",
            "|    mean_reward          | -74.7       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017923748 |\n",
            "|    clip_fraction        | 0.0764      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.24       |\n",
            "|    explained_variance   | 0.408       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.262       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | 0.00241     |\n",
            "|    std                  | 2.08        |\n",
            "|    value_loss           | 1           |\n",
            "-----------------------------------------\n",
            "Evaluation at step 20000: mean reward -74.67\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 108      |\n",
            "|    ep_rew_mean     | -36.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 931      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 87       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-66.04 +/- 91.05\n",
            "Episode length: 76.80 +/- 23.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 76.8        |\n",
            "|    mean_reward          | -66         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 88000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015463612 |\n",
            "|    clip_fraction        | 0.0786      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.41       |\n",
            "|    explained_variance   | 0.409       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.212      |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | 0.00295     |\n",
            "|    std                  | 2.26        |\n",
            "|    value_loss           | 0.739       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 22000: mean reward -66.04\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 98.4     |\n",
            "|    ep_rew_mean     | -63.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 913      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-6.47 +/- 97.99\n",
            "Episode length: 85.20 +/- 25.07\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 85.2        |\n",
            "|    mean_reward          | -6.47       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019233156 |\n",
            "|    clip_fraction        | 0.0865      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.58       |\n",
            "|    explained_variance   | 0.345       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.363       |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | 0.00235     |\n",
            "|    std                  | 2.48        |\n",
            "|    value_loss           | 1.17        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 24000: mean reward -6.47\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 105      |\n",
            "|    ep_rew_mean     | -51.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 870      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 112      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=18.72 +/- 70.72\n",
            "Episode length: 85.00 +/- 24.75\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 85           |\n",
            "|    mean_reward          | 18.7         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0132582765 |\n",
            "|    clip_fraction        | 0.0739       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.76        |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.00732     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | 0.00335      |\n",
            "|    std                  | 2.69         |\n",
            "|    value_loss           | 1.21         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 26000: mean reward 18.72\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 110      |\n",
            "|    ep_rew_mean     | -50.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 858      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 124      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create and train\n",
        "ent_coef=0.1\n",
        "eval_freq=2000\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef)\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mL_LYMHeejpF",
        "outputId": "ba829edc-96c3-4317-e961-6e757144f621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=-71.18 +/- 54.40\n",
            "Episode length: 132.20 +/- 93.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 132      |\n",
            "|    mean_reward     | -71.2    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -71.18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 89.2     |\n",
            "|    ep_rew_mean     | -60.2    |\n",
            "| time/              |          |\n",
            "|    fps             | 2849     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=49.02 +/- 56.47\n",
            "Episode length: 280.20 +/- 123.59\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 280          |\n",
            "|    mean_reward          | 49           |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 16000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057744836 |\n",
            "|    clip_fraction        | 0.0361       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5           |\n",
            "|    explained_variance   | 0.325        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.69         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00229     |\n",
            "|    std                  | 2.99         |\n",
            "|    value_loss           | 2.31         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward 49.02\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 99.4     |\n",
            "|    ep_rew_mean     | -57.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 1239     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=9.65 +/- 51.50\n",
            "Episode length: 195.00 +/- 131.54\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 195         |\n",
            "|    mean_reward          | 9.65        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005052996 |\n",
            "|    clip_fraction        | 0.0443      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.06       |\n",
            "|    explained_variance   | 0.392       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.824       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00238    |\n",
            "|    std                  | 3.07        |\n",
            "|    value_loss           | 2.05        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 6000: mean reward 9.65\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 108      |\n",
            "|    ep_rew_mean     | -44.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 995      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-41.04 +/- 34.42\n",
            "Episode length: 122.60 +/- 88.06\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 123        |\n",
            "|    mean_reward          | -41        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 32000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00429348 |\n",
            "|    clip_fraction        | 0.0348     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.11      |\n",
            "|    explained_variance   | 0.375      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.654      |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.00191   |\n",
            "|    std                  | 3.16       |\n",
            "|    value_loss           | 2.01       |\n",
            "----------------------------------------\n",
            "Evaluation at step 8000: mean reward -41.04\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 112      |\n",
            "|    ep_rew_mean     | -47.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 965      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-41.10 +/- 41.77\n",
            "Episode length: 110.40 +/- 66.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 110         |\n",
            "|    mean_reward          | -41.1       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005738468 |\n",
            "|    clip_fraction        | 0.0425      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.19       |\n",
            "|    explained_variance   | 0.355       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.353       |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.000935   |\n",
            "|    std                  | 3.29        |\n",
            "|    value_loss           | 1.75        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward -41.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 108      |\n",
            "|    ep_rew_mean     | -25.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 948      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=78.04 +/- 47.48\n",
            "Episode length: 126.80 +/- 46.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 127         |\n",
            "|    mean_reward          | 78          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006011667 |\n",
            "|    clip_fraction        | 0.0427      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.26       |\n",
            "|    explained_variance   | 0.435       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.888       |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.000293   |\n",
            "|    std                  | 3.42        |\n",
            "|    value_loss           | 1.83        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 12000: mean reward 78.04\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 105      |\n",
            "|    ep_rew_mean     | 0.897    |\n",
            "| time/              |          |\n",
            "|    fps             | 924      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=68.53 +/- 73.17\n",
            "Episode length: 254.00 +/- 211.85\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 254          |\n",
            "|    mean_reward          | 68.5         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 56000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068119545 |\n",
            "|    clip_fraction        | 0.0453       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.34        |\n",
            "|    explained_variance   | 0.515        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.573        |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    std                  | 3.57         |\n",
            "|    value_loss           | 1.79         |\n",
            "------------------------------------------\n",
            "Evaluation at step 14000: mean reward 68.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -7.14    |\n",
            "| time/              |          |\n",
            "|    fps             | 891      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 64       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=19.62 +/- 72.17\n",
            "Episode length: 143.00 +/- 69.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 143         |\n",
            "|    mean_reward          | 19.6        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006785945 |\n",
            "|    clip_fraction        | 0.0432      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.43       |\n",
            "|    explained_variance   | 0.486       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.47        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0012     |\n",
            "|    std                  | 3.74        |\n",
            "|    value_loss           | 1.96        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 16000: mean reward 19.62\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -3.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 901      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=9.54 +/- 56.16\n",
            "Episode length: 212.20 +/- 68.08\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 212          |\n",
            "|    mean_reward          | 9.54         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 72000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057632835 |\n",
            "|    clip_fraction        | 0.0429       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.52        |\n",
            "|    explained_variance   | 0.503        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.555        |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.000551    |\n",
            "|    std                  | 3.88         |\n",
            "|    value_loss           | 1.71         |\n",
            "------------------------------------------\n",
            "Evaluation at step 18000: mean reward 9.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -3.49    |\n",
            "| time/              |          |\n",
            "|    fps             | 889      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 82       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=19.90 +/- 100.30\n",
            "Episode length: 88.60 +/- 37.40\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 88.6         |\n",
            "|    mean_reward          | 19.9         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074172663 |\n",
            "|    clip_fraction        | 0.0506       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.61        |\n",
            "|    explained_variance   | 0.421        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.166        |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00123     |\n",
            "|    std                  | 4.07         |\n",
            "|    value_loss           | 1.82         |\n",
            "------------------------------------------\n",
            "Evaluation at step 20000: mean reward 19.90\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | 1.59     |\n",
            "| time/              |          |\n",
            "|    fps             | 861      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 95       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=59.80 +/- 82.59\n",
            "Episode length: 83.60 +/- 15.55\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 83.6         |\n",
            "|    mean_reward          | 59.8         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 88000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057702595 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.7         |\n",
            "|    explained_variance   | 0.45         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.29         |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | 7.55e-05     |\n",
            "|    std                  | 4.26         |\n",
            "|    value_loss           | 2.1          |\n",
            "------------------------------------------\n",
            "Evaluation at step 22000: mean reward 59.80\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -10.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 862      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 104      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=39.71 +/- 95.11\n",
            "Episode length: 168.20 +/- 142.91\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 168          |\n",
            "|    mean_reward          | 39.7         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 96000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054500066 |\n",
            "|    clip_fraction        | 0.0359       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.76        |\n",
            "|    explained_variance   | 0.483        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.315        |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    std                  | 4.38         |\n",
            "|    value_loss           | 2.32         |\n",
            "------------------------------------------\n",
            "Evaluation at step 24000: mean reward 39.71\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 125      |\n",
            "|    ep_rew_mean     | 21.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 851      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 115      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=11.23 +/- 45.41\n",
            "Episode length: 140.20 +/- 39.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 140         |\n",
            "|    mean_reward          | 11.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 104000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007170465 |\n",
            "|    clip_fraction        | 0.0483      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.83       |\n",
            "|    explained_variance   | 0.526       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.548       |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.000617   |\n",
            "|    std                  | 4.53        |\n",
            "|    value_loss           | 1.51        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 26000: mean reward 11.23\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -2.18    |\n",
            "| time/              |          |\n",
            "|    fps             | 848      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 125      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "71c7d9ee-a0bc-4a9a-81d3-3515602701b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: -78.79866129999999 +/- 133.82650038538142\n"
          ]
        }
      ],
      "source": [
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "e2c096df-2e31-4cdf-f66d-ca3f9ad36622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n",
            "reset\n",
            "reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    # print(obs, action)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbn1ZY4GGMZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-hPY2KxRxsE",
        "outputId": "409b1ca5-e574-479e-c62f-84f66515be4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Actions: [[ 0.05661448 -0.16774732]]\n",
            "ONNX Values: [[-0.05518252]]\n",
            "ONNX Log Prob: [-3.9599395]\n",
            "PyTorch Actions: [[ 0.05661447 -0.1677474 ]]\n",
            "PyTorch Values: [[-0.0551825]]\n",
            "PyTorch Log Prob: [-3.9599395]\n",
            "Actions match: True\n",
            "Values match: True\n",
            "Log prob match: True\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        # Run the policy in deterministic mode\n",
        "        actions, values, log_prob = self.policy(observation, deterministic=True)\n",
        "        return actions, values, log_prob\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\"\n",
        "model = PPO.load(model_path)\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "\n",
        "# Define dummy input based on the observation space shape\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1.onnx\"\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Create an ONNX runtime session\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "ort_inputs = {\"input\": observation}\n",
        "ort_outputs = ort_sess.run(None, ort_inputs)\n",
        "\n",
        "# Output from ONNX\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_outputs\n",
        "\n",
        "# Print ONNX outputs\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "# Check that the predictions are the same in PyTorch\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "\n",
        "# Print PyTorch outputs\n",
        "print(\"PyTorch Actions:\", pytorch_outputs[0].numpy())\n",
        "print(\"PyTorch Values:\", pytorch_outputs[1].numpy())\n",
        "print(\"PyTorch Log Prob:\", pytorch_outputs[2].numpy())\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = [output.numpy() for output in pytorch_outputs]\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "\n",
        "    actions_match = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match: {actions_match}\")\n",
        "    print(f\"Values match: {values_match}\")\n",
        "    print(f\"Log prob match: {log_prob_match}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(pytorch_outputs, ort_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx2tf import convert\n",
        "\n",
        "# Define paths\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Convert ONNX to TensorFlow SavedModel\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPSegjRq1cOI",
        "outputId": "fa8d3822-4eed-4746-bd7e-f9a0b9a6c98c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnx2tf/onnx2tf.py\", line 631, in convert\n",
            "    result = subprocess.check_output(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
            "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n",
            "\n",
            "\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Cast\u001b[35m onnx_op_name\u001b[0m: wa/policy/Cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.dtype\u001b[0m: \u001b[34mname\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Exp\u001b[35m onnx_op_name\u001b[0m: wa/policy/Exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: policy.log_std \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/policy/features_extractor/flatten/Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 8] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_22/Reshape:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Constant_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_66/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_67/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_66/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_44/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_67/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_45/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_68/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_69/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_68/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_46/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_69/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_47/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/value_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.value_net.weight \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.value_net.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: values \u001b[36mshape\u001b[0m: [1, 1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_70/AddV2:0 \u001b[34mshape\u001b[0m: (1, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_33/wa/policy/Shape:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ConstantOfShape\u001b[35m onnx_op_name\u001b[0m: wa/policy/ConstantOfShape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: constant\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.value\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_11/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_11/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_32/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_32/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_34/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m27 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_35/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m28 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_34/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m29 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_35/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m30 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_22/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m31 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Log\u001b[35m onnx_op_name\u001b[0m: wa/policy/Log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_11/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m32 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_33/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m33 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_5_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_23/Reshape:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m34 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_2_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_33/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_23/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m35 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_3_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_22/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_218/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m36 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Neg\u001b[35m onnx_op_name\u001b[0m: wa/policy/Neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_23/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_11/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m37 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/policy/Div\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_11/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_218/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_11/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m38 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_11/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_11/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_34/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m39 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_4_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_34/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_35/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m40 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceSum\u001b[35m onnx_op_name\u001b[0m: wa/policy/ReduceSum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: log_prob \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_sum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_35/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_sum_23/Sum:0 \u001b[34mshape\u001b[0m: (None,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.engine.functional.Functional at 0x7d035f42fdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnaMw0pAURsM",
        "outputId": "fb2b8e87-dc43-497e-f332-98dd0f8f6b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow.js model saved at: /content/drive/MyDrive/wimblepong/24-monday/ppo_custom_pong_1_tfjs\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tfjs\"\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow.js\n",
        "subprocess.run([\n",
        "    'tensorflowjs_converter',\n",
        "    '--input_format', 'tf_saved_model',\n",
        "    '--output_format', 'tfjs_graph_model',\n",
        "    \"--signature_name\", \"serving_default\",\n",
        "    tf_model_path,\n",
        "    tfjs_model_path\n",
        "])\n",
        "\n",
        "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1.onnx\"\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        return self.policy(observation, deterministic=True)\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Convert ONNX to TensorFlow\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True\n",
        ")\n",
        "\n",
        "# Load the TensorFlow model\n",
        "tf_model = tf.saved_model.load(tf_model_path)\n",
        "\n",
        "# Function to run inference on TensorFlow model\n",
        "def run_tf_model(tf_model, input_data):\n",
        "    concrete_func = tf_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "    input_tensor = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
        "    output_dict = concrete_func(input_tensor)\n",
        "    actions = output_dict['actions'].numpy()\n",
        "    values = output_dict['values'].numpy()\n",
        "    log_prob = output_dict['log_prob'].numpy()\n",
        "    return actions, values, log_prob\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Run inference with TensorFlow model\n",
        "tf_actions, tf_values, tf_log_prob = run_tf_model(tf_model, observation)\n",
        "\n",
        "# Run inference with ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_sess.run(None, {\"input\": observation})\n",
        "\n",
        "# Run inference with PyTorch model\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "    pytorch_actions = pytorch_outputs[0].numpy()\n",
        "    pytorch_values = pytorch_outputs[1].numpy()\n",
        "    pytorch_log_prob = pytorch_outputs[2].numpy()\n",
        "\n",
        "# Print Outputs\n",
        "print(\"PyTorch Actions:\", pytorch_actions)\n",
        "print(\"PyTorch Values:\", pytorch_values)\n",
        "print(\"PyTorch Log Prob:\", pytorch_log_prob)\n",
        "\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "print(\"TensorFlow Actions:\", tf_actions)\n",
        "print(\"TensorFlow Values:\", tf_values)\n",
        "print(\"TensorFlow Log Prob:\", tf_log_prob)\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs, tf_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = pytorch_outputs\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "    tf_actions, tf_values, tf_log_prob = tf_outputs\n",
        "\n",
        "    actions_match_pytorch_onnx = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match_pytorch_onnx = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match_pytorch_onnx = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    actions_match_pytorch_tf = np.allclose(pytorch_actions, tf_actions, atol=1e-5)\n",
        "    values_match_pytorch_tf = np.allclose(pytorch_values, tf_values, atol=1e-5)\n",
        "    log_prob_match_pytorch_tf = np.allclose(pytorch_log_prob, tf_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match (PyTorch vs ONNX): {actions_match_pytorch_onnx}\")\n",
        "    print(f\"Values match (PyTorch vs ONNX): {values_match_pytorch_onnx}\")\n",
        "    print(f\"Log prob match (PyTorch vs ONNX): {log_prob_match_pytorch_onnx}\")\n",
        "\n",
        "    print(f\"Actions match (PyTorch vs TensorFlow): {actions_match_pytorch_tf}\")\n",
        "    print(f\"Values match (PyTorch vs TensorFlow): {values_match_pytorch_tf}\")\n",
        "    print(f\"Log prob match (PyTorch vs TensorFlow): {log_prob_match_pytorch_tf}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(\n",
        "    (pytorch_actions, pytorch_values, pytorch_log_prob),\n",
        "    (onnx_actions, onnx_values, onnx_log_prob),\n",
        "    (tf_actions, tf_values, tf_log_prob)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UazEUD5K0s1",
        "outputId": "d436ce8f-1f2b-439d-b70f-2df9ab7909ec"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnx2tf/onnx2tf.py\", line 631, in convert\n",
            "    result = subprocess.check_output(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
            "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n",
            "\n",
            "\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Cast\u001b[35m onnx_op_name\u001b[0m: wa/policy/Cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.dtype\u001b[0m: \u001b[34mname\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Exp\u001b[35m onnx_op_name\u001b[0m: wa/policy/Exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: policy.log_std \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/policy/features_extractor/flatten/Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 8] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_26/Reshape:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Constant_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_78/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_79/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_78/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_52/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_79/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_53/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_80/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_81/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_80/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_54/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_81/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_55/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/value_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.value_net.weight \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.value_net.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: values \u001b[36mshape\u001b[0m: [1, 1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_82/AddV2:0 \u001b[34mshape\u001b[0m: (1, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_39/wa/policy/Shape:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ConstantOfShape\u001b[35m onnx_op_name\u001b[0m: wa/policy/ConstantOfShape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: constant\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.value\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_13/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_13/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_36/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_36/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_40/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m27 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_41/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m28 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_40/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m29 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_41/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m30 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_26/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m31 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Log\u001b[35m onnx_op_name\u001b[0m: wa/policy/Log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_13/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m32 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_39/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m33 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_5_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_27/Reshape:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m34 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_2_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_39/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_27/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m35 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_3_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_26/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_250/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m36 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Neg\u001b[35m onnx_op_name\u001b[0m: wa/policy/Neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_27/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_13/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m37 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/policy/Div\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_13/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_250/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_13/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m38 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_13/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_13/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_40/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m39 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_4_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_40/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_41/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m40 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceSum\u001b[35m onnx_op_name\u001b[0m: wa/policy/ReduceSum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: log_prob \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_sum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_41/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_sum_27/Sum:0 \u001b[34mshape\u001b[0m: (None,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n",
            "PyTorch Actions: [[ 0.05661447 -0.1677474 ]]\n",
            "PyTorch Values: [[-0.0551825]]\n",
            "PyTorch Log Prob: [-3.9599395]\n",
            "ONNX Actions: [[ 0.05661448 -0.16774732]]\n",
            "ONNX Values: [[-0.05518252]]\n",
            "ONNX Log Prob: [-3.9599395]\n",
            "TensorFlow Actions: [[ 0.05661446 -0.16774729]]\n",
            "TensorFlow Values: [[-0.05518243]]\n",
            "TensorFlow Log Prob: [-3.9599395]\n",
            "Actions match (PyTorch vs ONNX): True\n",
            "Values match (PyTorch vs ONNX): True\n",
            "Log prob match (PyTorch vs ONNX): True\n",
            "Actions match (PyTorch vs TensorFlow): True\n",
            "Values match (PyTorch vs TensorFlow): True\n",
            "Log prob match (PyTorch vs TensorFlow): True\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}