{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "56452466-6c1b-459c-e05e-0f935db3668f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=ac9dcecbbab13fdd09dde7c0b1d1f37cc5934746aa9c87066ba7aef65c1a4ea2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.1)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.0 ydf-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup Google Colab Environment\n",
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install imageio pillow\n",
        "%pip install tensorflowjs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "0d8f0ff9-19e3-4cfa-ebdb-88a35e439b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"6-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "b419cb6e-2409-4046-a9c4-f82f8365d7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/skimage/util/dtype.py:27: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'blue', 'Player2': 'red'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    # \"conceed_point\": lambda ball, paddle, rally_length: -0.1,\n",
        "    \"serve\": lambda ball_speed: ball_speed / 10,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -60]), high=np.array([1, 60]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -np.inf, -np.inf, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([COURT_WIDTH, COURT_HEIGHT, np.inf, np.inf, COURT_WIDTH, COURT_HEIGHT, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "          #  {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "\n",
        "        self.starting_state_index = 0\n",
        "        self.serve_delay = 50\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 15\n",
        "\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "\n",
        "        self.is_done = False\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"Environment reset\")\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        computer = starting_state['opponent']\n",
        "        player = starting_state['player']\n",
        "\n",
        "        self.game_state = {\n",
        "            'server': server,\n",
        "            'positions_reversed': positions_reversed,\n",
        "            'opponent': computer,\n",
        "            'player': player,\n",
        "            Player.Player1: {'x': 0, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "            Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "            'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "            'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 30 * np.random.rand()\n",
        "        self.serve_delay = 100 * np.random.rand()\n",
        "        self.direction = self.direction if np.random.rand() > 0.5 else -self.direction\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.is_done = False\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "            self.game_state[Player.Player2]['x'] = 0\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = 0\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius']\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction}\n",
        "        computer_player_actions = self.get_computer_player_actions(self.game_state['opponent'])\n",
        "        actions = {self.game_state['opponent']: computer_player_actions, self.game_state['player']: model_player_actions}\n",
        "        reward = self.update_game_state(actions, 2.5)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        terminated = self.check_done()\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def update_game_state(self, actions, delta_time):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        model_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            self.is_done = True\n",
        "            return 0.01\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            if serving_from_left:\n",
        "                ball['x'] = game_state[server]['width'] + ball['radius']\n",
        "            else:\n",
        "                ball['x'] = COURT_WIDTH - game_state[server]['width'] - ball['radius']\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if model_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward\n",
        "\n",
        "    def get_computer_player_actions(self, player):\n",
        "        state = self.game_state\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4 ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "\n",
        "    def bounded_value(self, value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x']),\n",
        "            float(state['ball']['y']),\n",
        "            float(state['ball']['dx']),\n",
        "            float(state['ball']['dy']),\n",
        "            float(paddle['x']),\n",
        "            float(paddle['y']),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(is_server),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def check_done(self):\n",
        "        if self.game_state['stats']['rally_length'] > 100:\n",
        "            return True\n",
        "        if self.step_count > 1000:\n",
        "            return True\n",
        "        return self.is_done\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "outputId": "33460732-d787-4f91-8939-f22c24a8a1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment reset\n",
            "Environment reset\n",
            "Initial observation: [[0.00999315 0.00999997 0.00995036 0.00995036 0.         0.00999996\n",
            "  0.00707071 0.00707071]]\n",
            "Action taken: [ 0.14008403 51.690395  ]\n",
            "Action taken: [ 0.14008403 51.690395  ]\n",
            "Observation: [[ 0.00706622  0.00707105  0.00703597 -0.999949    0.         -0.99929583\n",
            "   0.00499969  0.00499969]]\n",
            "Reward: [0.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [ 0.7669443 -4.942801 ]\n",
            "Action taken: [ 0.7669443 -4.942801 ]\n",
            "Observation: [[ 0.00576955 -1.3709505   0.00574484 -1.1932889   0.         -0.59796655\n",
            "  -1.4140368   0.00408214]]\n",
            "Reward: [1.9998039]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [  0.91694933 -44.478245  ]\n",
            "Action taken: [  0.91694933 -44.478245  ]\n",
            "Observation: [[ 1.7319448  -1.4962285   0.00497518 -0.8874667   0.          0.9496263\n",
            "  -0.99992496  0.00353516]]\n",
            "Reward: [0.02100232]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [  0.675326 -34.00926 ]\n",
            "Action taken: [  0.675326 -34.00926 ]\n",
            "Observation: [[ 1.7499822  -1.5388896   0.00444993 -0.73779315  0.          1.4516258\n",
            "  -0.8164489   0.00316187]]\n",
            "Reward: [0.02286555]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [  0.74023485 -50.629353  ]\n",
            "Action taken: [  0.74023485 -50.629353  ]\n",
            "Observation: [[ 1.7320484  -1.5629351   0.00406221 -0.6448931   0.          1.7469234\n",
            "  -0.7070714   0.00288631]]\n",
            "Reward: [0.02475386]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [  0.8116776 -31.510416 ]\n",
            "Action taken: [  0.8116776 -31.510416 ]\n",
            "Observation: [[ 1.7162344  -1.579819    0.00376087 -0.5800728   0.          1.5917844\n",
            "  -0.63242704  0.00267214]]\n",
            "Reward: [0.0265699]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [ 0.6800271 -8.751628 ]\n",
            "Action taken: [ 0.6800271 -8.751628 ]\n",
            "Observation: [[ 1.704989   -1.5929409   0.00351797 -0.53154564  0.          1.2976042\n",
            "  -0.5773262   0.00249949]]\n",
            "Reward: [0.02829872]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [  0.7622731 -58.189236 ]\n",
            "Action taken: [  0.7622731 -58.189236 ]\n",
            "Observation: [[ 1.6973403  -1.6036838   0.00331678 -0.49346063  0.          1.1228597\n",
            "  -0.5345015   0.00235649]]\n",
            "Reward: [0.02994372]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [ 0.6328975 54.800438 ]\n",
            "Action taken: [ 0.6328975 54.800438 ]\n",
            "Observation: [[ 1.6922317  -1.6127485   0.00314657 -0.4625405   0.          0.22081165\n",
            "  -0.49998122  0.0022355 ]]\n",
            "Reward: [0.03151249]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [  0.71356833 -14.97114   ]\n",
            "Action taken: [  0.71356833 -14.97114   ]\n",
            "Observation: [[ 1.6888847  -1.6205478   0.00300013 -0.43678814  0.          0.44948488\n",
            "  -0.47138748  0.00213142]]\n",
            "Reward: [0.03301294]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [ 0.08481074 -3.7663636 ]\n",
            "Action taken: [ 0.08481074 -3.7663636 ]\n",
            "Observation: [[ 1.6867636  -1.6273533   0.00287241 -0.41490775  0.          0.4888278\n",
            "  -0.4471979   0.00204063]]\n",
            "Reward: [0.03445232]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [  0.7519573 -53.334553 ]\n",
            "Action taken: [  0.7519573 -53.334553 ]\n",
            "Observation: [[ 1.6855024  -1.6333554   0.00275972 -0.3960172   0.          1.0516053\n",
            "  -0.4263869   0.00196052]]\n",
            "Reward: [0.03583692]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [  0.8371961 -13.807061 ]\n",
            "Action taken: [  0.8371961 -13.807061 ]\n",
            "Observation: [[ 1.6848496  -1.6386951   0.00265933 -0.3794922   0.          0.975555\n",
            "  -0.40823466  0.00188916]]\n",
            "Reward: [0.03717221]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [ 0.8753082 -9.90486  ]\n",
            "Action taken: [ 0.8753082 -9.90486  ]\n",
            "Observation: [[ 1.6846302  -1.6434796   0.00256916 -0.364877    0.          0.9139288\n",
            "  -0.39221942  0.00182505]]\n",
            "Reward: [0.03846288]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [0.8252467 9.44821  ]\n",
            "Action taken: [0.8252467 9.44821  ]\n",
            "Observation: [[ 1.6847203  -1.6477935   0.00248757 -0.35182995  0.          0.70637673\n",
            "  -0.3779523   0.00176706]]\n",
            "Reward: [0.03971298]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [  0.35820228 -24.693464  ]\n",
            "Action taken: [  0.35820228 -24.693464  ]\n",
            "Observation: [[ 1.685032   -1.651704    0.0024133  -0.34008935  0.          0.83580834\n",
            "  -0.3651368   0.00171425]]\n",
            "Reward: [0.04092598]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [ 0.98675746 58.96386   ]\n",
            "Action taken: [ 0.98675746 58.96386   ]\n",
            "Observation: [[ 1.6855016e+00 -1.6552659e+00  2.3453042e-03 -3.2945073e-01\n",
            "   0.0000000e+00 -2.5468978e-01 -3.5354233e-01  1.6659149e-03]]\n",
            "Reward: [0.04210494]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [ 0.9905151 25.710934 ]\n",
            "Action taken: [ 0.9905151 25.710934 ]\n",
            "Observation: [[ 1.6860826e+00 -1.6585245e+00  2.2827503e-03 -3.1975186e-01\n",
            "   0.0000000e+00 -7.1623731e-01 -3.4298655e-01  1.6214420e-03]]\n",
            "Reward: [0.04325252]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [  0.20594497 -28.556849  ]\n",
            "Action taken: [  0.20594497 -28.556849  ]\n",
            "Observation: [[ 1.6867411e+00 -1.6615171e+00  2.2249487e-03 -3.1086203e-01\n",
            "   0.0000000e+00 -1.5875442e-01 -3.3332312e-01  1.5803468e-03]]\n",
            "Reward: [0.04437107]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [ 0.5299981 22.67005  ]\n",
            "Action taken: [ 0.5299981 22.67005  ]\n",
            "Observation: [[ 1.6874517e+00 -1.6642753e+00  2.1713264e-03 -3.0267471e-01\n",
            "   0.0000000e+00 -5.8920711e-01 -3.2443300e-01  1.5422222e-03]]\n",
            "Reward: [0.04546265]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [  0.92059505 -18.731735  ]\n",
            "Action taken: [  0.92059505 -18.731735  ]\n",
            "Observation: [[ 1.6881957e+00 -1.6668255e+00  2.1214033e-03 -2.9510200e-01\n",
            "   0.0000000e+00 -2.0619331e-01 -3.1621826e-01  1.5067265e-03]]\n",
            "Reward: [0.04652911]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [  0.60557944 -47.743263  ]\n",
            "Action taken: [  0.60557944 -47.743263  ]\n",
            "Observation: [[ 1.6889592e+00 -1.5535223e+00  2.0747723e-03  2.9189365e+00\n",
            "   0.0000000e+00  7.5182056e-01 -3.0859751e-01  1.4735709e-03]]\n",
            "Reward: [0.04757209]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [  0.50815123 -12.935049  ]\n",
            "Action taken: [  0.50815123 -12.935049  ]\n",
            "Observation: [[ 1.6897317e+00 -1.3175129e+00  2.0310869e-03  2.4547784e+00\n",
            "   0.0000000e+00  9.6671122e-01 -3.0150241e-01  1.4425089e-03]]\n",
            "Reward: [0.04859307]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [ 0.1397466 34.035435 ]\n",
            "Action taken: [ 0.1397466 34.035435 ]\n",
            "Observation: [[ 1.6905054e+00 -1.1129811e+00  1.9900496e-03  2.1590149e+00\n",
            "   0.0000000e+00  2.5211355e-01 -2.9487523e-01  1.4133290e-03]]\n",
            "Reward: [0.04959337]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [ 0.30435336 11.955004  ]\n",
            "Action taken: [ 0.30435336 11.955004  ]\n",
            "Observation: [[ 1.6912746e+00 -9.2772579e-01  1.9514033e-03  1.9495313e+00\n",
            "   0.0000000e+00 -1.7095020e-03 -2.8866670e-01  1.3858486e-03]]\n",
            "Reward: [0.0505742]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [0.88998234 7.4729266 ]\n",
            "Action taken: [0.88998234 7.4729266 ]\n",
            "Observation: [[ 1.6920350e+00 -7.5432545e-01  1.9149244e-03  1.7911705e+00\n",
            "   0.0000000e+00 -1.6039826e-01 -2.8283450e-01  1.3599087e-03]]\n",
            "Reward: [0.05153665]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [ 0.31993443 16.780048  ]\n",
            "Action taken: [ 0.31993443 16.780048  ]\n",
            "Observation: [[ 1.6927832e+00 -5.8800608e-01  1.8804175e-03  1.6660342e+00\n",
            "   0.0000000e+00 -5.1834846e-01 -2.7734208e-01  1.3353706e-03]]\n",
            "Reward: [0.05248171]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [ 0.23499402 -7.2024155 ]\n",
            "Action taken: [ 0.23499402 -7.2024155 ]\n",
            "Observation: [[ 1.6935174e+00 -4.2561543e-01  1.8477112e-03  1.5639241e+00\n",
            "   0.0000000e+00 -3.5039431e-01 -2.7215770e-01  1.3121122e-03]]\n",
            "Reward: [0.0534103]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [ 3.758253e-02 -5.574232e+01]\n",
            "Action taken: [ 3.758253e-02 -5.574232e+01]\n",
            "Observation: [[ 1.6942358e+00 -2.6509693e-01  1.8166540e-03  1.4785470e+00\n",
            "   0.0000000e+00  8.8701087e-01 -2.6725358e-01  1.2900262e-03]]\n",
            "Reward: [0.05432323]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [  0.3510004 -42.814594 ]\n",
            "Action taken: [  0.3510004 -42.814594 ]\n",
            "Observation: [[ 1.69493723e+00 -1.05209775e-01  1.78711209e-03  1.40578103e+00\n",
            "   0.00000000e+00  1.01800537e+00 -2.62605369e-01  1.26901711e-03]]\n",
            "Reward: [0.05522129]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [ 0.16984628 20.675707  ]\n",
            "Action taken: [ 0.16984628 20.675707  ]\n",
            "Observation: [[ 1.6956213e+00  5.4626100e-02  1.7589660e-03  1.3427992e+00\n",
            "   0.0000000e+00  5.3668886e-01 -2.5819156e-01  1.2490002e-03]]\n",
            "Reward: [0.05610517]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [  0.11487572 -12.713313  ]\n",
            "Action taken: [  0.11487572 -12.713313  ]\n",
            "Observation: [[ 1.6962873e+00  2.1442117e-01  1.7321091e-03  1.2875885e+00\n",
            "   0.0000000e+00  8.0802578e-01 -2.5399306e-01  1.2298997e-03]]\n",
            "Reward: [0.05697553]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [  0.16701926 -52.821274  ]\n",
            "Action taken: [  0.16701926 -52.821274  ]\n",
            "Observation: [[ 1.6969353e+00  3.7368506e-01  1.7064459e-03  1.2386720e+00\n",
            "   0.0000000e+00  9.6352440e-01 -2.4999295e-01  1.2116478e-03]]\n",
            "Reward: [0.05783296]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [  0.7513193 -36.64002  ]\n",
            "Action taken: [  0.7513193 -36.64002  ]\n",
            "Observation: [[ 1.6975654e+00  5.3149545e-01  1.6818907e-03  1.1949381e+00\n",
            "   0.0000000e+00  9.3731034e-01 -2.4617606e-01  1.1941832e-03]]\n",
            "Reward: [0.05867804]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [  0.78906524 -25.451601  ]\n",
            "Action taken: [  0.78906524 -25.451601  ]\n",
            "Observation: [[ 1.6981777e+00  6.8658471e-01  1.6583657e-03  1.1555316e+00\n",
            "   0.0000000e+00  9.1312563e-01 -2.4252883e-01  1.1774512e-03]]\n",
            "Reward: [0.05951127]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [ 0.49634475 38.841396  ]\n",
            "Action taken: [ 0.49634475 38.841396  ]\n",
            "Observation: [[ 1.6987725e+00  8.3744925e-01  1.6358010e-03  1.1197833e+00\n",
            "   0.0000000e+00  1.1505973e-02 -2.3903903e-01  1.1614017e-03]]\n",
            "Reward: [0.06033316]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [  0.97740984 -12.889339  ]\n",
            "Action taken: [  0.97740984 -12.889339  ]\n",
            "Observation: [[ 1.6993501e+00  9.8248321e-01  1.6141331e-03  1.0871598e+00\n",
            "   0.0000000e+00  3.1020415e-01 -2.3569569e-01  1.1459898e-03]]\n",
            "Reward: [0.06114413]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [ 0.9492313 49.81825  ]\n",
            "Action taken: [ 0.9492313 49.81825  ]\n",
            "Observation: [[ 1.6999110e+00  1.1201197e+00  1.5933039e-03  1.0572309e+00\n",
            "   0.0000000e+00 -8.5645747e-01 -2.3248881e-01  1.1311739e-03]]\n",
            "Reward: [0.06194463]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [ 3.4737363e-02 -4.2338779e+01]\n",
            "Action taken: [ 3.4737363e-02 -4.2338779e+01]\n",
            "Observation: [[ 1.7004555e+00  1.2489668e+00  1.5732608e-03  1.0296452e+00\n",
            "   0.0000000e+00  1.5306135e-01 -2.2940937e-01  1.1169169e-03]]\n",
            "Reward: [0.06273504]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [1.9221675e-02 4.1493137e+01]\n",
            "Action taken: [1.9221675e-02 4.1493137e+01]\n",
            "Observation: [[ 1.7009841e+00  1.3679183e+00  1.5539555e-03  1.0041119e+00\n",
            "   0.0000000e+00 -8.3282506e-01 -2.2644915e-01  1.1031844e-03]]\n",
            "Reward: [0.06351575]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [  0.6622016 -52.374306 ]\n",
            "Action taken: [  0.6622016 -52.374306 ]\n",
            "Observation: [[ 1.7014974e+00  1.4762237e+00  1.5353438e-03  9.8038858e-01\n",
            "   0.0000000e+00  4.3233591e-01 -2.2360063e-01  1.0899450e-03]]\n",
            "Reward: [0.0642871]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [  0.97206324 -16.335321  ]\n",
            "Action taken: [  0.97206324 -16.335321  ]\n",
            "Observation: [[ 1.7019960e+00  1.5735164e+00  1.5173851e-03  9.5827097e-01\n",
            "   0.0000000e+00  8.1647390e-01 -2.2085696e-01  1.0771698e-03]]\n",
            "Reward: [0.06504943]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [  0.16630718 -13.909167  ]\n",
            "Action taken: [  0.16630718 -13.909167  ]\n",
            "Observation: [[ 1.7024800e+00  1.6597952e+00  1.5000423e-03  9.3758577e-01\n",
            "   0.0000000e+00  9.4824916e-01 -2.1821189e-01  1.0648323e-03]]\n",
            "Reward: [0.06580304]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [ 0.29339206 22.971933  ]\n",
            "Action taken: [ 0.29339206 22.971933  ]\n",
            "Observation: [[ 1.7029501e+00  1.7353765e+00  1.4832808e-03  9.1818476e-01\n",
            "   0.0000000e+00  3.7759054e-01 -2.1565963e-01  1.0529081e-03]]\n",
            "Reward: [0.06654821]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [ 0.22244188 11.323298  ]\n",
            "Action taken: [ 0.22244188 11.323298  ]\n",
            "Observation: [[ 1.7034068e+00  1.8008256e+00  1.4670689e-03  8.9994031e-01\n",
            "   0.0000000e+00  9.4957590e-02 -2.1319488e-01  1.0413746e-03]]\n",
            "Reward: [0.06728525]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [  0.78096974 -14.451772  ]\n",
            "Action taken: [  0.78096974 -14.451772  ]\n",
            "Observation: [[ 1.7038505e+00  1.8568814e+00  1.4513772e-03  8.8274193e-01\n",
            "   0.0000000e+00  4.5235029e-01 -2.1081275e-01  1.0302109e-03]]\n",
            "Reward: [0.06801441]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [0.84633183 7.3007827 ]\n",
            "Action taken: [0.84633183 7.3007827 ]\n",
            "Observation: [[ 1.7042818e+00  1.9043870e+00  1.4361783e-03  8.6649328e-01\n",
            "   0.0000000e+00  2.6424980e-01 -2.0850872e-01  1.0193976e-03]]\n",
            "Reward: [0.06873593]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [ 0.7069919 50.985058 ]\n",
            "Action taken: [ 0.7069919 50.985058 ]\n",
            "Observation: [[ 1.6942092e+00  1.9442290e+00 -6.9281349e+00  7.0059717e-02\n",
            "   0.0000000e+00 -1.0192538e+00 -2.0627862e-01  1.0089169e-03]]\n",
            "Reward: [0.13886634]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [0.95469   3.0477664]\n",
            "Action taken: [0.95469   3.0477664]\n",
            "Observation: [[ 1.5544248e+00  1.8825405e+00 -4.8989573e+00  6.9352180e-02\n",
            "   0.0000000e+00 -1.0733646e+00 -2.0411859e-01  9.9875184e-04]]\n",
            "Reward: [0.14020097]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [ 0.963488 42.136223]\n",
            "Action taken: [ 0.963488 42.136223]\n",
            "Observation: [[ 1.4273878e+00  1.8286525e+00 -3.9999890e+00  6.8665653e-02\n",
            "   0.0000000e+00 -2.0380800e+00 -2.0202503e-01  9.8888704e-04]]\n",
            "Reward: [0.1414652]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [  0.98969084 -23.326212  ]\n",
            "Action taken: [  0.98969084 -23.326212  ]\n",
            "Observation: [[ 1.3101273e+00  1.7812072e+00 -3.4640951e+00  6.7999125e-02\n",
            "   0.0000000e+00 -1.4189129e+00 -1.9999458e-01  9.7930792e-04]]\n",
            "Reward: [0.14264928]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [ 0.597986 54.11012 ]\n",
            "Action taken: [ 0.597986 54.11012 ]\n",
            "Observation: [[ 1.2004529e+00  1.7391551e+00 -3.0983827e+00  6.7351632e-02\n",
            "   0.0000000e+00 -2.5413888e+00 -1.9802414e-01  9.7000098e-04]]\n",
            "Reward: [0.1437444]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [ 0.25578952 14.274653  ]\n",
            "Action taken: [ 0.25578952 14.274653  ]\n",
            "Observation: [[ 1.0967064e+00  1.7016698e+00 -2.8284245e+00  6.6722289e-02\n",
            "   0.0000000e+00 -2.6493268e+00 -1.9611083e-01  9.6095359e-04]]\n",
            "Reward: [0.14474267]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [  0.20742986 -12.855875  ]\n",
            "Action taken: [  0.20742986 -12.855875  ]\n",
            "Observation: [[ 9.9760586e-01  1.6680914e+00 -2.6186128e+00  6.6110261e-02\n",
            "   0.0000000e+00 -2.2370636e+00 -1.9425192e-01  9.5215382e-04]]\n",
            "Reward: [0.14563733]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [  0.28771216 -56.070522  ]\n",
            "Action taken: [  0.28771216 -56.070522  ]\n",
            "Observation: [[ 9.0214288e-01  1.6378851e+00 -2.4494884e+00  6.5514781e-02\n",
            "   0.0000000e+00 -1.0465270e+00 -1.9244489e-01  9.4359065e-04]]\n",
            "Reward: [0.14642267]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [  0.12221459 -21.597454  ]\n",
            "Action taken: [  0.12221459 -21.597454  ]\n",
            "Observation: [[ 8.0951297e-01  1.6106126e+00 -2.3094003e+00  6.4935103e-02\n",
            "   0.0000000e+00 -5.8909631e-01 -1.9068737e-01  9.3525363e-04]]\n",
            "Reward: [0.14709416]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [ 0.85104454 -7.9795823 ]\n",
            "Action taken: [ 0.85104454 -7.9795823 ]\n",
            "Observation: [[ 7.1906793e-01  1.5859100e+00 -2.1908896e+00  6.4370543e-02\n",
            "   0.0000000e+00 -4.1745916e-01 -1.8897715e-01  9.2713290e-04]]\n",
            "Reward: [0.14764845]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [ 0.9154222 16.99503  ]\n",
            "Action taken: [ 0.9154222 16.99503  ]\n",
            "Observation: [[ 6.3028169e-01  1.5634733e+00 -2.0889316e+00  6.3820459e-02\n",
            "   0.0000000e+00 -7.6592660e-01 -1.8731213e-01  9.1921940e-04]]\n",
            "Reward: [0.14808342]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [  0.86782575 -23.188414  ]\n",
            "Action taken: [  0.86782575 -23.188414  ]\n",
            "Observation: [[ 5.4272640e-01  1.5430459e+00 -1.9999999e+00  6.3284241e-02\n",
            "   0.0000000e+00 -2.7196088e-01 -1.8569034e-01  9.1150432e-04]]\n",
            "Reward: [0.14839813]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [  0.05032205 -11.768784  ]\n",
            "Action taken: [  0.05032205 -11.768784  ]\n",
            "Observation: [[ 4.5605582e-01  1.5244092e+00 -1.9215379e+00  6.2761314e-02\n",
            "   0.0000000e+00 -2.0394415e-02 -1.8410999e-01  9.0397958e-04]]\n",
            "Reward: [0.1485928]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [  0.78741825 -31.281694  ]\n",
            "Action taken: [  0.78741825 -31.281694  ]\n",
            "Observation: [[ 3.6999178e-01  1.5073760e+00 -1.8516403e+00  6.2251139e-02\n",
            "   0.0000000e+00  6.4585757e-01 -1.8256930e-01  8.9663744e-04]]\n",
            "Reward: [0.14866872]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [0.33510795 3.2736907 ]\n",
            "Action taken: [0.33510795 3.2736907 ]\n",
            "Observation: [[ 2.8431553e-01  1.4917847e+00 -1.7888546e+00  6.1753210e-02\n",
            "   0.0000000e+00  5.6894225e-01 -1.8106663e-01  8.8947063e-04]]\n",
            "Reward: [0.14862828]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [  0.8669332 -12.762331 ]\n",
            "Action taken: [  0.8669332 -12.762331 ]\n",
            "Observation: [[ 1.9886152e-01  1.4774953e+00 -1.7320510e+00  6.1267037e-02\n",
            "   0.0000000e+00  8.3530521e-01 -1.7960049e-01  8.8247232e-04]]\n",
            "Reward: [0.14847471]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.33984804 19.934324  ]\n",
            "Action taken: [ 0.33984804 19.934324  ]\n",
            "Observation: [[ 1.1351154e-01  1.4643856e+00 -1.6803365e+00  6.0792170e-02\n",
            "   0.0000000e+00  3.9701357e-01 -1.7816940e-01  8.7563595e-04]]\n",
            "Reward: [0.14821212]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [ 0.91387135 48.06726   ]\n",
            "Action taken: [ 0.91387135 48.06726   ]\n",
            "Observation: [[ 2.8191388e-02  1.4523476e+00 -1.6329936e+00  6.0328178e-02\n",
            "   0.0000000e+00 -6.5091014e-01 -1.7677197e-01  8.6895534e-04]]\n",
            "Reward: [0.14784536]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [  0.80014575 -12.974813  ]\n",
            "Action taken: [  0.80014575 -12.974813  ]\n",
            "Observation: [[-5.7131398e-02  1.4412870e+00 -1.5894393e+00  5.9874650e-02\n",
            "   0.0000000e+00 -3.6177945e-01 -1.7540692e-01  8.6242473e-04]]\n",
            "Reward: [0.1473798]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [  0.44519332 -57.299503  ]\n",
            "Action taken: [  0.44519332 -57.299503  ]\n",
            "Observation: [[-1.4245118e-01  1.4311202e+00 -1.5491937e+00  5.9431199e-02\n",
            "   0.0000000e+00  8.9753783e-01 -1.7407301e-01  8.5603859e-04]]\n",
            "Reward: [0.14682135]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [ 0.5788971 46.950718 ]\n",
            "Action taken: [ 0.5788971 46.950718 ]\n",
            "Observation: [[-2.2772501e-01  1.4217722e+00 -1.5118583e+00  5.8997456e-02\n",
            "   0.0000000e+00 -1.4463383e-01 -1.7276908e-01  8.4979157e-04]]\n",
            "Reward: [0.14617626]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [ 0.4223005 20.942654 ]\n",
            "Action taken: [ 0.4223005 20.942654 ]\n",
            "Observation: [[-3.1287602e-01  1.4131770e+00 -1.4770983e+00  5.8573075e-02\n",
            "   0.0000000e+00 -6.0729438e-01 -1.7149402e-01  8.4367878e-04]]\n",
            "Reward: [0.14545101]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [ 0.37375483 17.470314  ]\n",
            "Action taken: [ 0.37375483 17.470314  ]\n",
            "Observation: [[-3.9779523e-01  1.4052746e+00 -1.4446307e+00  5.8157723e-02\n",
            "   0.0000000e+00 -9.8605305e-01 -1.7024677e-01  8.3769549e-04]]\n",
            "Reward: [0.14465222]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [ 0.18039332 -7.6277637 ]\n",
            "Action taken: [ 0.18039332 -7.6277637 ]\n",
            "Observation: [[-4.8234382e-01  1.3980118e+00 -1.4142141e+00  5.7751082e-02\n",
            "   0.0000000e+00 -8.0518013e-01 -1.6902636e-01  8.3183707e-04]]\n",
            "Reward: [0.14378653]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [ 0.819844 12.657101]\n",
            "Action taken: [ 0.819844 12.657101]\n",
            "Observation: [[-5.6635725e-01  1.3913400e+00 -1.3856412e+00  5.7352852e-02\n",
            "   0.0000000e+00 -1.0742275e+00 -1.6783181e-01  8.2609936e-04]]\n",
            "Reward: [0.14286059]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [ 0.46762678 -7.591777  ]\n",
            "Action taken: [ 0.46762678 -7.591777  ]\n",
            "Observation: [[-6.4964831e-01  1.3852160e+00 -1.3587329e+00  5.6962751e-02\n",
            "   0.0000000e+00 -8.9230180e-01 -1.6666223e-01  8.2047819e-04]]\n",
            "Reward: [0.14188088]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [  0.9683885 -45.058174 ]\n",
            "Action taken: [  0.9683885 -45.058174 ]\n",
            "Observation: [[-7.3201132e-01  1.3795996e+00 -1.3333338e+00  5.6580499e-02\n",
            "   0.0000000e+00  1.2248494e-01 -1.6551678e-01  8.1496965e-04]]\n",
            "Reward: [0.14085369]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [ 0.18908495 58.429096  ]\n",
            "Action taken: [ 0.18908495 58.429096  ]\n",
            "Observation: [[-8.1322694e-01  1.3744549e+00 -1.3093079e+00  5.6205843e-02\n",
            "   0.0000000e+00 -1.1840901e+00 -1.6439462e-01  8.0957013e-04]]\n",
            "Reward: [0.1397851]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [ 0.5529569 59.586487 ]\n",
            "Action taken: [ 0.5529569 59.586487 ]\n",
            "Observation: [[-8.9306712e-01  1.3697491e+00 -1.2865356e+00  5.5838533e-02\n",
            "   0.0000000e+00 -2.4171615e+00 -1.6329499e-01  8.0427597e-04]]\n",
            "Reward: [0.13868089]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [ 0.4619011 45.176857 ]\n",
            "Action taken: [ 0.4619011 45.176857 ]\n",
            "Observation: [[-9.7130078e-01  1.3654523e+00 -1.2649117e+00  5.5478331e-02\n",
            "   0.0000000e+00 -3.1571884e+00 -1.6221713e-01  7.9908379e-04]]\n",
            "Reward: [0.13754655]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [ 0.8545015 34.286793 ]\n",
            "Action taken: [ 0.8545015 34.286793 ]\n",
            "Observation: [[-1.0476987e+00  1.3615370e+00 -1.2443427e+00  5.5125009e-02\n",
            "   0.0000000e+00 -3.5193768e+00 -1.6116032e-01  7.9399045e-04]]\n",
            "Reward: [0.13638724]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [  0.41531926 -46.522667  ]\n",
            "Action taken: [  0.41531926 -46.522667  ]\n",
            "Observation: [[-1.1220396e+00  1.3579777e+00 -1.2247455e+00  5.4778356e-02\n",
            "   0.0000000e+00 -2.5134985e+00 -1.6012391e-01  7.8899274e-04]]\n",
            "Reward: [0.13520777]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [  0.07480267 -34.734467  ]\n",
            "Action taken: [  0.07480267 -34.734467  ]\n",
            "Observation: [[-1.1941144e+00  1.3547515e+00 -1.2060460e+00  5.4438159e-02\n",
            "   0.0000000e+00 -1.8266501e+00 -1.5910724e-01  7.8408781e-04]]\n",
            "Reward: [0.13401258]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [ 0.7612069 15.110138 ]\n",
            "Action taken: [ 0.7612069 15.110138 ]\n",
            "Observation: [[-1.2637317e+00  1.3518372e+00 -1.1881777e+00  5.4104224e-02\n",
            "   0.0000000e+00 -2.0324023e+00 -1.5810969e-01  7.7927270e-04]]\n",
            "Reward: [0.13280581]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [ 0.98949456 25.497618  ]\n",
            "Action taken: [ 0.98949456 25.497618  ]\n",
            "Observation: [[-1.3307214e+00  1.3492143e+00 -1.1710807e+00  5.3776361e-02\n",
            "   0.0000000e+00 -2.3812187e+00 -1.5713069e-01  7.7454478e-04]]\n",
            "Reward: [0.1315912]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [ 0.8267217 59.9103   ]\n",
            "Action taken: [ 0.8267217 59.9103   ]\n",
            "Observation: [[-1.3949375e+00  1.3468651e+00 -1.1547011e+00  5.3454384e-02\n",
            "   0.0000000e+00 -2.9576497e+00 -1.5616964e-01  7.6990144e-04]]\n",
            "Reward: [0.13037223]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [  0.9441775 -29.245249 ]\n",
            "Action taken: [  0.9441775 -29.245249 ]\n",
            "Observation: [[-1.4562612e+00  1.3447725e+00 -1.1389902e+00  5.3138122e-02\n",
            "   0.0000000e+00 -2.3809333e+00 -1.5522601e-01  7.6534011e-04]]\n",
            "Reward: [0.12915199]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [  0.47888523 -10.875937  ]\n",
            "Action taken: [  0.47888523 -10.875937  ]\n",
            "Observation: [[-1.5146016e+00  1.3429208e+00 -1.1239036e+00  5.2827410e-02\n",
            "   0.0000000e+00 -2.1358528e+00 -1.5429927e-01  7.6085853e-04]]\n",
            "Reward: [0.12793328]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 0.7269542 28.485455 ]\n",
            "Action taken: [ 0.7269542 28.485455 ]\n",
            "Observation: [[-1.5698968e+00  1.3412956e+00 -1.1094010e+00  5.2522086e-02\n",
            "   0.0000000e+00 -2.4703991e+00 -1.5338895e-01  7.5645430e-04]]\n",
            "Reward: [0.12671863]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [ 4.345965e-03 -5.005672e+00]\n",
            "Action taken: [ 4.345965e-03 -5.005672e+00]\n",
            "Observation: [[-1.6221130e+00  1.3398832e+00 -1.0954458e+00  5.2221995e-02\n",
            "   0.0000000e+00 -2.3074296e+00 -1.5249455e-01  7.5212528e-04]]\n",
            "Reward: [0.12551026]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [  0.35676265 -58.955627  ]\n",
            "Action taken: [  0.35676265 -58.955627  ]\n",
            "Observation: [[-1.6712438e+00  1.3386710e+00 -1.0820042e+00  5.1926989e-02\n",
            "   0.0000000e+00 -1.4185497e+00 -1.5161560e-01  7.4786932e-04]]\n",
            "Reward: [-9.237092]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [  0.67567503 -50.38167   ]\n",
            "Action taken: [  0.67567503 -50.38167   ]\n",
            "Environment reset\n",
            "Observation: [[ 1.6923544  -0.01376218  0.9147326   1.3356948   0.         -0.4478551\n",
            "   5.385074   -9.433028  ]]\n",
            "Reward: [0.01938651]\n",
            "iteration: 88\n",
            "Done: [ True]\n",
            "Environment reset\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0') for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv())\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "R172XbXX5Am5",
        "outputId": "e7736cc8-fe63-4043-a55d-d6979cd68726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([1.173e+03, 4.450e+02, 1.000e+01, 1.000e+01, 0.000e+00, 3.550e+02,\n",
            "       1.000e+00, 0.000e+00], dtype=float32), {})\n",
            "Action taken: [  0.41326118 -30.036526  ]\n",
            "Observation: [1.173000e+03 4.450000e+02 1.000000e+01 0.000000e+00 0.000000e+00\n",
            " 4.300913e+02 1.000000e+00 0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.30303037 21.731445  ]\n",
            "Observation: [1.1730000e+03 4.5186722e+02 1.0000000e+01 2.7468817e+00 0.0000000e+00\n",
            " 3.7576270e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [ 0.23771848 30.775232  ]\n",
            "Observation: [1.1730000e+03 4.6445709e+02 1.0000000e+01 5.0359497e+00 0.0000000e+00\n",
            " 2.9882462e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [ 0.02417503 12.9953165 ]\n",
            "Observation: [1.1730000e+03 4.8181586e+02 1.0000000e+01 6.9435067e+00 0.0000000e+00\n",
            " 2.6633633e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [ 0.43754497 25.08639   ]\n",
            "Observation: [1.1730000e+03 5.0314868e+02 1.0000000e+01 8.5331373e+00 0.0000000e+00\n",
            " 2.0362036e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [  0.47403017 -37.70245   ]\n",
            "Observation: [1.1730000e+03 5.2779327e+02 1.0000000e+01 9.8578291e+00 0.0000000e+00\n",
            " 2.9787646e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [  0.19182333 -25.514242  ]\n",
            "Observation: [1.1730000e+03 5.5519763e+02 1.0000000e+01 1.0961740e+01 0.0000000e+00\n",
            " 3.6166208e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [  0.36528093 -43.736664  ]\n",
            "Observation: [1.1730000e+03 5.8099799e+02 1.0000000e+01 1.0320159e+01 0.0000000e+00\n",
            " 4.7100375e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.3445338 19.885456 ]\n",
            "Observation: [1.1730000e+03 5.9563116e+02 1.0000000e+01 5.8532510e+00 0.0000000e+00\n",
            " 4.2129010e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [ 0.12481071 -9.560768  ]\n",
            "Observation: [1.1730000e+03 6.0095819e+02 1.0000000e+01 2.1308274e+00 0.0000000e+00\n",
            " 4.4519202e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [2.7581133e-02 5.1199287e+01]\n",
            "Observation: [ 1.1730000e+03  5.9853021e+02  1.0000000e+01 -9.7119236e-01\n",
            "  0.0000000e+00  3.1719382e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.20808002 37.25371   ]\n",
            "Observation: [ 1.1730000e+03  5.8963971e+02  1.0000000e+01 -3.5562086e+00\n",
            "  0.0000000e+00  2.2405952e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [0.2748712 8.882144 ]\n",
            "Observation: [ 1.1730000e+03  5.7536371e+02  1.0000000e+01 -5.7103891e+00\n",
            "  0.0000000e+00  2.0185417e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [  0.9354631 -48.239136 ]\n",
            "Observation: [ 1.1730000e+03  5.5659985e+02  1.0000000e+01 -7.5055394e+00\n",
            "  0.0000000e+00  3.2245200e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [0.06454489 5.6555862 ]\n",
            "Observation: [ 1.1730000e+03  5.3409613e+02  1.0000000e+01 -9.0014982e+00\n",
            "  0.0000000e+00  3.0831305e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [ 0.641852 49.851246]\n",
            "Observation: [ 1.1730000e+03  5.0847580e+02  1.0000000e+01 -1.0248130e+01\n",
            "  0.0000000e+00  1.8368492e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.3602705 26.609514 ]\n",
            "Observation: [ 1.1730000e+03  4.8025833e+02  1.0000000e+01 -1.1286990e+01\n",
            "  0.0000000e+00  1.1716114e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [ 0.5311209 36.12368  ]\n",
            "Observation: [ 1.1730000e+03  4.4987656e+02  1.0000000e+01 -1.2152707e+01\n",
            "  0.0000000e+00  2.6851940e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [  0.6955506 -25.009724 ]\n",
            "Observation: [ 1.1730000e+03  4.1769122e+02  1.0000000e+01 -1.2874138e+01\n",
            "  0.0000000e+00  8.9376251e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [ 0.88767606 34.561935  ]\n",
            "Observation: [ 1.1730000e+03  3.8400290e+02  1.0000000e+01 -1.3475329e+01\n",
            "  0.0000000e+00  2.9714108e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [  0.885115 -26.777529]\n",
            "Observation: [ 1.1730000e+03  3.4906210e+02  1.0000000e+01 -1.3976323e+01\n",
            "  0.0000000e+00  6.9915230e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [ 0.89340085 42.543556  ]\n",
            "Observation: [ 1.1730000e+03  3.1307755e+02  1.0000000e+01 -1.4393818e+01\n",
            "  0.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [ 0.2587263 50.852753 ]\n",
            "Observation: [ 1.1730000e+03  2.7622324e+02  1.0000000e+01 -1.4741730e+01\n",
            "  0.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [ 0.8178401 43.86859  ]\n",
            "Observation: [ 1.1730000e+03  2.4518602e+02  1.0000000e+01 -1.2414882e+01\n",
            "  0.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [ 0.9131591 23.597975 ]\n",
            "Observation: [1173.        226.18889   -10.         -7.598853    0.          0.\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.55242485 29.408182  ]\n",
            "Observation: [1148.        207.19176   -10.         -7.598853    0.          0.\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [  0.41686782 -53.24088   ]\n",
            "Observation: [1123.        188.19463   -10.         -7.598853    0.        133.1022\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [  0.4744293 -12.341555 ]\n",
            "Observation: [1098.        169.1975    -10.         -7.598853    0.        163.95609\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [0.9204312 1.9382472]\n",
            "Observation: [1073.        150.20036   -10.         -7.598853    0.        159.11047\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [  0.30408645 -16.523718  ]\n",
            "Observation: [1048.        131.20323   -10.         -7.598853    0.        200.41975\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [ 0.7190894 24.797287 ]\n",
            "Observation: [1023.        112.20609   -10.         -7.598853    0.        138.42654\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [  0.67267317 -59.129772  ]\n",
            "Observation: [998.        93.20896  -10.        -7.598853   0.       286.25098\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [  0.04915946 -32.863274  ]\n",
            "Observation: [973.        74.21183  -10.        -7.598853   0.       368.40915\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [  0.30335596 -35.72888   ]\n",
            "Observation: [948.        55.214695 -10.        -7.598853   0.       457.73135\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [ 0.01765391 -1.556816  ]\n",
            "Observation: [923.        36.217564 -10.        -7.598853   0.       461.6234\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [  0.92601895 -11.381369  ]\n",
            "Observation: [898.        17.22043  -10.        -7.598853   0.       490.0768\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [  0.30472305 -12.939833  ]\n",
            "Observation: [873.        12.       -10.         7.598853   0.       522.4264\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [  0.7607513 -34.063976 ]\n",
            "Observation: [848.        30.997133 -10.         7.598853   0.       607.58636\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [ 0.9914718 25.677868 ]\n",
            "Observation: [823.        49.994267 -10.         7.598853   0.       543.39166\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.49824566 32.489754  ]\n",
            "Observation: [798.        68.991394 -10.         7.598853   0.       462.1673\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [ 0.7299181 26.114353 ]\n",
            "Observation: [773.        87.98853  -10.         7.598853   0.       396.8814\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [ 0.90579206 40.852806  ]\n",
            "Observation: [748.       106.985664 -10.         7.598853   0.       294.7494\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [ 0.9007149 49.42443  ]\n",
            "Observation: [723.       125.982796 -10.         7.598853   0.       171.18832\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [  0.04002114 -26.880703  ]\n",
            "Observation: [698.       144.97993  -10.         7.598853   0.       238.39008\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [ 0.1450882 12.401741 ]\n",
            "Observation: [673.       163.97707  -10.         7.598853   0.       207.38573\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [ 0.28447074 33.32331   ]\n",
            "Observation: [648.       182.9742   -10.         7.598853   0.       124.077446\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [  0.9840044 -39.828278 ]\n",
            "Observation: [623.       201.97133  -10.         7.598853   0.       223.64813\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [  0.22090149 -16.846468  ]\n",
            "Observation: [598.       220.96846  -10.         7.598853   0.       265.7643\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.44776398 32.342125  ]\n",
            "Observation: [573.       239.96559  -10.         7.598853   0.       184.909\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [ 0.9255464 -4.64154  ]\n",
            "Observation: [548.       258.96274  -10.         7.598853   0.       196.51285\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [  0.58386385 -57.188698  ]\n",
            "Observation: [523.       277.95987  -10.         7.598853   0.       339.4846\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [ 0.9038092 47.817753 ]\n",
            "Observation: [498.       296.957    -10.         7.598853   0.       219.94022\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [  0.7917883 -27.752243 ]\n",
            "Observation: [473.       315.95413  -10.         7.598853   0.       289.32083\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [ 0.61017233 39.190006  ]\n",
            "Observation: [448.       334.95126  -10.         7.598853   0.       191.3458\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.6529992 36.811813 ]\n",
            "Observation: [423.       353.9484   -10.         7.598853   0.        99.31627\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [ 0.23825711 13.111245  ]\n",
            "Observation: [398.       372.94553  -10.         7.598853   0.        66.538155\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [  0.33008027 -22.046513  ]\n",
            "Observation: [373.       391.94266  -10.         7.598853   0.       121.654434\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [ 0.38721836 19.947819  ]\n",
            "Observation: [348.       410.9398   -10.         7.598853   0.        71.78489\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [  0.10338798 -19.405838  ]\n",
            "Observation: [323.       429.93692  -10.         7.598853   0.       120.299484\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [  0.48600656 -17.508774  ]\n",
            "Observation: [298.       448.93405  -10.         7.598853   0.       164.07143\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [ 2.3946741e-04 -2.4119066e+01]\n",
            "Observation: [273.       467.93118  -10.         7.598853   0.       224.36908\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [  0.04360808 -14.570114  ]\n",
            "Observation: [248.       486.9283   -10.         7.598853   0.       260.79437\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [  0.13331364 -45.012123  ]\n",
            "Observation: [223.       505.92545  -10.         7.598853   0.       373.32468\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [ 0.09757111 40.51637   ]\n",
            "Observation: [198.       524.9226   -10.         7.598853   0.       272.03375\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [  0.5016988 -21.426083 ]\n",
            "Observation: [173.       543.91974  -10.         7.598853   0.       325.59897\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [  0.8215211 -17.226255 ]\n",
            "Observation: [148.       562.9169   -10.         7.598853   0.       368.6646\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [ 0.44680062 37.175476  ]\n",
            "Observation: [123.       581.914    -10.         7.598853   0.       275.72592\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [  0.5535986 -49.24792  ]\n",
            "Observation: [ 98.       600.91113  -10.         7.598853   0.       398.8457\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [ 0.92050934 -0.4415304 ]\n",
            "Observation: [ 73.       619.90826  -10.         7.598853   0.       399.94952\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [ 0.8291389 51.843735 ]\n",
            "Observation: [ 48.       638.9054   -10.         7.598853   0.       270.3402\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [2.2412121e-02 2.6160393e+01]\n",
            "Observation: [ 23.       657.9025   -10.         7.598853   0.       204.93922\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [  0.9060198 -16.713184 ]\n",
            "Observation: [ -2.       676.89966  -10.         7.598853   0.       246.72218\n",
            "   0.         0.      ]\n",
            "Reward: -4.709604200911083\n",
            "iteration: 71\n",
            "Done: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: [  0.27076316 -52.13344   ]\n",
            "Observation: [ -2.       676.89966  -10.         7.598853   0.       246.72218\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 72\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ],
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and train\n",
        "ent_coef=0.5\n",
        "eval_freq=2000\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef)\n",
        "# model = PPO.load(\"ppo_custom_pong\")\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_sjyFuJcGT-w",
        "outputId": "740a35f5-26f4-45c7-f1ef-e2ef82a38594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Eval num_timesteps=8000, episode_reward=-2.54 +/- 0.68\n",
            "Episode length: 68.00 +/- 11.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 68       |\n",
            "|    mean_reward     | -2.54    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -2.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.17    |\n",
            "| time/              |          |\n",
            "|    fps             | 2147     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-1.71 +/- 0.92\n",
            "Episode length: 83.80 +/- 23.01\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 83.8       |\n",
            "|    mean_reward          | -1.71      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 16000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17352124 |\n",
            "|    clip_fraction        | 0.422      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.17      |\n",
            "|    explained_variance   | 0.244      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.28      |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | 0.0395     |\n",
            "|    std                  | 1.38       |\n",
            "|    value_loss           | 0.929      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -1.71\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | -1.06    |\n",
            "| time/              |          |\n",
            "|    fps             | 1165     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-0.57 +/- 1.86\n",
            "Episode length: 120.80 +/- 23.59\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 121        |\n",
            "|    mean_reward          | -0.572     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 24000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16630663 |\n",
            "|    clip_fraction        | 0.435      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.82      |\n",
            "|    explained_variance   | 0.568      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.74      |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | 0.0424     |\n",
            "|    std                  | 1.9        |\n",
            "|    value_loss           | 0.747      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -0.57\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -0.991   |\n",
            "| time/              |          |\n",
            "|    fps             | 1042     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-1.54 +/- 0.84\n",
            "Episode length: 101.20 +/- 24.84\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 101        |\n",
            "|    mean_reward          | -1.54      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 32000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16927126 |\n",
            "|    clip_fraction        | 0.476      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.5       |\n",
            "|    explained_variance   | 0.64       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.09      |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | 0.0363     |\n",
            "|    std                  | 2.71       |\n",
            "|    value_loss           | 0.547      |\n",
            "----------------------------------------\n",
            "Evaluation at step 8000: mean reward -1.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -1.22    |\n",
            "| time/              |          |\n",
            "|    fps             | 981      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1.46 +/- 1.18\n",
            "Episode length: 94.20 +/- 20.31\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 94.2       |\n",
            "|    mean_reward          | -1.46      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 40000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14823309 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.17      |\n",
            "|    explained_variance   | 0.675      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.56      |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | 0.0285     |\n",
            "|    std                  | 3.74       |\n",
            "|    value_loss           | 0.614      |\n",
            "----------------------------------------\n",
            "Evaluation at step 10000: mean reward -1.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -1.67    |\n",
            "| time/              |          |\n",
            "|    fps             | 937      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-0.91 +/- 1.39\n",
            "Episode length: 111.40 +/- 57.57\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 111        |\n",
            "|    mean_reward          | -0.906     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 48000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16196406 |\n",
            "|    clip_fraction        | 0.418      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.81      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.67      |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 5.12       |\n",
            "|    value_loss           | 0.672      |\n",
            "----------------------------------------\n",
            "Evaluation at step 12000: mean reward -0.91\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.16    |\n",
            "| time/              |          |\n",
            "|    fps             | 926      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=0.32 +/- 2.31\n",
            "Episode length: 158.60 +/- 35.51\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 159        |\n",
            "|    mean_reward          | 0.324      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 56000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13600472 |\n",
            "|    clip_fraction        | 0.411      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -6.43      |\n",
            "|    explained_variance   | 0.627      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.79      |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | 0.0288     |\n",
            "|    std                  | 7.02       |\n",
            "|    value_loss           | 0.766      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 14000: mean reward 0.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | -1.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 839      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-2.09 +/- 1.43\n",
            "Episode length: 92.00 +/- 25.58\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 92         |\n",
            "|    mean_reward          | -2.09      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 64000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14800479 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.08      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.49      |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | 0.0366     |\n",
            "|    std                  | 9.76       |\n",
            "|    value_loss           | 0.663      |\n",
            "----------------------------------------\n",
            "Evaluation at step 16000: mean reward -2.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | -0.0483  |\n",
            "| time/              |          |\n",
            "|    fps             | 834      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 78       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=1.36 +/- 5.39\n",
            "Episode length: 138.40 +/- 110.77\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 138        |\n",
            "|    mean_reward          | 1.36       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 72000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14038308 |\n",
            "|    clip_fraction        | 0.404      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.72      |\n",
            "|    explained_variance   | 0.689      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.63      |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | 0.0373     |\n",
            "|    std                  | 13.2       |\n",
            "|    value_loss           | 0.777      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 1.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -0.39    |\n",
            "| time/              |          |\n",
            "|    fps             | 826      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-0.64 +/- 0.66\n",
            "Episode length: 110.20 +/- 35.67\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 110        |\n",
            "|    mean_reward          | -0.645     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 80000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17711544 |\n",
            "|    clip_fraction        | 0.463      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.35      |\n",
            "|    explained_variance   | 0.595      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.05      |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | 0.0447     |\n",
            "|    std                  | 18.2       |\n",
            "|    value_loss           | 0.47       |\n",
            "----------------------------------------\n",
            "Evaluation at step 20000: mean reward -0.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 129      |\n",
            "|    ep_rew_mean     | 1.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=1.48 +/- 2.63\n",
            "Episode length: 168.40 +/- 81.02\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 168        |\n",
            "|    mean_reward          | 1.48       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 88000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16022202 |\n",
            "|    clip_fraction        | 0.45       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.99      |\n",
            "|    explained_variance   | 0.503      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.49      |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | 0.0404     |\n",
            "|    std                  | 25.4       |\n",
            "|    value_loss           | 0.546      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 22000: mean reward 1.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 137      |\n",
            "|    ep_rew_mean     | 4        |\n",
            "| time/              |          |\n",
            "|    fps             | 828      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=25.55 +/- 48.76\n",
            "Episode length: 236.80 +/- 128.23\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 237        |\n",
            "|    mean_reward          | 25.6       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 96000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17208788 |\n",
            "|    clip_fraction        | 0.462      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.68      |\n",
            "|    explained_variance   | 0.444      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.69      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | 0.0502     |\n",
            "|    std                  | 36.1       |\n",
            "|    value_loss           | 0.392      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 24000: mean reward 25.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 134      |\n",
            "|    ep_rew_mean     | 6.9      |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=1.97 +/- 2.30\n",
            "Episode length: 161.60 +/- 67.48\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 162        |\n",
            "|    mean_reward          | 1.97       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 104000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17116281 |\n",
            "|    clip_fraction        | 0.451      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.4      |\n",
            "|    explained_variance   | 0.562      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.87      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 51.4       |\n",
            "|    value_loss           | 0.395      |\n",
            "----------------------------------------\n",
            "Evaluation at step 26000: mean reward 1.97\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | 10.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 819      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "# Sync normalization statistics from the training environment to the evaluation environment\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "eval_env.ret_rms = train_env.ret_rms\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mL_LYMHeejpF",
        "outputId": "a0c1b264-c71d-45dc-9f35-6c430a93be40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=-84.46 +/- 115.27\n",
            "Episode length: 128.80 +/- 39.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 129      |\n",
            "|    mean_reward     | -84.5    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -84.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -275     |\n",
            "| time/              |          |\n",
            "|    fps             | 2629     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-55.49 +/- 114.04\n",
            "Episode length: 131.00 +/- 70.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -55.5       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004481554 |\n",
            "|    clip_fraction        | 0.0276      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.1       |\n",
            "|    explained_variance   | 0.559       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.941      |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.00016     |\n",
            "|    std                  | 2.25e+06    |\n",
            "|    value_loss           | 1.42        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -55.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 121      |\n",
            "|    ep_rew_mean     | -252     |\n",
            "| time/              |          |\n",
            "|    fps             | 1159     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-54.59 +/- 49.95\n",
            "Episode length: 149.80 +/- 69.42\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 150          |\n",
            "|    mean_reward          | -54.6        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 24000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036968566 |\n",
            "|    clip_fraction        | 0.0201       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.1        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.763       |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00063      |\n",
            "|    std                  | 2.35e+06     |\n",
            "|    value_loss           | 1.69         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -54.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -262     |\n",
            "| time/              |          |\n",
            "|    fps             | 1023     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-59.87 +/- 43.11\n",
            "Episode length: 162.80 +/- 36.45\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 163          |\n",
            "|    mean_reward          | -59.9        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 32000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034573479 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.2        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.578       |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | 0.000596     |\n",
            "|    std                  | 2.45e+06     |\n",
            "|    value_loss           | 1.48         |\n",
            "------------------------------------------\n",
            "Evaluation at step 8000: mean reward -59.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 111      |\n",
            "|    ep_rew_mean     | -241     |\n",
            "| time/              |          |\n",
            "|    fps             | 941      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-164.55 +/- 161.91\n",
            "Episode length: 116.60 +/- 45.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 117         |\n",
            "|    mean_reward          | -165        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004476109 |\n",
            "|    clip_fraction        | 0.0281      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.3       |\n",
            "|    explained_variance   | 0.511       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.654      |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.000234    |\n",
            "|    std                  | 2.58e+06    |\n",
            "|    value_loss           | 1.46        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward -164.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | -255     |\n",
            "| time/              |          |\n",
            "|    fps             | 889      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-150.29 +/- 81.91\n",
            "Episode length: 91.40 +/- 36.15\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 91.4         |\n",
            "|    mean_reward          | -150         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034466675 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.4        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.01        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | 0.000546     |\n",
            "|    std                  | 2.68e+06     |\n",
            "|    value_loss           | 1.75         |\n",
            "------------------------------------------\n",
            "Evaluation at step 12000: mean reward -150.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -267     |\n",
            "| time/              |          |\n",
            "|    fps             | 870      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-122.84 +/- 159.98\n",
            "Episode length: 131.20 +/- 30.50\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -123        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003678766 |\n",
            "|    clip_fraction        | 0.0178      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.5       |\n",
            "|    explained_variance   | 0.588       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.18       |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.000835    |\n",
            "|    std                  | 2.8e+06     |\n",
            "|    value_loss           | 2.03        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward -122.84\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 112      |\n",
            "|    ep_rew_mean     | -266     |\n",
            "| time/              |          |\n",
            "|    fps             | 867      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-128.49 +/- 138.29\n",
            "Episode length: 138.00 +/- 51.68\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -128         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 64000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025062133 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.6        |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.02        |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | 0.00116      |\n",
            "|    std                  | 2.91e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 16000: mean reward -128.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -273     |\n",
            "| time/              |          |\n",
            "|    fps             | 850      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 77       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-75.76 +/- 35.28\n",
            "Episode length: 150.60 +/- 84.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 151          |\n",
            "|    mean_reward          | -75.8        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 72000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035605398 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.7        |\n",
            "|    explained_variance   | 0.479        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.703       |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | 0.000628     |\n",
            "|    std                  | 3.02e+06     |\n",
            "|    value_loss           | 1.78         |\n",
            "------------------------------------------\n",
            "Evaluation at step 18000: mean reward -75.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -245     |\n",
            "| time/              |          |\n",
            "|    fps             | 836      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-183.29 +/- 78.86\n",
            "Episode length: 98.40 +/- 33.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 98.4        |\n",
            "|    mean_reward          | -183        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003738511 |\n",
            "|    clip_fraction        | 0.0235      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.7       |\n",
            "|    explained_variance   | 0.544       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.86       |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.000683    |\n",
            "|    std                  | 3.16e+06    |\n",
            "|    value_loss           | 1.7         |\n",
            "-----------------------------------------\n",
            "Evaluation at step 20000: mean reward -183.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -235     |\n",
            "| time/              |          |\n",
            "|    fps             | 840      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-188.48 +/- 100.36\n",
            "Episode length: 95.40 +/- 32.52\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 95.4         |\n",
            "|    mean_reward          | -188         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 88000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040115286 |\n",
            "|    clip_fraction        | 0.0256       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.8        |\n",
            "|    explained_variance   | 0.502        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.799       |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.000368     |\n",
            "|    std                  | 3.31e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 22000: mean reward -188.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -277     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-109.59 +/- 63.85\n",
            "Episode length: 115.40 +/- 55.33\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 115          |\n",
            "|    mean_reward          | -110         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 96000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031413306 |\n",
            "|    clip_fraction        | 0.0192       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.9        |\n",
            "|    explained_variance   | 0.577        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.2         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | 0.000992     |\n",
            "|    std                  | 3.45e+06     |\n",
            "|    value_loss           | 1.44         |\n",
            "------------------------------------------\n",
            "Evaluation at step 24000: mean reward -109.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 115      |\n",
            "|    ep_rew_mean     | -209     |\n",
            "| time/              |          |\n",
            "|    fps             | 825      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=-130.40 +/- 80.68\n",
            "Episode length: 138.00 +/- 56.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -130         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035240077 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -33          |\n",
            "|    explained_variance   | 0.537        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.348       |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | 0.000891     |\n",
            "|    std                  | 3.61e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 26000: mean reward -130.40\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -234     |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "9142aaf6-b1f0-4e8b-c8dc-f263eacc127e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 36.81911273412406 +/- 62.40618049660436\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "77d49644-cce3-4465-9e91-6c20bad71e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n",
            "reset\n",
            "reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "eval_env.norm_reward = False  # Disable reward normalization for evaluation\n",
        "\n",
        "# Load the normalization statistics\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "\n",
        "# Sync the observation normalization statistics\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the VecNormalize wrapper\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0')])\n",
        "env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", env)\n",
        "\n",
        "# Extract normalization parameters\n",
        "mean = env.obs_rms.mean\n",
        "std = env.obs_rms.var ** 0.5\n",
        "\n",
        "# Save the mean and std to a file\n",
        "import json\n",
        "normalization_params = {'mean': mean.tolist(), 'std': std.tolist()}\n",
        "with open('normalization_params.json', 'w') as f:\n",
        "    json.dump(normalization_params, f)"
      ],
      "metadata": {
        "id": "TCTA0XkE21Kd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.policy)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Print the state dictionary keys to understand the structure\n",
        "for name, param in policy.state_dict().items():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "id": "tOQDfwZkEce8",
        "outputId": "69aa6137-811b-42f5-e1b8-c532007e06db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorCriticPolicy(\n",
            "  (features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (pi_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (vf_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (mlp_extractor): MlpExtractor(\n",
            "    (policy_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (value_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "log_std torch.Size([2])\n",
            "mlp_extractor.policy_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.policy_net.0.bias torch.Size([64])\n",
            "mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.policy_net.2.bias torch.Size([64])\n",
            "mlp_extractor.value_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.value_net.0.bias torch.Size([64])\n",
            "mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.value_net.2.bias torch.Size([64])\n",
            "action_net.weight torch.Size([2, 64])\n",
            "action_net.bias torch.Size([2])\n",
            "value_net.weight torch.Size([1, 64])\n",
            "value_net.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the TensorFlow model structure\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class CustomPongModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomPongModel, self).__init__()\n",
        "        self.dense1_policy = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense2_policy = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense1_value = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense2_value = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.action = tf.keras.layers.Dense(2)\n",
        "        self.value = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Policy network\n",
        "        pi_x = self.dense1_policy(x)\n",
        "        pi_x = self.dense2_policy(pi_x)\n",
        "        action = self.action(pi_x)\n",
        "\n",
        "        # Value network\n",
        "        vf_x = self.dense1_value(x)\n",
        "        vf_x = self.dense2_value(vf_x)\n",
        "        value = self.value(vf_x)\n",
        "\n",
        "        return action, value\n",
        "\n",
        "# Instantiate the TensorFlow model\n",
        "tf_model = CustomPongModel()\n"
      ],
      "metadata": {
        "id": "uI7i6NHBE6kO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# need to run once to initialise model\n",
        "dummy_input = np.random.randn(1, 8).astype(np.float32)  # Assuming input size is 8\n",
        "tf_model(dummy_input)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Extract the weights and biases\n",
        "weights = {}\n",
        "for name, param in policy.state_dict().items():\n",
        "    weights[name] = param.cpu().numpy()\n",
        "\n",
        "# Map the weights to the TensorFlow model\n",
        "# Note: Transpose the weights because PyTorch uses [out_features, in_features] and TensorFlow uses [in_features, out_features]\n",
        "\n",
        "# Policy network\n",
        "tf_model.dense1_policy.set_weights([weights['mlp_extractor.policy_net.0.weight'].T, weights['mlp_extractor.policy_net.0.bias']])\n",
        "tf_model.dense2_policy.set_weights([weights['mlp_extractor.policy_net.2.weight'].T, weights['mlp_extractor.policy_net.2.bias']])\n",
        "\n",
        "# Value network\n",
        "tf_model.dense1_value.set_weights([weights['mlp_extractor.value_net.0.weight'].T, weights['mlp_extractor.value_net.0.bias']])\n",
        "tf_model.dense2_value.set_weights([weights['mlp_extractor.value_net.2.weight'].T, weights['mlp_extractor.value_net.2.bias']])\n",
        "\n",
        "# Action and value outputs\n",
        "tf_model.action.set_weights([weights['action_net.weight'].T, weights['action_net.bias']])\n",
        "tf_model.value.set_weights([weights['value_net.weight'].T, weights['value_net.bias']])\n"
      ],
      "metadata": {
        "id": "46982YgHJNNr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model.save(\"tf_custom_pong_model.keras\")"
      ],
      "metadata": {
        "id": "4Yc_bcrpJwOJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeLcH84ysrNd",
        "outputId": "72ddfd7f-6bf1-429f-8867-862223902ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 10:55:57.692872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorflowjs_converter\", line 8, in <module>\n",
            "    sys.exit(pip_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 959, in pip_main\n",
            "    main([' '.join(sys.argv[1:])])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 963, in main\n",
            "    convert(argv[0].split(' '))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 949, in convert\n",
            "    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 635, in _dispatch_converter\n",
            "    dispatch_keras_h5_to_tfjs_graph_model_conversion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 227, in dispatch_keras_h5_to_tfjs_graph_model_conversion\n",
            "    model = tf_keras.models.load_model(h5_path, compile=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_api.py\", line 254, in load_model\n",
            "    return saving_lib.load_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_lib.py\", line 271, in load_model\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_lib.py\", line 236, in load_model\n",
            "    model = deserialize_keras_object(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/serialization_lib.py\", line 704, in deserialize_keras_object\n",
            "    cls = _retrieve_class_or_fn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/serialization_lib.py\", line 834, in _retrieve_class_or_fn\n",
            "    raise TypeError(\n",
            "TypeError: Could not locate class 'CustomPongModel'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CustomPongModel', 'config': {'trainable': True, 'dtype': 'float32'}, 'registered_name': 'Custom>CustomPongModel', 'build_config': {'input_shape': [1, 8]}, 'compile_config': None}\n"
          ]
        }
      ],
      "source": [
        "# pip install tensorflowjs\n",
        "# %pip install --upgrade numpy\n",
        "# %pip install --upgrade tensorflow\n",
        "# %pip install --upgrade tensorflowjs\n",
        "\n",
        "# Convert TensorFlow to TensorFlow.js\n",
        "!tensorflowjs_converter --input_format=keras --output_format=tfjs_graph_model ./tf_custom_pong_model.keras ./tfjs_custom_pong_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pygame\n",
        "%pip install moviepy"
      ],
      "metadata": {
        "id": "OkfFEBE-Gqtx",
        "outputId": "7f96a9b1-6613-4071-d834-5670e2954a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}