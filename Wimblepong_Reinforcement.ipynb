{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "fc1c823b-97f6-41b4-9f79-32cfc88f84ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=745d23a0a864def8c74535e8401d42b967696393fa2b2bc5530226a812e2e2f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m61.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.2)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.4.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.1 ydf-0.5.0\n",
            "Collecting onnx2tf\n",
            "  Downloading onnx2tf-1.22.4-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.15.0\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.17.1\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (3.20.3)\n",
            "Collecting coloredlogs (from onnxruntime==1.17.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.17.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.37.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.17.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.17.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n",
            "Installing collected packages: onnx2tf, onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.15.0 onnx2tf-1.22.4 onnxruntime-1.17.1\n",
            "Collecting onnx_graphsurgeon\n",
            "  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m132.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.25.2)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.14.0->onnx_graphsurgeon) (3.20.3)\n",
            "Installing collected packages: onnx_graphsurgeon\n",
            "Successfully installed onnx_graphsurgeon-0.5.2\n",
            "Collecting sng4onnx\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: sng4onnx\n",
            "Successfully installed sng4onnx-1.0.4\n"
          ]
        }
      ],
      "source": [
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install tensorflowjs\n",
        "%pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
        "\n",
        "# onnx2tf deps:\n",
        "%pip install onnx_graphsurgeon\n",
        "%pip install sng4onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "44f44a62-b54d-483b-d8eb-fa9d3c1ca242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"26-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "1d32bd76-67b7-4e0d-e2d4-6d72dd95571f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "import onnx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'red', 'Player2': 'blue'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    \"serve\": lambda ball_speed: ball_speed,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "def bounded_value(value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N932taVhk14O"
      },
      "outputs": [],
      "source": [
        "class ComputerPlayer:\n",
        "    def __init__(self):\n",
        "        self.reset(serve_delay=50, initial_direction=15, offset=0, max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "\n",
        "    def reset(self, serve_delay, initial_direction, offset, max_speed):\n",
        "        # print(\"ComputerPlayer reset\")\n",
        "        self.serve_delay = serve_delay\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = initial_direction\n",
        "        self.offset = initial_direction\n",
        "        self.max_speed = max_speed\n",
        "\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2  ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iIpdvdwMk14O"
      },
      "outputs": [],
      "source": [
        "class PongGame:\n",
        "    def __init__(self, server, positions_reversed, player, opponent):\n",
        "        self.game_state = {\n",
        "        'server': server,\n",
        "        'positions_reversed': positions_reversed,\n",
        "        'player': player,\n",
        "        'opponent': opponent,\n",
        "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def update_game_state(self, actions, delta_time, step_count):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            return 0.01, True\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            if self.game_state['player'] == server:\n",
        "                reward += abs(ball['dy']) * ((1000-step_count)/1000)\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if self.game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if player_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not player_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if player_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self, computer_player):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "          #  {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "        self.starting_state_index = 0\n",
        "\n",
        "        self.computer_player = computer_player\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"CustomPongEnv reset\")\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "        # print(self.starting_state_index)\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        player = starting_state['player']\n",
        "        opponent = starting_state['opponent']\n",
        "        self.computer_player.reset(serve_delay=random.randint(10, 10), initial_direction=random.randint(-60, 60), offset=random.randint(-PADDLE_HEIGHT/2, PADDLE_HEIGHT/2), max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 60}\n",
        "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
        "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
        "        reward, terminated = self.game.update_game_state(actions, 2.5, self.step_count)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        truncated = False\n",
        "        if self.step_count > 1000:\n",
        "            terminated = True\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x'] / COURT_WIDTH),\n",
        "            float(state['ball']['y'] / COURT_HEIGHT),\n",
        "            float(state['ball']['dx'] / 40),\n",
        "            float(state['ball']['dy'] / 40),\n",
        "            float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
        "            float(paddle['y'] / COURT_HEIGHT),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(int(is_server)),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "outputId": "27f4d6e8-3cf2-4e9c-e44c-db98f45d8183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: [[0.9691667 0.55625   0.25      0.25      1.        0.44375   1.\n",
            "  1.       ]]\n",
            "Action taken: [0.30341876 0.5783725 ]\n",
            "Observation: [[0.9691667  0.55625    0.25       0.         1.         0.55219483\n",
            "  1.         1.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [ 0.07648794 -0.65759397]\n",
            "Observation: [[0.9691667  0.57432413 0.25       0.14459312 1.         0.42889598\n",
            "  1.         1.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [0.29856917 0.47819215]\n",
            "Observation: [[ 0.9691667   0.5688361   0.25       -0.04390423  1.          0.518557\n",
            "   1.          1.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [ 0.13233593 -0.2981897 ]\n",
            "Observation: [[0.9691667  0.5792063  0.25       0.08296119 1.         0.46264642\n",
            "  1.         1.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [ 0.9160767 -0.893009 ]\n",
            "Observation: [[ 0.9691667   0.5785296  -0.25       -0.00541311  1.          0.29520723\n",
            "   0.          1.        ]]\n",
            "Reward: [2.499932]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [ 0.7906584 -0.4043491]\n",
            "Observation: [[ 0.9483333   0.57785296 -0.25       -0.00541311  1.          0.2193918\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01582886]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [0.24171086 0.41003293]\n",
            "Observation: [[ 0.9275      0.57717633 -0.25       -0.00541311  1.          0.29627296\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01509737]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [0.07914113 0.85583556]\n",
            "Observation: [[ 0.9066667   0.5764997  -0.25       -0.00541311  1.          0.45674214\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01496132]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [0.4562947 0.9099393]\n",
            "Observation: [[ 0.8858333   0.57582307 -0.25       -0.00541311  1.          0.62735575\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01507333]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [ 0.874794  -0.4290636]\n",
            "Observation: [[ 0.865       0.57514644 -0.25       -0.00541311  1.          0.54690635\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01530784]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [0.7607286  0.46911347]\n",
            "Observation: [[ 0.8441667   0.5744698  -0.25       -0.00541311  1.          0.6348651\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01560871]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [ 0.00349654 -0.9235633 ]\n",
            "Observation: [[ 0.8233333   0.5737932  -0.25       -0.00541311  1.          0.46169698\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01594736]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [0.8323424  0.70549077]\n",
            "Observation: [[ 0.8025      0.57311654 -0.25       -0.00541311  1.          0.5939765\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01630796]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [0.9060531 0.8791178]\n",
            "Observation: [[ 0.7816667   0.57243985 -0.25       -0.00541311  1.          0.75881106\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01668116]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [0.46822786 0.10618816]\n",
            "Observation: [[ 0.7608333   0.5717632  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01706118]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [0.8754432  0.46586928]\n",
            "Observation: [[ 0.74        0.5710866  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01744435]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [ 0.61142707 -0.38803387]\n",
            "Observation: [[ 0.7191667   0.57040995 -0.25       -0.00541311  1.          0.7022436\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01782827]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [0.569852  0.4385594]\n",
            "Observation: [[ 0.6983333   0.5697333  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01821135]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [0.058012 0.286183]\n",
            "Observation: [[ 0.6775      0.5690567  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01859251]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [ 0.07220939 -0.5284772 ]\n",
            "Observation: [[ 0.6566667   0.56838006 -0.25       -0.00541311  1.          0.67591053\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01897106]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [ 0.704682   -0.70611495]\n",
            "Observation: [[ 0.6358333   0.5677034  -0.25       -0.00541311  1.          0.54351395\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01934652]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [ 0.5802298  -0.18662572]\n",
            "Observation: [[ 0.615       0.5670268  -0.25       -0.00541311  1.          0.5085217\n",
            "   0.          1.        ]]\n",
            "Reward: [0.01971857]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [0.01470845 0.9446148 ]\n",
            "Observation: [[ 0.5941667   0.56635016 -0.25       -0.00541311  1.          0.68563694\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02008703]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [0.7643995  0.78589565]\n",
            "Observation: [[ 0.5733333   0.5656735  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02045178]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [0.29482198 0.9567468 ]\n",
            "Observation: [[ 0.5525      0.56499684 -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02081278]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [ 0.33995438 -0.40349692]\n",
            "Observation: [[ 0.5316667   0.5643202  -0.25       -0.00541311  1.          0.69934434\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02117]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [ 0.5539638 -0.6848453]\n",
            "Observation: [[ 0.5108333   0.5636436  -0.25       -0.00541311  1.          0.57093585\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02152347]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [0.42712095 0.28622183]\n",
            "Observation: [[ 0.49        0.56296694 -0.25       -0.00541311  1.          0.62460244\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02187323]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [0.08498236 0.27559972]\n",
            "Observation: [[ 0.46916667  0.5622903  -0.25       -0.00541311  1.          0.67627734\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02221931]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [ 0.6564672  -0.28944457]\n",
            "Observation: [[ 0.44833332  0.5616137  -0.25       -0.00541311  1.          0.62200654\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02256179]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [ 0.8619909  -0.97428346]\n",
            "Observation: [[ 0.4275      0.56093705 -0.25       -0.00541311  1.          0.43932837\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02290072]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [ 0.07748324 -0.5898535 ]\n",
            "Observation: [[ 0.40666667  0.5602604  -0.25       -0.00541311  1.          0.32873082\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02323619]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [0.49959588 0.6328023 ]\n",
            "Observation: [[ 0.38583332  0.5595837  -0.25       -0.00541311  1.          0.44738126\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02356827]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [0.41269785 0.7479791 ]\n",
            "Observation: [[ 0.365       0.5589071  -0.25       -0.00541311  1.          0.58762735\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02389701]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [0.5693406  0.08143148]\n",
            "Observation: [[ 0.34416667  0.55823046 -0.25       -0.00541311  1.          0.60289574\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02422251]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [0.5664413  0.08143686]\n",
            "Observation: [[ 0.32333332  0.5575538  -0.25       -0.00541311  1.          0.61816514\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02454483]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [0.7614704  0.32639775]\n",
            "Observation: [[ 0.3025      0.5568772  -0.25       -0.00541311  1.          0.67936474\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02486404]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [0.11694719 0.0392953 ]\n",
            "Observation: [[ 0.28166667  0.55620056 -0.25       -0.00541311  1.          0.6867326\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02518022]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [ 0.5180267  -0.23741719]\n",
            "Observation: [[ 0.26083332  0.55552393 -0.25       -0.00541311  1.          0.64221686\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02549342]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [0.10467591 0.38982555]\n",
            "Observation: [[ 0.24        0.5548473  -0.25       -0.00541311  1.          0.71530914\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02580372]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [0.940317  0.7804766]\n",
            "Observation: [[ 0.21916667  0.55417067 -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02611117]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [0.38637802 0.26507837]\n",
            "Observation: [[ 0.19833334  0.55349404 -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02641584]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [ 0.09202895 -0.04351673]\n",
            "Observation: [[ 0.1775      0.55281734 -0.25       -0.00541311  1.          0.76684064\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02671778]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [0.81786793 0.93338233]\n",
            "Observation: [[ 0.15666667  0.5521407  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02701706]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [ 0.32287943 -0.11523549]\n",
            "Observation: [[ 0.13583334  0.5514641  -0.25       -0.00541311  1.          0.75339335\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02731372]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [ 0.44815454 -0.09168682]\n",
            "Observation: [[ 0.115       0.55078745 -0.25       -0.00541311  1.          0.73620206\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02760781]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [ 0.626929   -0.03652184]\n",
            "Observation: [[ 0.09416667  0.5501108  -0.25       -0.00541311  1.          0.7293542\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02789939]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [0.29252636 0.36937878]\n",
            "Observation: [[ 0.07333333  0.5494342  -0.25       -0.00541311  1.          0.775\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0281885]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [ 0.2418296 -0.5783562]\n",
            "Observation: [[ 0.0525      0.54875755 -0.25       -0.00541311  1.          0.6665582\n",
            "   0.          1.        ]]\n",
            "Reward: [0.02847519]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [ 0.22072239 -0.7213646 ]\n",
            "Observation: [[ 0.03166667  0.5480809  -0.25       -0.00541311  1.          0.53130233\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0287595]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [ 0.628122   -0.10238403]\n",
            "Observation: [[ 0.03083333  0.5474043   0.2857619  -0.12647685  1.          0.51210535\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0580785]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [ 0.3468645  -0.38068143]\n",
            "Observation: [[ 0.05464682  0.53159463  0.2857619  -0.12647685  1.          0.44072756\n",
            "   0.          1.        ]]\n",
            "Reward: [0.05862862]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [0.8675     0.18374647]\n",
            "Observation: [[ 0.07846031  0.51578504  0.2857619  -0.12647685  1.          0.47518003\n",
            "   0.          1.        ]]\n",
            "Reward: [0.05916911]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [0.8529403  0.05523859]\n",
            "Observation: [[ 0.10227381  0.49997544  0.2857619  -0.12647685  1.          0.48553726\n",
            "   0.          1.        ]]\n",
            "Reward: [0.05969974]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [ 0.6841932 -0.767396 ]\n",
            "Observation: [[ 0.1260873   0.48416585  0.2857619  -0.12647685  1.          0.34165052\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06022029]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [0.8115074  0.53830665]\n",
            "Observation: [[ 0.14990078  0.46835622  0.2857619  -0.12647685  1.          0.44258302\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06073057]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [ 0.80374014 -0.4873254 ]\n",
            "Observation: [[ 0.17371428  0.45254663  0.2857619  -0.12647685  1.          0.35120952\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06123038]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [0.40309715 0.0901907 ]\n",
            "Observation: [[ 0.19752777  0.43673703  0.2857619  -0.12647685  1.          0.36812028\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06171957]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [ 0.41443408 -0.01402969]\n",
            "Observation: [[ 0.22134125  0.4209274   0.2857619  -0.12647685  1.          0.3654897\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06219797]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [0.45957083 0.81228256]\n",
            "Observation: [[ 0.24515475  0.4051178   0.2857619  -0.12647685  1.          0.5177927\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06266546]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [ 0.38463897 -0.1626202 ]\n",
            "Observation: [[ 0.26896822  0.3893082   0.2857619  -0.12647685  1.          0.4873014\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0631219]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [0.03046304 0.7926548 ]\n",
            "Observation: [[ 0.29278174  0.3734986   0.2857619  -0.12647685  1.          0.63592416\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06356721]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [0.9950291  0.19503643]\n",
            "Observation: [[ 0.31659523  0.357689    0.2857619  -0.12647685  1.          0.6724935\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06400128]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.8435975  -0.72274226]\n",
            "Observation: [[ 0.3404087   0.34187937  0.2857619  -0.12647685  1.          0.5369793\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06442404]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [ 0.994156   -0.55770016]\n",
            "Observation: [[ 0.3642222   0.32606977  0.2857619  -0.12647685  1.          0.43241054\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06483544]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [0.28876352 0.40891817]\n",
            "Observation: [[ 0.38803568  0.31026018  0.2857619  -0.12647685  1.          0.50908273\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06523544]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [ 0.08104722 -0.13109311]\n",
            "Observation: [[ 0.41184917  0.29445055  0.2857619  -0.12647685  1.          0.48450273\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06562401]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [ 0.3115582  -0.10139658]\n",
            "Observation: [[ 0.4356627   0.27864096  0.2857619  -0.12647685  1.          0.46549088\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06600115]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [0.31973737 0.65903705]\n",
            "Observation: [[ 0.45947617  0.26283136  0.2857619  -0.12647685  1.          0.5890603\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06636684]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [ 0.4723618 -0.7349511]\n",
            "Observation: [[ 0.48328966  0.24702173  0.2857619  -0.12647685  1.          0.45125702\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06672111]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [0.88133985 0.4542631 ]\n",
            "Observation: [[ 0.50710315  0.23121214  0.2857619  -0.12647685  1.          0.5364313\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06706401]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [ 0.8168833 -0.9211791]\n",
            "Observation: [[ 0.53091663  0.21540253  0.2857619  -0.12647685  1.          0.36371025\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06739556]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [0.5299067 0.939752 ]\n",
            "Observation: [[ 0.5547301   0.19959292  0.2857619  -0.12647685  1.          0.5399138\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06771583]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [ 0.40893197 -0.5566905 ]\n",
            "Observation: [[ 0.5785436   0.18378331  0.2857619  -0.12647685  1.          0.43553427\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0680249]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [ 0.53960127 -0.6439017 ]\n",
            "Observation: [[ 0.6023571   0.16797371  0.2857619  -0.12647685  1.          0.3148027\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06832284]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [0.34624115 0.5864113 ]\n",
            "Observation: [[ 0.6261706   0.1521641   0.2857619  -0.12647685  1.          0.42475483\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06860975]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [0.814052   0.00194385]\n",
            "Observation: [[ 0.64998406  0.13635449  0.2857619  -0.12647685  1.          0.4251193\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06888574]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [ 0.06520034 -0.37066752]\n",
            "Observation: [[ 0.6737976   0.12054489  0.2857619  -0.12647685  1.          0.35561913\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06915093]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [ 0.2489144  -0.97878283]\n",
            "Observation: [[ 0.6976111   0.10473528  0.2857619  -0.12647685  1.          0.17209736\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06940544]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [ 0.5230871 -0.5690353]\n",
            "Observation: [[ 0.7214246   0.08892567  0.2857619  -0.12647685  1.          0.06540325\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06964942]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [0.19761014 0.3891657 ]\n",
            "Observation: [[ 0.74523807  0.07311606  0.2857619  -0.12647685  1.          0.13837181\n",
            "   0.          1.        ]]\n",
            "Reward: [0.06988301]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [ 0.8761894 -0.6428941]\n",
            "Observation: [[ 0.76905155  0.05730646  0.2857619  -0.12647685  1.          0.01782917\n",
            "   0.          1.        ]]\n",
            "Reward: [0.07010636]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [0.45905024 0.43810505]\n",
            "Observation: [[ 0.79286504  0.04149685  0.2857619  -0.12647685  1.          0.09997386\n",
            "   0.          1.        ]]\n",
            "Reward: [0.07031963]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [ 0.5728457  -0.27911448]\n",
            "Observation: [[ 0.8166785   0.02568725  0.2857619  -0.12647685  1.          0.0476399\n",
            "   0.          1.        ]]\n",
            "Reward: [0.07052299]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [0.911676  0.9605461]\n",
            "Observation: [[0.840492   0.015      0.2857619  0.12647685 1.         0.22774228\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07071662]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 0.61072797 -0.54061174]\n",
            "Observation: [[0.8643055  0.03080961 0.2857619  0.12647685 1.         0.12637758\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0709007]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [0.2039135  0.12424494]\n",
            "Observation: [[0.888119   0.04661921 0.2857619  0.12647685 1.         0.1496735\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07107542]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [ 0.2663642  -0.91090137]\n",
            "Observation: [[0.91193247 0.06242882 0.2857619  0.12647685 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07124096]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [0.20810634 0.6551497 ]\n",
            "Observation: [[0.93574595 0.07823843 0.2857619  0.12647685 1.         0.12284057\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07139752]\n",
            "iteration: 88\n",
            "Done: [False]\n",
            "Action taken: [ 0.3536492 -0.9410517]\n",
            "Observation: [[0.95955944 0.09404803 0.2857619  0.12647685 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0715453]\n",
            "iteration: 89\n",
            "Done: [False]\n",
            "Action taken: [ 0.49933422 -0.21010765]\n",
            "Observation: [[ 0.9691667   0.10985764 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10749207]\n",
            "iteration: 90\n",
            "Done: [False]\n",
            "Action taken: [ 0.79947025 -0.09555559]\n",
            "Observation: [[ 0.91768426  0.10843292 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10761809]\n",
            "iteration: 91\n",
            "Done: [False]\n",
            "Action taken: [0.74386   0.4529214]\n",
            "Observation: [[ 0.8662018   0.1070082  -0.61778927 -0.01139775  1.          0.08492276\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10769577]\n",
            "iteration: 92\n",
            "Done: [False]\n",
            "Action taken: [ 0.45736468 -0.74213135]\n",
            "Observation: [[ 0.8147194   0.10558348 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10772513]\n",
            "iteration: 93\n",
            "Done: [False]\n",
            "Action taken: [ 0.8847114 -0.6199839]\n",
            "Observation: [[ 0.76323694  0.10415877 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10770655]\n",
            "iteration: 94\n",
            "Done: [False]\n",
            "Action taken: [ 0.3386462 -0.4769312]\n",
            "Observation: [[ 0.7117545   0.10273404 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10764061]\n",
            "iteration: 95\n",
            "Done: [False]\n",
            "Action taken: [0.16771187 0.04868028]\n",
            "Observation: [[ 0.66027206  0.10130933 -0.61778927 -0.01139775  1.          0.00912755\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10752817]\n",
            "iteration: 96\n",
            "Done: [False]\n",
            "Action taken: [ 0.0169152  -0.62987673]\n",
            "Observation: [[ 0.6087896   0.09988461 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10737035]\n",
            "iteration: 97\n",
            "Done: [False]\n",
            "Action taken: [0.83525467 0.34517467]\n",
            "Observation: [[ 0.5573072   0.09845989 -0.61778927 -0.01139775  1.          0.06472025\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10716841]\n",
            "iteration: 98\n",
            "Done: [False]\n",
            "Action taken: [0.7828825 0.650711 ]\n",
            "Observation: [[ 0.50582474  0.09703518 -0.61778927 -0.01139775  1.          0.18672857\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10692389]\n",
            "iteration: 99\n",
            "Done: [False]\n",
            "Action taken: [0.88839984 0.1816612 ]\n",
            "Observation: [[ 0.45434228  0.09561045 -0.61778927 -0.01139775  1.          0.22079004\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10663842]\n",
            "iteration: 100\n",
            "Done: [False]\n",
            "Action taken: [ 0.80397636 -0.8060152 ]\n",
            "Observation: [[ 0.40285984  0.09418574 -0.61778927 -0.01139775  1.          0.06966219\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10631382]\n",
            "iteration: 101\n",
            "Done: [False]\n",
            "Action taken: [ 0.3248238 -0.4918572]\n",
            "Observation: [[ 0.3513774   0.09276102 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10595199]\n",
            "iteration: 102\n",
            "Done: [False]\n",
            "Action taken: [0.2920925 0.8074961]\n",
            "Observation: [[ 0.29989496  0.0913363  -0.61778927 -0.01139775  1.          0.15140551\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10555494]\n",
            "iteration: 103\n",
            "Done: [False]\n",
            "Action taken: [ 0.62128824 -0.03946991]\n",
            "Observation: [[ 0.24841252  0.08991158 -0.61778927 -0.01139775  1.          0.14400491\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10512477]\n",
            "iteration: 104\n",
            "Done: [False]\n",
            "Action taken: [ 0.45811126 -0.09215362]\n",
            "Observation: [[ 0.19693008  0.08848687 -0.61778927 -0.01139775  1.          0.1267261\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10466358]\n",
            "iteration: 105\n",
            "Done: [False]\n",
            "Action taken: [ 0.7639277 -0.6139644]\n",
            "Observation: [[ 0.14544764  0.08706214 -0.61778927 -0.01139775  1.          0.01160778\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10417353]\n",
            "iteration: 106\n",
            "Done: [False]\n",
            "Action taken: [0.95770895 0.3172325 ]\n",
            "Observation: [[ 0.09396521  0.08563743 -0.61778927 -0.01139775  1.          0.07108887\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10365679]\n",
            "iteration: 107\n",
            "Done: [False]\n",
            "Action taken: [ 0.9268033 -0.6881758]\n",
            "Observation: [[ 0.04248277  0.08421271 -0.61778927 -0.01139775  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10311549]\n",
            "iteration: 108\n",
            "Done: [False]\n",
            "Action taken: [0.65034884 0.5551822 ]\n",
            "Observation: [[0.03083333 0.08278799 0.342471   0.00445632 1.         0.10409667\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13667247]\n",
            "iteration: 109\n",
            "Done: [False]\n",
            "Action taken: [ 0.45424497 -0.15395926]\n",
            "Observation: [[0.05937259 0.08334503 0.342471   0.00445632 1.         0.0752293\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13576986]\n",
            "iteration: 110\n",
            "Done: [False]\n",
            "Action taken: [0.40814343 0.9227536 ]\n",
            "Observation: [[0.08791184 0.08390207 0.342471   0.00445632 1.         0.2482456\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13478544]\n",
            "iteration: 111\n",
            "Done: [False]\n",
            "Action taken: [ 0.6516726  -0.41464898]\n",
            "Observation: [[0.11645108 0.08445911 0.342471   0.00445632 1.         0.17049892\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13372558]\n",
            "iteration: 112\n",
            "Done: [False]\n",
            "Action taken: [ 0.5890304 -0.4927444]\n",
            "Observation: [[0.14499034 0.08501615 0.342471   0.00445632 1.         0.07810935\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13259695]\n",
            "iteration: 113\n",
            "Done: [False]\n",
            "Action taken: [0.54154456 0.55549216]\n",
            "Observation: [[0.17352958 0.08557319 0.342471   0.00445632 1.         0.18226412\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13140638]\n",
            "iteration: 114\n",
            "Done: [False]\n",
            "Action taken: [ 0.47937357 -0.6025325 ]\n",
            "Observation: [[0.20206884 0.08613022 0.342471   0.00445632 1.         0.06928928\n",
            "  0.         1.        ]]\n",
            "Reward: [0.13016076]\n",
            "iteration: 115\n",
            "Done: [False]\n",
            "Action taken: [ 0.37987697 -0.6220259 ]\n",
            "Observation: [[0.23060809 0.08668727 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12886696]\n",
            "iteration: 116\n",
            "Done: [False]\n",
            "Action taken: [ 0.4966229  -0.84115094]\n",
            "Observation: [[0.25914735 0.0872443  0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12753165]\n",
            "iteration: 117\n",
            "Done: [False]\n",
            "Action taken: [0.72948426 0.5632943 ]\n",
            "Observation: [[0.2876866  0.08780134 0.342471   0.00445632 1.         0.10561768\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12616138]\n",
            "iteration: 118\n",
            "Done: [False]\n",
            "Action taken: [0.937945  0.7550574]\n",
            "Observation: [[0.31622583 0.08835838 0.342471   0.00445632 1.         0.24719094\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12476237]\n",
            "iteration: 119\n",
            "Done: [False]\n",
            "Action taken: [0.62634486 0.19800833]\n",
            "Observation: [[0.3447651  0.08891542 0.342471   0.00445632 1.         0.2843175\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12334055]\n",
            "iteration: 120\n",
            "Done: [False]\n",
            "Action taken: [ 0.14667258 -0.50825596]\n",
            "Observation: [[0.37330434 0.08947247 0.342471   0.00445632 1.         0.18901952\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12190144]\n",
            "iteration: 121\n",
            "Done: [False]\n",
            "Action taken: [ 0.3493     -0.43563098]\n",
            "Observation: [[0.40184358 0.0900295  0.342471   0.00445632 1.         0.1073387\n",
            "  0.         1.        ]]\n",
            "Reward: [0.12045026]\n",
            "iteration: 122\n",
            "Done: [False]\n",
            "Action taken: [ 0.630551   -0.84795535]\n",
            "Observation: [[0.43038285 0.09058654 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11899178]\n",
            "iteration: 123\n",
            "Done: [False]\n",
            "Action taken: [ 0.7056098  -0.09518971]\n",
            "Observation: [[0.4589221  0.09114358 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11753038]\n",
            "iteration: 124\n",
            "Done: [False]\n",
            "Action taken: [0.37549713 0.6049651 ]\n",
            "Observation: [[0.48746136 0.09170062 0.342471   0.00445632 1.         0.11343095\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11607007]\n",
            "iteration: 125\n",
            "Done: [False]\n",
            "Action taken: [ 0.57513547 -0.03232199]\n",
            "Observation: [[0.51600057 0.09225766 0.342471   0.00445632 1.         0.10737059\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11461443]\n",
            "iteration: 126\n",
            "Done: [False]\n",
            "Action taken: [ 0.0449508 -0.882991 ]\n",
            "Observation: [[0.54453987 0.0928147  0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.1131667]\n",
            "iteration: 127\n",
            "Done: [False]\n",
            "Action taken: [ 0.3987336  -0.00065894]\n",
            "Observation: [[0.5730791  0.09337174 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11172973]\n",
            "iteration: 128\n",
            "Done: [False]\n",
            "Action taken: [ 0.05104256 -0.93882924]\n",
            "Observation: [[0.60161835 0.09392878 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.11030607]\n",
            "iteration: 129\n",
            "Done: [False]\n",
            "Action taken: [ 0.6097565  -0.34039217]\n",
            "Observation: [[0.6301576  0.09448582 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10889789]\n",
            "iteration: 130\n",
            "Done: [False]\n",
            "Action taken: [ 0.9890706  -0.14221513]\n",
            "Observation: [[0.65869683 0.09504285 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10750712]\n",
            "iteration: 131\n",
            "Done: [False]\n",
            "Action taken: [0.06500573 0.34490168]\n",
            "Observation: [[0.6872361  0.0955999  0.342471   0.00445632 1.         0.06466907\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10613538]\n",
            "iteration: 132\n",
            "Done: [False]\n",
            "Action taken: [ 0.13670796 -0.20879461]\n",
            "Observation: [[0.7157754  0.09615693 0.342471   0.00445632 1.         0.02552008\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10478406]\n",
            "iteration: 133\n",
            "Done: [False]\n",
            "Action taken: [0.4688371  0.16061465]\n",
            "Observation: [[0.7443146  0.09671398 0.342471   0.00445632 1.         0.05563533\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10345431]\n",
            "iteration: 134\n",
            "Done: [False]\n",
            "Action taken: [ 0.4466304  -0.01392984]\n",
            "Observation: [[0.77285385 0.09727102 0.342471   0.00445632 1.         0.05302348\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10214705]\n",
            "iteration: 135\n",
            "Done: [False]\n",
            "Action taken: [0.7231036 0.2806242]\n",
            "Observation: [[0.8013931  0.09782805 0.342471   0.00445632 1.         0.10564052\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10086305]\n",
            "iteration: 136\n",
            "Done: [False]\n",
            "Action taken: [ 0.703583   -0.58577883]\n",
            "Observation: [[0.82993233 0.0983851  0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09960289]\n",
            "iteration: 137\n",
            "Done: [False]\n",
            "Action taken: [ 0.64129907 -0.8986186 ]\n",
            "Observation: [[0.85847163 0.09894213 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09836701]\n",
            "iteration: 138\n",
            "Done: [False]\n",
            "Action taken: [ 0.56523913 -0.31535763]\n",
            "Observation: [[0.8870109  0.09949917 0.342471   0.00445632 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09715568]\n",
            "iteration: 139\n",
            "Done: [False]\n",
            "Action taken: [0.8182671 0.8272576]\n",
            "Observation: [[0.9155501  0.10005621 0.342471   0.00445632 1.         0.15511079\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0959691]\n",
            "iteration: 140\n",
            "Done: [False]\n",
            "Action taken: [ 0.7654553  -0.67547625]\n",
            "Observation: [[0.94408935 0.10061325 0.342471   0.00445632 1.         0.028459\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09480734]\n",
            "iteration: 141\n",
            "Done: [False]\n",
            "Action taken: [ 0.24994391 -0.82397664]\n",
            "Observation: [[ 0.9691667   0.10117029 -0.52728564 -0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11704776]\n",
            "iteration: 142\n",
            "Done: [False]\n",
            "Action taken: [0.31588566 0.6526132 ]\n",
            "Observation: [[ 0.9252262   0.08237588 -0.52728564 -0.15035532  1.          0.12236498\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11558158]\n",
            "iteration: 143\n",
            "Done: [False]\n",
            "Action taken: [0.33454925 0.7003006 ]\n",
            "Observation: [[ 0.8812857   0.06358146 -0.52728564 -0.15035532  1.          0.25367135\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11411472]\n",
            "iteration: 144\n",
            "Done: [False]\n",
            "Action taken: [ 0.14817685 -0.5964034 ]\n",
            "Observation: [[ 0.83734524  0.04478705 -0.52728564 -0.15035532  1.          0.1418457\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11265088]\n",
            "iteration: 145\n",
            "Done: [False]\n",
            "Action taken: [ 0.36907014 -0.17944737]\n",
            "Observation: [[ 0.7934048   0.02599263 -0.52728564 -0.15035532  1.          0.10819931\n",
            "   0.          1.        ]]\n",
            "Reward: [0.11119342]\n",
            "iteration: 146\n",
            "Done: [False]\n",
            "Action taken: [ 0.47624    -0.32074252]\n",
            "Observation: [[ 0.74946433  0.015      -0.52728564  0.15035532  1.          0.04806009\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10974533]\n",
            "iteration: 147\n",
            "Done: [False]\n",
            "Action taken: [ 0.94328564 -0.7362986 ]\n",
            "Observation: [[ 0.70552385  0.03379441 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10830926]\n",
            "iteration: 148\n",
            "Done: [False]\n",
            "Action taken: [ 0.37581414 -0.8269802 ]\n",
            "Observation: [[ 0.6615834   0.05258883 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10688756]\n",
            "iteration: 149\n",
            "Done: [False]\n",
            "Action taken: [0.77892125 0.64746517]\n",
            "Observation: [[ 0.61764294  0.07138325 -0.52728564  0.15035532  1.          0.12139972\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10548227]\n",
            "iteration: 150\n",
            "Done: [False]\n",
            "Action taken: [ 0.9344325  -0.12751573]\n",
            "Observation: [[ 0.57370245  0.09017766 -0.52728564  0.15035532  1.          0.09749052\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10409515]\n",
            "iteration: 151\n",
            "Done: [False]\n",
            "Action taken: [ 0.5089339  -0.30147663]\n",
            "Observation: [[ 0.52976197  0.10897207 -0.52728564  0.15035532  1.          0.04096365\n",
            "   0.          1.        ]]\n",
            "Reward: [0.1027277]\n",
            "iteration: 152\n",
            "Done: [False]\n",
            "Action taken: [ 0.44208252 -0.74863964]\n",
            "Observation: [[ 0.48582152  0.12776649 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10138121]\n",
            "iteration: 153\n",
            "Done: [False]\n",
            "Action taken: [0.45197558 0.5381721 ]\n",
            "Observation: [[ 0.44188106  0.1465609  -0.52728564  0.15035532  1.          0.10090727\n",
            "   0.          1.        ]]\n",
            "Reward: [0.10005669]\n",
            "iteration: 154\n",
            "Done: [False]\n",
            "Action taken: [ 0.93643725 -0.7563355 ]\n",
            "Observation: [[ 0.3979406   0.16535532 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09875505]\n",
            "iteration: 155\n",
            "Done: [False]\n",
            "Action taken: [0.6598461  0.73130226]\n",
            "Observation: [[ 0.35400012  0.18414973 -0.52728564  0.15035532  1.          0.13711917\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09747691]\n",
            "iteration: 156\n",
            "Done: [False]\n",
            "Action taken: [ 0.34340286 -0.7917721 ]\n",
            "Observation: [[ 0.31005967  0.20294414 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09622284]\n",
            "iteration: 157\n",
            "Done: [False]\n",
            "Action taken: [0.3594725 0.8086955]\n",
            "Observation: [[ 0.26611918  0.22173856 -0.52728564  0.15035532  1.          0.1516304\n",
            "   0.          1.        ]]\n",
            "Reward: [0.0949932]\n",
            "iteration: 158\n",
            "Done: [False]\n",
            "Action taken: [0.22813867 0.20721862]\n",
            "Observation: [[ 0.22217873  0.24053298 -0.52728564  0.15035532  1.          0.1904839\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09378822]\n",
            "iteration: 159\n",
            "Done: [False]\n",
            "Action taken: [ 0.05733369 -0.82359284]\n",
            "Observation: [[ 0.17823826  0.25932738 -0.52728564  0.15035532  1.          0.03606024\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09260806]\n",
            "iteration: 160\n",
            "Done: [False]\n",
            "Action taken: [ 0.99973255 -0.7458543 ]\n",
            "Observation: [[ 0.13429779  0.2781218  -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09145276]\n",
            "iteration: 161\n",
            "Done: [False]\n",
            "Action taken: [ 0.01842115 -0.6856899 ]\n",
            "Observation: [[ 0.09035733  0.29691622 -0.52728564  0.15035532  1.          0.\n",
            "   0.          1.        ]]\n",
            "Reward: [0.09032226]\n",
            "iteration: 162\n",
            "Done: [False]\n",
            "Action taken: [0.5962932  0.20651232]\n",
            "Observation: [[ 0.04641686  0.31571063 -0.52728564  0.15035532  1.          0.03872106\n",
            "   0.          1.        ]]\n",
            "Reward: [0.08921644]\n",
            "iteration: 163\n",
            "Done: [False]\n",
            "Action taken: [0.34178182 0.5995142 ]\n",
            "Observation: [[0.03083333 0.33450505 0.3692696  0.04895095 1.         0.15112998\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10573699]\n",
            "iteration: 164\n",
            "Done: [False]\n",
            "Action taken: [0.39882705 0.6713878 ]\n",
            "Observation: [[0.0616058  0.34062392 0.3692696  0.04895095 1.         0.27701518\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10442107]\n",
            "iteration: 165\n",
            "Done: [False]\n",
            "Action taken: [ 0.5987323  -0.90815896]\n",
            "Observation: [[0.09237827 0.34674278 0.3692696  0.04895095 1.         0.10673538\n",
            "  0.         1.        ]]\n",
            "Reward: [0.1031143]\n",
            "iteration: 166\n",
            "Done: [False]\n",
            "Action taken: [0.7333611  0.30729073]\n",
            "Observation: [[0.12315074 0.35286167 0.3692696  0.04895095 1.         0.16435239\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10181874]\n",
            "iteration: 167\n",
            "Done: [False]\n",
            "Action taken: [ 0.58868855 -0.90943515]\n",
            "Observation: [[0.15392321 0.35898054 0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.10053621]\n",
            "iteration: 168\n",
            "Done: [False]\n",
            "Action taken: [ 0.3842785 -0.9277315]\n",
            "Observation: [[0.18469568 0.3650994  0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09926832]\n",
            "iteration: 169\n",
            "Done: [False]\n",
            "Action taken: [ 0.07097389 -0.74343526]\n",
            "Observation: [[0.21546814 0.37121826 0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09801644]\n",
            "iteration: 170\n",
            "Done: [False]\n",
            "Action taken: [ 0.70259964 -0.6234976 ]\n",
            "Observation: [[0.24624062 0.37733713 0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09678178]\n",
            "iteration: 171\n",
            "Done: [False]\n",
            "Action taken: [ 0.71069944 -0.36682233]\n",
            "Observation: [[0.2770131  0.383456   0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09556533]\n",
            "iteration: 172\n",
            "Done: [False]\n",
            "Action taken: [ 0.15882653 -0.54867667]\n",
            "Observation: [[0.30778554 0.3895749  0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09436794]\n",
            "iteration: 173\n",
            "Done: [False]\n",
            "Action taken: [0.9597224  0.19080456]\n",
            "Observation: [[0.33855802 0.39569375 0.3692696  0.04895095 1.         0.03577586\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09319031]\n",
            "iteration: 174\n",
            "Done: [False]\n",
            "Action taken: [ 0.10123028 -0.3463571 ]\n",
            "Observation: [[0.3693305  0.4018126  0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.092033]\n",
            "iteration: 175\n",
            "Done: [False]\n",
            "Action taken: [ 0.6634254 -0.555822 ]\n",
            "Observation: [[0.40010294 0.40793148 0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.09089642]\n",
            "iteration: 176\n",
            "Done: [False]\n",
            "Action taken: [0.5667827  0.68744385]\n",
            "Observation: [[0.43087542 0.41405034 0.3692696  0.04895095 1.         0.12889573\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0897809]\n",
            "iteration: 177\n",
            "Done: [False]\n",
            "Action taken: [0.67962885 0.37136   ]\n",
            "Observation: [[0.4616479  0.4201692  0.3692696  0.04895095 1.         0.19852573\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08868667]\n",
            "iteration: 178\n",
            "Done: [False]\n",
            "Action taken: [ 0.01041575 -0.83898157]\n",
            "Observation: [[0.49242038 0.4262881  0.3692696  0.04895095 1.         0.04121668\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08761385]\n",
            "iteration: 179\n",
            "Done: [False]\n",
            "Action taken: [0.8259942  0.26530817]\n",
            "Observation: [[0.5231928  0.43240696 0.3692696  0.04895095 1.         0.09096196\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08656248]\n",
            "iteration: 180\n",
            "Done: [False]\n",
            "Action taken: [ 0.7452676  -0.03983827]\n",
            "Observation: [[0.5539653  0.43852583 0.3692696  0.04895095 1.         0.08349229\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08553255]\n",
            "iteration: 181\n",
            "Done: [False]\n",
            "Action taken: [0.17466748 0.35683197]\n",
            "Observation: [[0.5847378  0.4446447  0.3692696  0.04895095 1.         0.15039828\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08452399]\n",
            "iteration: 182\n",
            "Done: [False]\n",
            "Action taken: [0.8001384  0.43087178]\n",
            "Observation: [[0.6155102  0.45076355 0.3692696  0.04895095 1.         0.23118673\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08353664]\n",
            "iteration: 183\n",
            "Done: [False]\n",
            "Action taken: [ 0.80669934 -0.4986499 ]\n",
            "Observation: [[0.64628273 0.45688242 0.3692696  0.04895095 1.         0.13768989\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08257034]\n",
            "iteration: 184\n",
            "Done: [False]\n",
            "Action taken: [ 0.7321432  -0.91743845]\n",
            "Observation: [[0.6770552  0.4630013  0.3692696  0.04895095 1.         0.\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08162485]\n",
            "iteration: 185\n",
            "Done: [False]\n",
            "Action taken: [0.43582013 0.34209615]\n",
            "Observation: [[0.7078276  0.46912017 0.3692696  0.04895095 1.         0.06414303\n",
            "  0.         1.        ]]\n",
            "Reward: [0.08069994]\n",
            "iteration: 186\n",
            "Done: [False]\n",
            "Action taken: [0.5891961  0.68584657]\n",
            "Observation: [[0.73860013 0.47523904 0.3692696  0.04895095 1.         0.19273926\n",
            "  0.         1.        ]]\n",
            "Reward: [0.0797953]\n",
            "iteration: 187\n",
            "Done: [False]\n",
            "Action taken: [0.8603528 0.7292916]\n",
            "Observation: [[0.7693726  0.4813579  0.3692696  0.04895095 1.         0.32948142\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07891063]\n",
            "iteration: 188\n",
            "Done: [False]\n",
            "Action taken: [ 0.39162832 -0.81886   ]\n",
            "Observation: [[0.80014503 0.48747677 0.3692696  0.04895095 1.         0.1759452\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07804559]\n",
            "iteration: 189\n",
            "Done: [False]\n",
            "Action taken: [ 0.7125356 -0.6134665]\n",
            "Observation: [[0.83091754 0.49359563 0.3692696  0.04895095 1.         0.06092022\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07719985]\n",
            "iteration: 190\n",
            "Done: [False]\n",
            "Action taken: [0.11838544 0.29822633]\n",
            "Observation: [[0.86169    0.49971452 0.3692696  0.04895095 1.         0.11683766\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07637305]\n",
            "iteration: 191\n",
            "Done: [False]\n",
            "Action taken: [0.65962034 0.40539345]\n",
            "Observation: [[0.89246243 0.5058334  0.3692696  0.04895095 1.         0.19284892\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07556481]\n",
            "iteration: 192\n",
            "Done: [False]\n",
            "Action taken: [ 0.8191788  -0.89833033]\n",
            "Observation: [[0.92323494 0.5119523  0.3692696  0.04895095 1.         0.02441199\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07477474]\n",
            "iteration: 193\n",
            "Done: [False]\n",
            "Action taken: [0.6866768 0.5409377]\n",
            "Observation: [[0.9540074  0.5180711  0.3692696  0.04895095 1.         0.12583782\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07400248]\n",
            "iteration: 194\n",
            "Done: [False]\n",
            "Action taken: [0.16803144 0.51361066]\n",
            "Observation: [[0.98477983 0.52419    0.3692696  0.04895095 1.         0.2221398\n",
            "  0.         1.        ]]\n",
            "Reward: [0.07324763]\n",
            "iteration: 195\n",
            "Done: [False]\n",
            "Action taken: [ 0.7232285 -0.5253704]\n",
            "Observation: [[1.0155523  0.53030884 0.3692696  0.04895095 1.         0.12363286\n",
            "  0.         1.        ]]\n",
            "Reward: [-10.]\n",
            "iteration: 196\n",
            "Done: [False]\n",
            "Action taken: [0.47206846 0.52479476]\n",
            "Observation: [[0.9691667 0.55625   0.25      0.25      1.        0.44375   1.\n",
            "  1.       ]]\n",
            "Reward: [0.00250654]\n",
            "iteration: 197\n",
            "Done: [ True]\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R172XbXX5Am5",
        "outputId": "df9d5637-9c12-4932-bdc1-fcb367fa004a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([0.9691667, 0.55625  , 0.25     , 0.25     , 1.       , 0.44375  ,\n",
            "       1.       , 1.       ], dtype=float32), {})\n",
            "Action taken: [ 0.9963445  -0.11009466]\n",
            "Observation: [ 0.9691667   0.55625    -0.25        0.          1.          0.42310724\n",
            "  0.          1.        ]\n",
            "Reward: 2.01\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.41906592 -0.79476184]\n",
            "Observation: [ 0.9483333  0.55625   -0.25       0.         1.         0.2740894\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [0.9478894  0.87098277]\n",
            "Observation: [ 0.9275      0.55625    -0.25        0.          1.          0.43739867\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [0.93524736 0.6148688 ]\n",
            "Observation: [ 0.9066667  0.55625   -0.25       0.         1.         0.5526866\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [ 0.4522378 -0.5251924]\n",
            "Observation: [ 0.8858333  0.55625   -0.25       0.         1.         0.454213\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [0.03803585 0.8478745 ]\n",
            "Observation: [ 0.865       0.55625    -0.25        0.          1.          0.61318946\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [0.42516726 0.6377705 ]\n",
            "Observation: [ 0.8441667   0.55625    -0.25        0.          1.          0.73277146\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [ 0.4881051  -0.11752756]\n",
            "Observation: [ 0.8233333  0.55625   -0.25       0.         1.         0.710735\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.31020772 -0.33025002]\n",
            "Observation: [ 0.8025     0.55625   -0.25       0.         1.         0.6488131\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [ 0.7774263  -0.10741559]\n",
            "Observation: [ 0.7816667  0.55625   -0.25       0.         1.         0.6286727\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [ 0.53772575 -0.87638557]\n",
            "Observation: [ 0.7608333   0.55625    -0.25        0.          1.          0.46435043\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.08503608 -0.7140222 ]\n",
            "Observation: [ 0.74        0.55625    -0.25        0.          1.          0.33047128\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [ 0.7387859  -0.48030448]\n",
            "Observation: [ 0.7191667   0.55625    -0.25        0.          1.          0.24041417\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [ 0.6035119  -0.52194566]\n",
            "Observation: [ 0.6983333   0.55625    -0.25        0.          1.          0.14254937\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [0.31587932 0.17943428]\n",
            "Observation: [ 0.6775     0.55625   -0.25       0.         1.         0.1761933\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [ 0.1019162 -0.9242682]\n",
            "Observation: [ 0.6566667   0.55625    -0.25        0.          1.          0.00289301\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.36609307 -0.43404204]\n",
            "Observation: [ 0.6358333  0.55625   -0.25       0.         1.         0.\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [ 0.5347734  -0.38692835]\n",
            "Observation: [ 0.615    0.55625 -0.25     0.       1.       0.       0.       1.     ]\n",
            "Reward: 0.01\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [0.91601294 0.15265109]\n",
            "Observation: [ 0.5941667   0.55625    -0.25        0.          1.          0.02862208\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [0.398003   0.07964855]\n",
            "Observation: [ 0.5733333   0.55625    -0.25        0.          1.          0.04355618\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [0.39916646 0.1292825 ]\n",
            "Observation: [ 0.5525      0.55625    -0.25        0.          1.          0.06779665\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [0.7093783  0.25447145]\n",
            "Observation: [ 0.5316667   0.55625    -0.25        0.          1.          0.11551005\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [0.14736481 0.68784934]\n",
            "Observation: [ 0.5108333  0.55625   -0.25       0.         1.         0.2444818\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [0.23216777 0.83784705]\n",
            "Observation: [ 0.49        0.55625    -0.25        0.          1.          0.40157813\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [0.24508575 0.45326853]\n",
            "Observation: [ 0.46916667  0.55625    -0.25        0.          1.          0.48656598\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.6082919 -0.7518596]\n",
            "Observation: [ 0.44833332  0.55625    -0.25        0.          1.          0.3455923\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [ 0.32505852 -0.38478336]\n",
            "Observation: [ 0.4275      0.55625    -0.25        0.          1.          0.27344543\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [0.6040326  0.60282147]\n",
            "Observation: [ 0.40666667  0.55625    -0.25        0.          1.          0.38647443\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [0.98602897 0.17018402]\n",
            "Observation: [ 0.38583332  0.55625    -0.25        0.          1.          0.41838396\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [ 0.63128126 -0.5204453 ]\n",
            "Observation: [ 0.365       0.55625    -0.25        0.          1.          0.32080045\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [ 0.28990972 -0.5961987 ]\n",
            "Observation: [ 0.34416667  0.55625    -0.25        0.          1.          0.20901321\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [ 0.44520256 -0.8035426 ]\n",
            "Observation: [ 0.32333332  0.55625    -0.25        0.          1.          0.05834896\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [0.0721717  0.16888203]\n",
            "Observation: [ 0.3025      0.55625    -0.25        0.          1.          0.09001434\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [0.45970505 0.57884866]\n",
            "Observation: [ 0.28166667  0.55625    -0.25        0.          1.          0.19854847\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [0.07472283 0.72632   ]\n",
            "Observation: [ 0.26083332  0.55625    -0.25        0.          1.          0.3347335\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [0.10324377 0.44873205]\n",
            "Observation: [ 0.24        0.55625    -0.25        0.          1.          0.41887072\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [0.41043875 0.23036084]\n",
            "Observation: [ 0.21916667  0.55625    -0.25        0.          1.          0.46206337\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [ 0.6343284  -0.90636045]\n",
            "Observation: [ 0.19833334  0.55625    -0.25        0.          1.          0.2921208\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [0.4116054  0.68321633]\n",
            "Observation: [ 0.1775      0.55625    -0.25        0.          1.          0.42022386\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.6908958 -0.6039947]\n",
            "Observation: [ 0.15666667  0.55625    -0.25        0.          1.          0.30697486\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [ 0.2680218  -0.60129976]\n",
            "Observation: [ 0.13583334  0.55625    -0.25        0.          1.          0.19423115\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [0.34244713 0.7959932 ]\n",
            "Observation: [ 0.115       0.55625    -0.25        0.          1.          0.34347987\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [0.01061163 0.8057704 ]\n",
            "Observation: [ 0.09416667  0.55625    -0.25        0.          1.          0.49456182\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [0.5313101 0.7677056]\n",
            "Observation: [ 0.07333333  0.55625    -0.25        0.          1.          0.63850665\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [ 0.49532962 -0.02822956]\n",
            "Observation: [ 0.0525     0.55625   -0.25       0.         1.         0.6332136\n",
            "  0.         1.       ]\n",
            "Reward: 0.01\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [ 0.7130848  -0.56966096]\n",
            "Observation: [ 0.03166667  0.55625    -0.25        0.          1.          0.5264022\n",
            "  0.          1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [ 0.39415446 -0.2545316 ]\n",
            "Observation: [0.03083333 0.55625    0.30158833 0.08185805 1.         0.47867748\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [ 0.31554475 -0.4851148 ]\n",
            "Observation: [0.05596569 0.56648225 0.30158833 0.08185805 1.         0.38771847\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.79618365 -0.8987087 ]\n",
            "Observation: [0.08109805 0.5767145  0.30158833 0.08185805 1.         0.21921058\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [0.3837831  0.48649183]\n",
            "Observation: [0.10623041 0.5869468  0.30158833 0.08185805 1.         0.31042778\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [0.3254838  0.59345925]\n",
            "Observation: [0.13136277 0.597179   0.30158833 0.08185805 1.         0.4217014\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [0.503824  0.3171709]\n",
            "Observation: [0.15649512 0.60741127 0.30158833 0.08185805 1.         0.48117095\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [0.0511052 0.6269883]\n",
            "Observation: [0.1816275  0.61764354 0.30158833 0.08185805 1.         0.5987312\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [0.14672232 0.6065616 ]\n",
            "Observation: [0.20675986 0.6278758  0.30158833 0.08185805 1.         0.71246153\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.70171237 -0.15253532]\n",
            "Observation: [0.23189221 0.638108   0.30158833 0.08185805 1.         0.6838612\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [0.3414206  0.14297664]\n",
            "Observation: [0.25702456 0.6483403  0.30158833 0.08185805 1.         0.7106693\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [0.59261584 0.9951077 ]\n",
            "Observation: [0.2821569  0.65857255 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [0.9902432  0.00300835]\n",
            "Observation: [0.30728927 0.6688048  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [ 0.45307377 -0.7779095 ]\n",
            "Observation: [0.33242166 0.6790371  0.30158833 0.08185805 1.         0.629142\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [0.00548865 0.60628283]\n",
            "Observation: [0.35755402 0.6892693  0.30158833 0.08185805 1.         0.74282\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [0.9139218 0.7221263]\n",
            "Observation: [0.38268638 0.6995016  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [0.91367465 0.2923604 ]\n",
            "Observation: [0.40781873 0.70973384 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [0.07837347 0.47194695]\n",
            "Observation: [0.4329511  0.7199661  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [0.40832415 0.6957447 ]\n",
            "Observation: [0.45808345 0.7301983  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [0.9809106  0.74865687]\n",
            "Observation: [0.4832158  0.7404306  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [0.02487038 0.00705958]\n",
            "Observation: [0.50834817 0.75066286 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [ 0.641034   -0.59737563]\n",
            "Observation: [0.5334805  0.76089513 0.30158833 0.08185805 1.         0.66299206\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [0.37973177 0.59209466]\n",
            "Observation: [0.5586129  0.77112734 0.30158833 0.08185805 1.         0.7740098\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [ 0.5926745 -0.8222752]\n",
            "Observation: [0.58374524 0.7813596  0.30158833 0.08185805 1.         0.61983323\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [0.44425917 0.92654103]\n",
            "Observation: [0.6088776  0.7915919  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [0.47490937 0.3081336 ]\n",
            "Observation: [0.63400996 0.80182415 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [0.10382925 0.697541  ]\n",
            "Observation: [0.6591423  0.81205636 0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 71\n",
            "Done: False\n",
            "Action taken: [ 0.7169733 -0.8100465]\n",
            "Observation: [0.6842747  0.82228863 0.30158833 0.08185805 1.         0.62311625\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 72\n",
            "Done: False\n",
            "Action taken: [0.7920861  0.38169476]\n",
            "Observation: [0.70940703 0.8325209  0.30158833 0.08185805 1.         0.694684\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 73\n",
            "Done: False\n",
            "Action taken: [ 0.5698501  -0.07655038]\n",
            "Observation: [0.7345394  0.8427532  0.30158833 0.08185805 1.         0.6803309\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 74\n",
            "Done: False\n",
            "Action taken: [ 0.8767663  -0.11470309]\n",
            "Observation: [0.75967175 0.8529854  0.30158833 0.08185805 1.         0.658824\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 75\n",
            "Done: False\n",
            "Action taken: [0.47087753 0.31492713]\n",
            "Observation: [0.7848041  0.86321765 0.30158833 0.08185805 1.         0.71787286\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 76\n",
            "Done: False\n",
            "Action taken: [0.97458285 0.8020344 ]\n",
            "Observation: [0.80993646 0.8734499  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 77\n",
            "Done: False\n",
            "Action taken: [ 0.56678224 -0.10397571]\n",
            "Observation: [0.8350688  0.8836822  0.30158833 0.08185805 1.         0.75550455\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 78\n",
            "Done: False\n",
            "Action taken: [0.10787839 0.2550911 ]\n",
            "Observation: [0.8602012  0.8939144  0.30158833 0.08185805 1.         0.775\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 79\n",
            "Done: False\n",
            "Action taken: [ 0.9372104  -0.29584152]\n",
            "Observation: [0.88533354 0.9041467  0.30158833 0.08185805 1.         0.7195297\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 80\n",
            "Done: False\n",
            "Action taken: [ 0.38820815 -0.0216769 ]\n",
            "Observation: [0.9104659  0.91437894 0.30158833 0.08185805 1.         0.7154653\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 81\n",
            "Done: False\n",
            "Action taken: [ 0.9353664 -0.7766226]\n",
            "Observation: [0.93559825 0.9246112  0.30158833 0.08185805 1.         0.56984854\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 82\n",
            "Done: False\n",
            "Action taken: [ 0.29275155 -0.21960439]\n",
            "Observation: [0.9607306  0.9348435  0.30158833 0.08185805 1.         0.52867275\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 83\n",
            "Done: False\n",
            "Action taken: [0.09222496 0.6252975 ]\n",
            "Observation: [0.985863   0.9450757  0.30158833 0.08185805 1.         0.64591604\n",
            " 0.         1.        ]\n",
            "Reward: 0.02\n",
            "iteration: 84\n",
            "Done: False\n",
            "Action taken: [ 0.7645796 -0.9610336]\n",
            "Observation: [1.0109954  0.95530796 0.30158833 0.08185805 1.         0.46572223\n",
            " 0.         1.        ]\n",
            "Reward: -123.7367789238741\n",
            "iteration: 85\n",
            "Done: False\n",
            "Action taken: [0.85728043 0.9293642 ]\n",
            "Observation: [1.0109954  0.95530796 0.30158833 0.08185805 1.         0.46572223\n",
            " 0.         1.        ]\n",
            "Reward: 0.01\n",
            "iteration: 86\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_sjyFuJcGT-w",
        "outputId": "21e6fe8c-b715-4aa9-9b27-3e86f3e7f80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Eval num_timesteps=8000, episode_reward=79.05 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 79.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward 79.05\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 102      |\n",
            "|    ep_rew_mean     | -5.34    |\n",
            "| time/              |          |\n",
            "|    fps             | 1123     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=112.64 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 113         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020809008 |\n",
            "|    clip_fraction        | 0.0928      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.97       |\n",
            "|    explained_variance   | -0.00189    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.41        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.00172     |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 1.76        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward 112.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 105      |\n",
            "|    ep_rew_mean     | 6.39     |\n",
            "| time/              |          |\n",
            "|    fps             | 803      |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 20       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=113.17 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 113         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011888538 |\n",
            "|    clip_fraction        | 0.0757      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.13       |\n",
            "|    explained_variance   | 0.393       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.812       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | 0.000316    |\n",
            "|    std                  | 1.19        |\n",
            "|    value_loss           | 1.87        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward 113.17\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 105      |\n",
            "|    ep_rew_mean     | 10.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 729      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=145.18 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 145         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012414405 |\n",
            "|    clip_fraction        | 0.0779      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.27       |\n",
            "|    explained_variance   | 0.424       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.466       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.000322   |\n",
            "|    std                  | 1.27        |\n",
            "|    value_loss           | 1.78        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 8000: mean reward 145.18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 107      |\n",
            "|    ep_rew_mean     | 25.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 697      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=149.30 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 149         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014540492 |\n",
            "|    clip_fraction        | 0.0835      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.41       |\n",
            "|    explained_variance   | 0.48        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.49        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -5.09e-05   |\n",
            "|    std                  | 1.37        |\n",
            "|    value_loss           | 1.68        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 10000: mean reward 149.30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | 56.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 680      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 60       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=149.79 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011924944 |\n",
            "|    clip_fraction        | 0.0742      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.56       |\n",
            "|    explained_variance   | 0.493       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.524       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | 0.00176     |\n",
            "|    std                  | 1.47        |\n",
            "|    value_loss           | 1.34        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 12000: mean reward 149.79\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | 56.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 669      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=149.76 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017536905 |\n",
            "|    clip_fraction        | 0.0923      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.72       |\n",
            "|    explained_variance   | 0.538       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.281       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | 0.000929    |\n",
            "|    std                  | 1.6         |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward 149.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | 51.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 662      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 86       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=150.03 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 150          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 64000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0152664175 |\n",
            "|    clip_fraction        | 0.0962       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.87        |\n",
            "|    explained_variance   | 0.547        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.248        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.000505    |\n",
            "|    std                  | 1.71         |\n",
            "|    value_loss           | 1.18         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 16000: mean reward 150.03\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | 87.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 656      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 99       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=150.08 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 72000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009988172 |\n",
            "|    clip_fraction        | 0.0712      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4          |\n",
            "|    explained_variance   | 0.616       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.101       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | 0.00109     |\n",
            "|    std                  | 1.82        |\n",
            "|    value_loss           | 0.76        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 150.08\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | 132      |\n",
            "| time/              |          |\n",
            "|    fps             | 651      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 113      |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=150.35 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012270648 |\n",
            "|    clip_fraction        | 0.0832      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.13       |\n",
            "|    explained_variance   | 0.679       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.256      |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.000237   |\n",
            "|    std                  | 1.94        |\n",
            "|    value_loss           | 0.571       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 20000: mean reward 150.35\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 121      |\n",
            "|    ep_rew_mean     | 142      |\n",
            "| time/              |          |\n",
            "|    fps             | 648      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 126      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=150.30 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 88000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013779258 |\n",
            "|    clip_fraction        | 0.0779      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.25       |\n",
            "|    explained_variance   | 0.658       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.036       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | 0.000292    |\n",
            "|    std                  | 2.07        |\n",
            "|    value_loss           | 0.687       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 22000: mean reward 150.30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | 161      |\n",
            "| time/              |          |\n",
            "|    fps             | 645      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 139      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=150.30 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009358646 |\n",
            "|    clip_fraction        | 0.0759      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.35       |\n",
            "|    explained_variance   | 0.698       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.106      |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00243    |\n",
            "|    std                  | 2.17        |\n",
            "|    value_loss           | 0.709       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 24000: mean reward 150.30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | 211      |\n",
            "| time/              |          |\n",
            "|    fps             | 642      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 152      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=150.37 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 150          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0144842565 |\n",
            "|    clip_fraction        | 0.0892       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.48        |\n",
            "|    explained_variance   | 0.727        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.0384      |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    std                  | 2.33         |\n",
            "|    value_loss           | 0.559        |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 26000: mean reward 150.37\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 124      |\n",
            "|    ep_rew_mean     | 220      |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABh4klEQVR4nO3deVxU9f4/8NcMyzBsAyiyKALuiGuuuJukolEuaRqZaVe9ltdM07SuW5r+NG/lUpLV1TK1zMzS703DfUkRRcwtVxRcEBVhWASGmc/vD5wjI6AzCpxheD0fj3nAnHPmzHuGWV6c8z6foxBCCBARERHZKKXcBRARERGVJ4YdIiIismkMO0RERGTTGHaIiIjIpjHsEBERkU1j2CEiIiKbxrBDRERENo1hh4iIiGwaww4RERHZNIYdqvRmzZoFhUIhdxlWbdWqVVAoFLh8+bIs9//6668jKChIlvsmICsrC//4xz/g6+sLhUKBCRMmyF1SpfOk76G4uDh06NABLi4uUCgUSEhIKJf66NEYdkpgfFErFArs37+/2HwhBAICAqBQKPD888/LUKH5goKCpMeiUCjg4uKCtm3b4rvvvpO7tCqpW7duJn+PopdGjRrJXd5TuX79OmbNmmXzH+Z//vknZs2ahfT0dLlLMdu8efOwatUqjB07FqtXr8awYcOean0GgwELFy5EcHAwnJyc0KxZM6xbt86s2964cQNTp05F9+7d4ebmBoVCgd27dz9VPdZKp9Nh0KBBSEtLw6efforVq1cjMDBQ7rLMEhcXh3HjxiE0NBQuLi6oXbs2Bg8ejHPnzpW4/JkzZ9C7d2+4urrCy8sLw4YNw61bt4otZ8lrx9x1msP+iW5VRTg5OWHt2rXo1KmTyfQ9e/bg6tWrUKlUMlVmmRYtWmDSpEkACj9ovv76awwfPhx5eXkYNWqUzNVVPbVq1cL8+fOLTddoNDJUU3auX7+O2bNnIygoCC1atDCZ99VXX8FgMMhTWBn7888/MXv2bLz++uvw8PCQuxyz7Ny5E+3bt8fMmTPLZH0ffPAB/t//+38YNWoU2rRpg19//RWvvPIKFAoFhgwZ8sjbnj17FgsWLED9+vXRtGlTHDx4sExqskYXL17ElStX8NVXX+Ef//iH3OVYZMGCBThw4AAGDRqEZs2aISUlBcuWLcMzzzyDQ4cOoUmTJtKyV69eRZcuXaDRaDBv3jxkZWVh0aJFOHHiBA4fPgxHR0dpWXNfO5as0yyCilm5cqUAIAYMGCCqV68udDqdyfxRo0aJVq1aicDAQNG3b1+ZqjRPSTWmpqYKV1dXERISIlNVltHpdCIvL6/U+TNnzhTW8lLW6/Xi3r17pc7v2rWrCA0NrcCKChlf04mJieV2H3FxcQKAWLlyZbndhzX4+OOPy/25LGvBwcFl9ll19epV4eDgIN566y1pmsFgEJ07dxa1atUSBQUFj7y9VqsVd+7cEUII8dNPPwkAYteuXWVSW3l6kvfQnj17BADx008/PXbZrKysp6iu7B04cKDY5+65c+eESqUSUVFRJtPHjh0r1Gq1uHLlijQtJiZGABBffvmlNM2S14656zQXd2M9wtChQ3Hnzh3ExMRI0/Lz87Fhwwa88sorJd7GYDDgs88+Q2hoKJycnODj44MxY8bg7t27Jsv9+uuv6Nu3L/z9/aFSqVC3bl3MmTMHer3eZLlu3bqhSZMmOH36NLp37w5nZ2fUrFkTCxcufOLH5e3tjUaNGuHixYsW1z5x4kRUq1YNQghp2r/+9S8oFAosWbJEmnbz5k0oFAosX74cQOHzNmPGDLRq1QoajQYuLi7o3Lkzdu3aZVLD5cuXoVAosGjRInz22WeoW7cuVCoVTp8+DQDYv38/2rRpAycnJ9StWxdffvml2Y/b+FwePXoUHTp0gFqtRnBwMKKjo4stm5eXh5kzZ6JevXpQqVQICAjAlClTkJeXZ7KcQqHAuHHjsGbNGoSGhkKlUmHr1q1m11SSDRs2QKFQYM+ePcXmffnll1AoFDh58iQA4K+//sLrr7+OOnXqwMnJCb6+vhg5ciTu3Lnz2PtRKBSYNWtWselBQUF4/fXXpetpaWl499130bRpU7i6usLd3R0RERE4fvy4tMzu3bvRpk0bAMCIESOkXXOrVq0CUHLPTnZ2NiZNmoSAgACoVCo0bNgQixYtMnltGescN24cNm3ahCZNmkClUiE0NNTs59nSv+Wj7mfWrFmYPHkyACA4OFh6nJb0cfz9998YPHgwvL29oVar0bBhQ3zwwQcmyxw7dgwRERFwd3eHq6srevTogUOHDhVbV3p6OiZMmCA9h/Xq1cOCBQukrWi7d++GQqFAYmIi/u///u+J6n3Yr7/+Cp1OhzfffFOaplAoMHbsWFy9evWxW2rc3Nzg5eX1xPcPALGxsejduzc0Gg2cnZ3RtWtXHDhwQJpfUe+hR3n99dfRtWtXAMCgQYOgUCjQrVs3aZ6rqysuXryIPn36wM3NDVFRUQDM/w4RQmDu3LmoVasWnJ2d0b17d5w6darY+/dJdejQodjWk/r16yM0NBRnzpwxmf7zzz/j+eefR+3ataVp4eHhaNCgAdavXy9Ns+S1Y+46zcXdWI8QFBSEsLAwrFu3DhEREQCA33//HRkZGRgyZIjJl7vRmDFjsGrVKowYMQLjx49HYmIili1bhmPHjuHAgQNwcHAAUNgX5OrqiokTJ8LV1RU7d+7EjBkzoNVq8fHHH5us8+7du+jduzcGDBiAwYMHY8OGDXjvvffQtGlTqS5LFBQU4OrVq/D09LS49s6dO+PTTz/FqVOnpM2Y+/btg1KpxL59+zB+/HhpGgB06dIFAKDVavH1119j6NChGDVqFDIzM/HNN9+gV69eOHz4cLHdHitXrkRubi5Gjx4NlUoFLy8vnDhxAj179oS3tzdmzZqFgoICzJw5Ez4+PmY/9rt376JPnz4YPHgwhg4divXr12Ps2LFwdHTEyJEjARR+2LzwwgvYv38/Ro8ejZCQEJw4cQKffvopzp07h02bNpmsc+fOnVi/fj3GjRuH6tWrP7YRV6/X4/bt28Wmq9VquLi4oG/fvnB1dcX69eulD0ujH3/8EaGhodJzHxMTg0uXLmHEiBHw9fXFqVOnsGLFCpw6dQqHDh0qk8btS5cuYdOmTRg0aBCCg4Nx8+ZNfPnll+jatStOnz4Nf39/hISE4MMPP8SMGTMwevRodO7cGUDhB2ZJhBB44YUXsGvXLrzxxhto0aIFtm3bhsmTJ+PatWv49NNPTZbfv38/Nm7ciDfffBNubm5YsmQJBg4ciKSkJFSrVq3U2i39Wz7ufgYMGIBz585h3bp1+PTTT1G9enUAhf9AmOOvv/5C586d4eDggNGjRyMoKAgXL17E5s2b8dFHHwEATp06hc6dO8Pd3R1TpkyBg4MDvvzyS3Tr1g179uxBu3btAAA5OTno2rUrrl27hjFjxqB27dr4888/MW3aNNy4cQOfffYZQkJCsHr1arzzzjuoVauWtDvbWG9Jr8OSuLm5Sbvtjx07BhcXF4SEhJgs07ZtW2n+w7v+y9LOnTsRERGBVq1aYebMmVAqlVi5ciWeffZZ7Nu3D23btrWK99CYMWNQs2ZNzJs3D+PHj0ebNm1MPqsKCgrQq1cvdOrUCYsWLYKzs7N0O3O+Q2bMmIG5c+eiT58+6NOnD+Lj49GzZ0/k5+eb1GEwGJCWlmZWzRqNRlp/SYQQuHnzJkJDQ6Vp165dQ2pqKlq3bl1s+bZt2+J///ufdN3c144l6zSbxduCqgDj5sq4uDixbNky4ebmJnJycoQQQgwaNEh0795dCFF8F9G+ffsEALFmzRqT9W3durXYdOP6ihozZoxwdnYWubm50rSuXbsKAOK7776TpuXl5QlfX18xcODAxz6WwMBA0bNnT3Hr1i1x69YtceLECTFs2DABwGRTorm1p6amCgDiiy++EEIIkZ6eLpRKpRg0aJDw8fGRbjd+/Hjh5eUlDAaDEEKIgoKCYptE7969K3x8fMTIkSOlaYmJiQKAcHd3F6mpqSbL9+vXTzg5OZls1jx9+rSws7MzazeW8bn8z3/+I03Ly8sTLVq0EDVq1BD5+flCCCFWr14tlEql2Ldvn8nto6OjBQBx4MABaRoAoVQqxalTpx57/0VrKOkyZswYabmhQ4eKGjVqmGzWvXHjhlAqleLDDz+UppX0Olq3bp0AIPbu3StNK2kTPAAxc+bMYrcPDAwUw4cPl67n5uYKvV5vskxiYqJQqVQmtTxqN9bw4cNFYGCgdH3Tpk0CgJg7d67Jci+99JJQKBTiwoULJnU6OjqaTDt+/LgAIJYuXVrsvoqy9G9pzv08zW6sLl26CDc3N5PXsBBCep8IUfg6d3R0FBcvXpSmXb9+Xbi5uYkuXbpI0+bMmSNcXFzEuXPnTNY1depUYWdnJ5KSkqRppe1yL+21+PCl6N+0b9++ok6dOsXWlZ2dLQCIqVOnmv18WLoby2AwiPr164tevXqZPGc5OTkiODhYPPfcc9K0ingPPc6uXbtK3I01fPjwEp8rSz6HHR0dRd++fU2eh/fff18AMHn/Gj9Tzbk87u+wevVqAUB888030jTj+77od5TR5MmTBQDpO83c144l6zQXd2M9xuDBg3Hv3j1s2bIFmZmZ2LJlS6m7sH766SdoNBo899xzuH37tnRp1aoVXF1dTXbZqNVq6ffMzEzcvn0bnTt3Rk5ODv7++2+T9bq6uuLVV1+Vrjs6OqJt27a4dOmSWY/hjz/+gLe3N7y9vdG0aVOsXr0aI0aMMNmCZG7txl1ge/fuBQAcOHAAdnZ2mDx5Mm7evInz588DKNyy06lTJ+m/Ijs7O2mTqPE/jYKCArRu3Rrx8fHFah44cKDJf8t6vR7btm1Dv379TDZrhoSEoFevXmY9DwBgb2+PMWPGSNcdHR0xZswYpKam4ujRo9JzERISgkaNGpk8F88++ywAFNv11rVrVzRu3NjsGoKCghATE1PsUvRw4JdffhmpqakmR6ls2LABBoMBL7/8sjSt6OsoNzcXt2/fRvv27QGgxOf1SahUKiiVhR8Ver0ed+7cgaurKxo2bPjE9/G///0PdnZ20pZAo0mTJkEIgd9//91kenh4OOrWrStdb9asGdzd3R/7HrD0b/mk92OOW7duYe/evRg5cqTJaxiA9D7R6/X4448/0K9fP9SpU0ea7+fnh1deeQX79++HVquVHlvnzp3h6elp8tjCw8Oh1+ul9+ijlPQ6LOlS9D127969Eg/OcHJykuaXl4SEBJw/fx6vvPIK7ty5Iz3m7Oxs9OjRA3v37pV24VnTe6g0Y8eONblu7ufw9u3bkZ+fL7UQGJU0pICvr6/Zf+fmzZuXWuvff/+Nt956C2FhYRg+fLg03fj3Nuc1Ye5rx5J1mou7sR7D29sb4eHhWLt2LXJycqDX6/HSSy+VuOz58+eRkZGBGjVqlDg/NTVV+v3UqVP497//jZ07d0ofXkYZGRkm12vVqlVsU6qnpyf++usvsx5Du3btMHfuXOj1epw8eRJz587F3bt3TfbHWlJ7586dpc2I+/btQ+vWrdG6dWt4eXlh37598PHxwfHjx4uFwm+//Rb/+c9/8Pfff0On00nTg4ODi93fw9Nu3bqFe/fuoX79+sWWbdiwodmbNf39/eHi4mIyrUGDBgAK+4Xat2+P8+fP48yZM6Xumij6XJRW/6O4uLggPDz8kcsY+xF+/PFH9OjRA0Dh5vcWLVpI9QKF/TSzZ8/GDz/8UKyuh19HT8pgMGDx4sX44osvkJiYaNJX9qhdSI9y5coV+Pv7w83NzWS6cfP2lStXTKY/HA6AwvfAw30MD7P0b/mk92MOY2AqehTLw27duoWcnBw0bNiw2LyQkBAYDAYkJycjNDQU58+fx19//WX2YyvJ416HJVGr1cX6nYDCoGCcX16M/0wV/bJ9WEZGBjw9Pa3qPVQSe3t71KpVy2SauZ/DxvfHw5+H3t7exdoTnJycnujvXFRKSgr69u0LjUaDDRs2wM7OTppn/Hub85ow97VjyTrNxbBjhldeeQWjRo1CSkoKIiIiSj3c1GAwoEaNGlizZk2J840fSunp6ejatSvc3d3x4Ycfom7dunByckJ8fDzee++9YofoFn1hFSUeauQsTfXq1aUXe69evdCoUSM8//zzWLx4MSZOnGhR7QDQqVMnfPXVV7h06RL27duHzp07Q6FQoFOnTti3bx/8/f1hMBikvg0A+P777/H666+jX79+mDx5MmrUqAE7OzvMnz+/WKM0UL4fmI9jMBjQtGlTfPLJJyXODwgIMLleHrWqVCr069cPv/zyC7744gvcvHkTBw4cwLx580yWGzx4MP78809MnjwZLVq0gKurKwwGA3r37v3Eh3o/3CQ/b948TJ8+HSNHjsScOXPg5eUFpVKJCRMmVNjh5E/6HrD0b/m077WKZDAY8Nxzz2HKlCklzi/6hV6alJQUs+5Lo9FIr3M/Pz/s2rULQgiTf8Ju3LgBoPAfivJifL19/PHHxfr8jFxdXQHI+x4yR9EtpkaWfA6bS6/Xmz02jZeXV7Gm5IyMDERERCA9PV36fC/Kz88PwIO/f1E3btyAl5eXtIXG3NeOJes0F8OOGfr3748xY8bg0KFD+PHHH0tdrm7duti+fTs6duz4yC/A3bt3486dO9i4caPUwAsAiYmJZVp3afr27YuuXbti3rx5GDNmDFxcXMyuHYAUYmJiYhAXF4epU6cCKGxGXr58ubT1pFWrVtJtNmzYgDp16mDjxo0mL3Jzx/0wHrli/M+uqLNnz5q1DqBwLJjs7GyTrTvGQbKMjcV169bF8ePH0aNHD1lHZn755Zfx7bffYseOHThz5gyEECab3+/evYsdO3Zg9uzZmDFjhjS9pOeoJJ6ensUGxsvPzy/2AbNhwwZ0794d33zzjcn09PR0qUEXgEXPVWBgILZv347MzEyTrTvGXbhlNfBaefwtn3Q9xt1SxqOASuLt7Q1nZ+cSX9N///03lEqlFNDq1q2LrKysp/qv3fil8jgrV66UjvBp0aIFvv76a5w5c8Zk921sbKw0v7wYdzG6u7ub9bjL+z1U1sz9HDa+P86fP2+yu/PWrVvFtkImJyebvfV5165d0hFjQOFWlMjISJw7dw7bt28vcXd9zZo14e3tjSNHjhSb9/DBJ+a+dixZp7nYs2MGV1dXLF++HLNmzUJkZGSpyw0ePBh6vR5z5swpNq+goED6YjH+91j0v8X8/Hx88cUXZVv4I7z33nu4c+cOvvrqKwDm1w4U7rapWbMmPv30U+h0OnTs2BFAYQi6ePEiNmzYgPbt28Pe/kGWLukxx8bGmj2gmJ2dHXr16oVNmzYhKSlJmn7mzBls27bN7MddUFBgcrh6fn4+vvzyS3h7e0vhbPDgwbh27Zr03BR17949ZGdnm31/TyM8PBxeXl748ccf8eOPP6Jt27YmH1olPacA8Nlnn5m1/rp16xbr61ixYkWxLTt2dnbF7uOnn37CtWvXTKYZA6Q5Iwv36dMHer0ey5YtM5n+6aefQqFQPNFRhiUpj7+lJY+zKG9vb3Tp0gX//e9/TV7DwIO/oZ2dHXr27Ilff/3V5PDwmzdvSgOcuru7Ayh8bAcPHizx9Z+eno6CgoLH1vQkPTsvvvgiHBwcTD6vhBCIjo5GzZo1TY7Au3HjRrHd1k+jVatWqFu3LhYtWoSsrKxi8x/eglHe76GyZu7ncHh4OBwcHLB06VKT2kuq+0l7dvR6PV5++WUcPHgQP/30E8LCwkqte+DAgdiyZQuSk5OlaTt27MC5c+cwaNAgaZolrx1z12kubtkx06P2ERt17doVY8aMwfz585GQkICePXvCwcEB58+fx08//YTFixfjpZdeQocOHeDp6Ynhw4dj/PjxUCgUWL16dYVuKo+IiECTJk3wySef4K233jK7dqPOnTvjhx9+QNOmTaV9xM888wxcXFxw7ty5Yv06zz//PDZu3Ij+/fujb9++SExMRHR0NBo3blzih1ZJZs+eja1bt6Jz58548803UVBQgKVLlyI0NNTs/iV/f38sWLAAly9fRoMGDfDjjz8iISEBK1askA65HDZsGNavX49//vOf2LVrFzp27Ai9Xo+///4b69evx7Zt20o8JNJcGRkZ+P7770ucV7QR3cHBAQMGDMAPP/yA7OxsLFq0yGRZd3d3dOnSBQsXLoROp0PNmjXxxx9/mL2F8B//+Af++c9/YuDAgXjuuedw/PhxbNu2zWRrDVD4t/vwww8xYsQIdOjQASdOnMCaNWtM/qMECsOTh4cHoqOj4ebmBhcXF7Rr167E/yojIyPRvXt3fPDBB7h8+TKaN2+OP/74A7/++ismTJhg0iT8NMrjb2kMxR988AGGDBkCBwcHREZGFusFK8mSJUvQqVMnPPPMMxg9ejSCg4Nx+fJl/N///Z90mo25c+ciJiYGnTp1wptvvgl7e3t8+eWXyMvLMxlfa/Lkyfjtt9/w/PPP4/XXX0erVq2QnZ2NEydOYMOGDbh8+XKxv+XDnmSrUK1atTBhwgR8/PHH0Ol0aNOmDTZt2oR9+/ZhzZo1JrsCp02bhm+//RaJiYkmQzLMnTsXQGHvIgCsXr1aOjXPv//971LvW6lU4uuvv0ZERARCQ0MxYsQI1KxZE9euXcOuXbvg7u6OzZs3S8uX93uorJn7Oezt7Y13330X8+fPx/PPP48+ffrg2LFj+P3334v9zZ+0Z2fSpEn47bffEBkZibS0tGKfWUU/q95//3389NNP6N69O95++21kZWXh448/RtOmTTFixAhpOUteO+au02wWHbtVRRQ99PxRSjucc8WKFaJVq1ZCrVYLNzc30bRpUzFlyhRx/fp1aZkDBw6I9u3bC7VaLfz9/cWUKVPEtm3bih3+V9qIuw8fymtpjUIIsWrVqmKHlZpTuxBCfP755wKAGDt2rMn08PBwAUDs2LHDZLrBYBDz5s0TgYGBQqVSiZYtW4otW7YUexzGwyQ//vjjEmves2ePaNWqlXB0dBR16tQR0dHRZo+gbHwujxw5IsLCwoSTk5MIDAwUy5YtK7Zsfn6+WLBggQgNDRUqlUp4enqKVq1aidmzZ4uMjAxpOTx0CL85NeARh34+zDhiqEKhEMnJycXmX716VfTv3194eHgIjUYjBg0aJK5fv17ssPKSDpvV6/XivffeE9WrVxfOzs6iV69e4sKFCyUeej5p0iTh5+cn1Gq16Nixozh48KDo2rWr6Nq1q0k9v/76q2jcuLGwt7c3eW2V9HrNzMwU77zzjvD39xcODg6ifv364uOPPzY5lFaI0p/jh+sszdP+LUu6nzlz5oiaNWsKpVJp8eHIJ0+elP5mTk5OomHDhmL69Okmy8THx4tevXoJV1dX4ezsLLp37y7+/PPPYuvKzMwU06ZNE/Xq1ROOjo6ievXqokOHDmLRokXSUArGx1CWo73r9Xrp/ezo6ChCQ0PF999/X2w54yHWDz8/lrwHSnLs2DExYMAAUa1aNaFSqURgYKAYPHhwsc8dIcr3PfQ4jzr03MXFpdTbmfM5rNfrxezZs6X3Zbdu3cTJkyfNfl88jqWfVSdPnhQ9e/YUzs7OwsPDQ0RFRYmUlJRiy5n72rFkneZQCGGFnXdE5aBbt264ffv2I3smiIgqs6CgIHTr1k0avZwKsWeHiIiIbBp7doiInlJGRsZjBznz9fWtoGqovGVlZT2219Db27vUoQyo4jHsEBE9pbfffhvffvvtI5dhx4DtWLRoEWbPnv3IZR5uyiZ5sWeHiOgpnT59GtevX3/kMk87ii1Zj0uXLj32FCKdOnWSTm1A8mPYISIiIpvGBmUiIiKyaezZQeH5SK5fvw43NzdZTw9ARERE5hNCIDMzE/7+/sXONVYUww4Kz5f08AkBiYiIqHJITk4udhb5ohh2AOlEhMnJydJ5Z4iIiMi6abVaBAQEmJxQuCQMO3hwFmN3d3eGHSIiokrmcS0obFAmIiIim8awQ0RERDaNYYeIiIhsGsMOERER2TSGHSIiIrJpDDtERERk0xh2iIiIyKYx7BAREZFNY9ghIiIim8awQ0RERDaNYYeIiIhsmqxhZ+/evYiMjIS/vz8UCgU2bdpkMv/111+HQqEwufTu3dtkmbS0NERFRcHd3R0eHh544403kJWVVYGPgoiIiKyZrCcCzc7ORvPmzTFy5EgMGDCgxGV69+6NlStXStdVKpXJ/KioKNy4cQMxMTHQ6XQYMWIERo8ejbVr15Zr7UREVP70BoG8Aj0UUEChQOEFCigVhSd/NP6k8ieEgEEABiFgEAJC+r3wpzA8mKd/eL5BwNtNBScHO1lqlzXsREREICIi4pHLqFQq+Pr6ljjvzJkz2Lp1K+Li4tC6dWsAwNKlS9GnTx8sWrQI/v7+ZV4zEVFVpTcI5Or0yNXpkVdguP+7AbkF96fpDA/N0yO3yHJ5Bfd/6vT3b1PC8kWXK9BDpxdm1VYYggClwhiKFCbXlfevKx4KSUoFACk8mQYpY7hSFlkXHl4XLAtaFZXLDEJAbyghkBQJKwZRGGD0hkfPLzrvaax+oy061/cumwdoIVnDjjl2796NGjVqwNPTE88++yzmzp2LatWqAQAOHjwIDw8PKegAQHh4OJRKJWJjY9G/f3+5yiYiG2cwiAdf0kW+uIsGgDzpi7vo/MJ5+QUG6YsFePClImD8wgGAB18+QgAC95cv8nvx6QIGwyPWAzx0v0XXI6DTi4eCx4NwYm7wkIPJ81A4Rc5yqjxjSDTZAmdhMCxLVh12evfujQEDBiA4OBgXL17E+++/j4iICBw8eBB2dnZISUlBjRo1TG5jb28PLy8vpKSklLrevLw85OXlSde1Wm25PQYiW1OgNyAztwDaXB209wqQmauTfr+n00v/YeOh/36L/ueMYv+FQ9pNgaLTjbsuAJP/1hUm/6WXMv3+fUIBky/wvCJbIopvWTCGlBLmFd0SoTMgX2+Q4+m3Go52SqgclHBysIOTgxJO9nZwcrCDyv7BNJWD3f3pSqju/5SWvz9PVWzeQ8vb28HRXgmFAiZbGlAkvElh7aHrD6bfD4FFby9dNw17oui0ktZtECbh0hJPEr8s3ZoiIEy2ZhkDh1JZ5PcS5iuKzJPmKxWwK22+Eo9dnzWx6rAzZMgQ6femTZuiWbNmqFu3Lnbv3o0ePXo88Xrnz5+P2bNnl0WJRJVOfoHhfjjRmYQW4zRt7v3p93TQSj8fTMvO18v9EKyOg53i/he38Yva9ItbCgJFvsQd7ZWwKxLqlPdTmkkIROEXDmC6O8X4e+F043/NRXbLlLQeRckhs+h6FAAc7JRS8CjtcTjaK2GntK4vM6JHseqw87A6deqgevXquHDhAnr06AFfX1+kpqaaLFNQUIC0tLRS+3wAYNq0aZg4caJ0XavVIiAgoNzqJiprGTk63MrKKxJQioeSkqfpkKsrmy0Szo52cHOyh7uTA9zVDnB3sofasbD5sOhuFONuk5J2pRT+LHl3ivSP80P/lRe9LYr9913yOot+gRf94lY9tCWicGvDg+WM81RFtkSUFAT4xU9k3SpV2Ll69Sru3LkDPz8/AEBYWBjS09Nx9OhRtGrVCgCwc+dOGAwGtGvXrtT1qFSqYkd1EVUGl25l4bPt57H5r+tP3SzoprKHu9qhSGB5EFwePa3wdwc7DtNFRJWDrGEnKysLFy5ckK4nJiYiISEBXl5e8PLywuzZszFw4ED4+vri4sWLmDJlCurVq4devXoBAEJCQtC7d2+MGjUK0dHR0Ol0GDduHIYMGcIjscimJKflYOnO8/g5/hr0hQ0H0KgLw4ibyjSUuDvdDyb3t7YUnaa5/7urkz23RhBRlaEQ4mn/P3xyu3fvRvfu3YtNHz58OJYvX45+/frh2LFjSE9Ph7+/P3r27Ik5c+bAx8dHWjYtLQ3jxo3D5s2boVQqMXDgQCxZsgSurq5m16HVaqHRaJCRkQF3d/cyeWxEZeGmNhfLdl7AD3FJ0pEw4SE18M5zDRDqr5G5OiIieZn7/S1r2LEWDDtkbe5k5SF6z0V8d/AK8goKe2w61auOiT0b4JnanjJXR0RkHcz9/q5UPTtEti7jng5f7b2E/x5IRM79o55aB3piUs+GCKtbTebqiIgqJ4YdIiuQlVeAVQcSsWLvJWhzCwAATWtqMKlnA3Rt4G11Y1YQEVUmDDtEMsrV6fH9oSv4YvdFpGXnAwAa+rhhYs8G6NnYhyGHiKgMMOwQySC/wIAf45KwdOcFpGYWjuYdXN0FE8Lr4/lm/jxSioioDDHsEFWgAr0BG49dw+Lt53Et/R4AoKaHGm/3qI8Bz9SEPceuISIqcww7RBXAYBDY/Nd1fLb9PBJvZwMAarip8K9n62FwmwCo7O1krpCIyHYx7BCVIyEE/jh9E5/8cQ5nb2YCALxcHDG2a1282j5QOr0CERGVH4YdonIghMDe87fxnz/O4q+rGQAANyd7jO5cByM6BcNVxbceEVFF4ScuURk7dOkO/vPHWcRdvgug8ISZIzsGY1TnOtA4O8hcHRFR1cOwQ1RGjiXdxScx57Dv/G0AgKO9Eq+1D8TYbnVRzZUnniUikgvDDtFTOn1di09izmL7mVQAgIOdAi+3CcC47vXhq3GSuToiImLYIXpCF1Kz8On2c/i/v24AAJQKYOAztTC+R30EeDnLXB0RERkx7BBZKOlODj7bcQ6bjl2D4f5pdCOb+2NCeH3U9XaVtzgiIiqGYYfITDcy7mHpzgtYH5eMgvsp57nGPpj4XAOE+JV+tl0iIpIXww7RY9zKzMPy3RfxfewV5BcYAABdGnhj0nMN0DzAQ97iiIjosRh2iB5h9cHLmPe/v3FPpwcAtA3ywqSeDdCuTjWZKyMiInMx7BCVIj0nHzN/OwWDAJoHeODdng3QqV51nomciKiSYdghKsXhxDQYBFCnugs2vdmBIYeIqJLiKZaJSnHoUhoAoH3dagw6RESVGMMOUSkOXboDAGjP/hwiokqNYYeoBBk5OpxJ0QIA2gd7yVwNERE9DYYdohIcvpwGIYA63i6o4c5TPhARVWYMO0Ql4C4sIiLbwbBDVAJj2GnHXVhERJUeww7RQzJydDh9436/DrfsEBFVegw7RA+JM/brVHeBD/t1iIgqPYYdoodIu7C4VYeIyCYw7BA95FCisTmZ/TpERLaAYYeoiIx7Opy6zn4dIiJbwrBDVERcYmG/TjD7dYiIbAbDDlERsdyFRURkcxh2iIqQTv7JXVhERDaDYYfovsJ+nQwAQLtghh0iIlvBsEN035HLaTAIIKiaM3w17NchIrIVDDtE9/F8WEREtolhh+i+2ET26xAR2SJZw87evXsRGRkJf39/KBQKbNq0qdRl//nPf0KhUOCzzz4zmZ6WloaoqCi4u7vDw8MDb7zxBrKyssq3cLI52lwdTl6736/DI7GIiGyKrGEnOzsbzZs3x+eff/7I5X755RccOnQI/v7+xeZFRUXh1KlTiImJwZYtW7B3716MHj26vEomG1W0X8dPo5a7HCIiKkP2ct55REQEIiIiHrnMtWvX8K9//Qvbtm1D3759TeadOXMGW7duRVxcHFq3bg0AWLp0Kfr06YNFixaVGI6ISmI85JxHYRER2R6r7tkxGAwYNmwYJk+ejNDQ0GLzDx48CA8PDynoAEB4eDiUSiViY2MrslSq5GKNzcl1uQuLiMjWyLpl53EWLFgAe3t7jB8/vsT5KSkpqFGjhsk0e3t7eHl5ISUlpdT15uXlIS8vT7qu1WrLpmCqlDJzdThxjePrEBHZKqvdsnP06FEsXrwYq1atgkKhKNN1z58/HxqNRroEBASU6fqpcjly+S4MAgis5gx/D/brEBHZGqsNO/v27UNqaipq164Ne3t72Nvb48qVK5g0aRKCgoIAAL6+vkhNTTW5XUFBAdLS0uDr61vquqdNm4aMjAzpkpycXJ4PhaycNL4Ot+oQEdkkq92NNWzYMISHh5tM69WrF4YNG4YRI0YAAMLCwpCeno6jR4+iVatWAICdO3fCYDCgXbt2pa5bpVJBpVKVX/FUqRjDDg85JyKyTbKGnaysLFy4cEG6npiYiISEBHh5eaF27dqoVs30P20HBwf4+vqiYcOGAICQkBD07t0bo0aNQnR0NHQ6HcaNG4chQ4bwSCwyS2auDievF/ZsteNggkRENknW3VhHjhxBy5Yt0bJlSwDAxIkT0bJlS8yYMcPsdaxZswaNGjVCjx490KdPH3Tq1AkrVqwor5LJxhy5chd6g0BtL2fUZL8OEZFNknXLTrdu3SCEMHv5y5cvF5vm5eWFtWvXlmFVVJU8OB8Wd2EREdkqq21QJqoIxsEEeT4sIiLbxbBDVVZWXkGR82Ex7BAR2SqGHaqyjlxOg94gEOClZr8OEZENY9ihKkvahcXxdYiIbBrDDlVZD5qTGXaIiGwZww5VSVl5BQ/Oh8UjsYiIbBrDDlVJxn6dWp5q1PJ0lrscIiIqRww7VCXFJvKQcyKiqoJhh6ok9usQEVUdDDtU5WTnFeCvq/f7dYLZr0NEZOsYdqjKMZ4Pq5anGgFe7NchIrJ1DDtU5Rh3YbXj+DpERFUCww5VObE8+ScRUZXCsENVStF+HTYnExFVDQw7VKUcvXIXBQaBmh7s1yEiqioYdqhK4SHnRERVD8MOVSnGwQR5iggioqqDYYeqjJz8AhxPTgcAhHHLDhFRlcGwQ1VG0X6dWp5qucshIqIKwrBDVYY0vk4dLygUCpmrISKiisKwQ1XGoUs8+ScRUVXEsENVQk5+Af66mg4AaM+Rk4mIqhSGHaoS4q+kQ6cX8Nc4IcCL/TpERFUJww5VCUXH12G/DhFR1cKwQ1UCBxMkIqq6GHbI5t3L1+O4sV+HYYeIqMph2CGbF590Fzq9gB/7dYiIqiSGHbJ57NchIqraGHbI5j0IOzwfFhFRVcSwQzbtXr4eCffPh8V+HSKiqolhh2xa0X6d2l7OcpdDREQyYNghmxZrPB9WMM+HRURUVTHskE3j+bCIiIhhh2wW+3WIiAhg2CEbdizpLvL1Bvi6OyGwGvt1iIiqKoYdsllFDzlnvw4RUdXFsEM261BiYb9OO+7CIiKq0mQNO3v37kVkZCT8/f2hUCiwadMmk/mzZs1Co0aN4OLiAk9PT4SHhyM2NtZkmbS0NERFRcHd3R0eHh544403kJWVVYGPgqxRrk6PhKR0AOzXISKq6mQNO9nZ2WjevDk+//zzEuc3aNAAy5Ytw4kTJ7B//34EBQWhZ8+euHXrlrRMVFQUTp06hZiYGGzZsgV79+7F6NGjK+ohkJWKv9+v4+OuQhD7dYiIqjSFEELIXQQAKBQK/PLLL+jXr1+py2i1Wmg0Gmzfvh09evTAmTNn0LhxY8TFxaF169YAgK1bt6JPnz64evUq/P39zbpv43ozMjLg7u5eFg+HZPZJzDks2XEeL7bwx+IhLeUuh4iIyoG539+VpmcnPz8fK1asgEajQfPmzQEABw8ehIeHhxR0ACA8PBxKpbLY7q6i8vLyoNVqTS5kW2KLnPyTiIiqNqsPO1u2bIGrqyucnJzw6aefIiYmBtWrVwcApKSkoEaNGibL29vbw8vLCykpKaWuc/78+dBoNNIlICCgXB8DVaxcnR7H7o+v0y6YJ/8kIqrqrD7sdO/eHQkJCfjzzz/Ru3dvDB48GKmpqU+1zmnTpiEjI0O6JCcnl1G1ZA2OJaUjv8CAGm4qBFd3kbscIiKSmdWHHRcXF9SrVw/t27fHN998A3t7e3zzzTcAAF9f32LBp6CgAGlpafD19S11nSqVCu7u7iYXsh2HiuzC4vg6RERk9WHnYQaDAXl5eQCAsLAwpKen4+jRo9L8nTt3wmAwoF27dnKVSDI7xH4dIiIqwl7OO8/KysKFCxek64mJiUhISICXlxeqVauGjz76CC+88AL8/Pxw+/ZtfP7557h27RoGDRoEAAgJCUHv3r0xatQoREdHQ6fTYdy4cRgyZIjZR2KRbSnar9O+Dvt1iIhI5rBz5MgRdO/eXbo+ceJEAMDw4cMRHR2Nv//+G99++y1u376NatWqoU2bNti3bx9CQ0Ol26xZswbjxo1Djx49oFQqMXDgQCxZsqTCHwtZh4Tkwn4db/brEBHRfbKGnW7duuFRw/xs3Ljxsevw8vLC2rVry7IsqsTYr0NERA+rdD07RI9S9OSfREREAMMO2ZBcnR7xPB8WERE9hGGHbMbxIv06ddivQ0RE9zHskM04dCkNQOGoyezXISIiI4YdshkcX4eIiErCsEM2obBf5y4Ahh0iIjLFsEM24XhyOvIKDKjuqkJdb/brEBHRAww7ZBNiEwv7ddrXYb8OERGZYtghm2Ds12nHXVhERPQQhh2q9PIK9Dh6pbBfJ4yDCRIR0UMYdqjSO56ccb9fxxF1vV3lLoeIiKwMww5VekV3YbFfh4iIHsawQ5VebCLH1yEiotIx7FClVrRfp30w+3WIiKg4hh2q1P66moFcnQHVXBxRrwb7dYiIqDiGHarUDl18sAuL/TpERFQShh2q1IoOJkhERFQShh2qtPILDDhyxRh22JxMREQlY9ihSuuvq+ns1yEiosdi2KFK68H4OjwfFhERlc7enIVatmxp9pdJfHz8UxVEZK5Dl7gLi4iIHs+ssNOvXz/p99zcXHzxxRdo3LgxwsLCAACHDh3CqVOn8Oabb5ZLkUQPyy8wPBhfh2GHiIgewaywM3PmTOn3f/zjHxg/fjzmzJlTbJnk5OSyrY6oFCeupeOeTg8vF0fUZ78OERE9gsU9Oz/99BNee+21YtNfffVV/Pzzz2VSFNHjGHdhtQtmvw4RET2axWFHrVbjwIEDxaYfOHAATk5OZVIU0eMYm5O5C4uIiB7HrN1YRU2YMAFjx45FfHw82rZtCwCIjY3Ff//7X0yfPr3MCyR6mE5vwJHL7NchIiLzWBx2pk6dijp16mDx4sX4/vvvAQAhISFYuXIlBg8eXOYFEj3sr6sZ7NchIiKzWRR2CgoKMG/ePIwcOZLBhmQjja8T7AWlkv06RET0aBb17Njb22PhwoUoKCgor3qIHqto2CEiInocixuUe/TogT179pRHLUSPZdKvU5f9OkRE9HgW9+xERERg6tSpOHHiBFq1agUXFxeT+S+88EKZFUf0MGO/jqezAxrUcJO7HCIiqgQsDjvGUZI/+eSTYvMUCgX0ev3TV0VUithE4y6sauzXISIis1gcdgwGQ3nUQWSWB+fDYr8OERGZh2c9p0qjsF/n/sjJHF+HiIjMZPGWHQDIzs7Gnj17kJSUhPz8fJN548ePL5PCiB524loGcvL18HB2QEMf9usQEZF5LA47x44dQ58+fZCTk4Ps7Gx4eXnh9u3bcHZ2Ro0aNRh2qNxwfB0iInoSFu/GeueddxAZGYm7d+9CrVbj0KFDuHLlClq1aoVFixZZtK69e/ciMjIS/v7+UCgU2LRpkzRPp9PhvffeQ9OmTeHi4gJ/f3+89tpruH79usk60tLSEBUVBXd3d3h4eOCNN95AVlaWpQ+LKoFYqV+Hu7CIiMh8FoedhIQETJo0CUqlEnZ2dsjLy0NAQAAWLlyI999/36J1ZWdno3nz5vj888+LzcvJyUF8fDymT5+O+Ph4bNy4EWfPni12aHtUVBROnTqFmJgYbNmyBXv37sXo0aMtfVhk5Yr26zDsEBGRJSzejeXg4AClsjAj1ahRA0lJSQgJCYFGo0FycrJF64qIiEBERESJ8zQaDWJiYkymLVu2DG3btkVSUhJq166NM2fOYOvWrYiLi0Pr1q0BAEuXLkWfPn2waNEi+Pv7W/rwyEqdvJaBbPbrEBHRE7A47LRs2RJxcXGoX78+unbtihkzZuD27dtYvXo1mjRpUh41SjIyMqBQKODh4QEAOHjwIDw8PKSgAwDh4eFQKpWIjY1F//79S1xPXl4e8vLypOtarbZc66anZzzkvG0Q+3WIiMgyFu/GmjdvHvz8/AAAH330ETw9PTF27FjcunULK1asKPMCjXJzc/Hee+9h6NChcHd3BwCkpKSgRo0aJsvZ29vDy8sLKSkppa5r/vz50Gg00iUgIKDc6qayYRxMkLuwiIjIUhZv2Sm6FaVGjRrYunVrmRZUEp1Oh8GDB0MIgeXLlz/1+qZNm4aJEydK17VaLQOPFSvQGxCXyH4dIiJ6MhZv2fnvf/+LxMTE8qilRMagc+XKFcTExEhbdQDA19cXqampJssXFBQgLS0Nvr6+pa5TpVLB3d3d5ELW6+R1LbLz9dCoHdDIl/06RERkGYvDzvz581GvXj3Url0bw4YNw9dff40LFy6UR21S0Dl//jy2b9+OatVM/6sPCwtDeno6jh49Kk3buXMnDAYD2rVrVy41UcUzjq/TluPrEBHRE7A47Jw/fx5JSUmYP38+nJ2dsWjRIjRs2BC1atXCq6++atG6srKykJCQgISEBABAYmIiEhISkJSUBJ1Oh5deeglHjhzBmjVroNfrkZKSgpSUFGnU5pCQEPTu3RujRo3C4cOHceDAAYwbNw5DhgzhkVg2xBh2uAuLiIiehEIIIZ70xjk5Odi3bx/WrVuHNWvWQAiBgoICs2+/e/dudO/evdj04cOHY9asWQgODi7xdrt27UK3bt0AFA4qOG7cOGzevBlKpRIDBw7EkiVL4OrqanYdWq0WGo0GGRkZ3KVlZQr0BrT4MAZZeQX4v/GdEOqvkbskIiKyEuZ+f1vcoPzHH39g9+7d2L17N44dO4aQkBB07doVGzZsQJcuXSxaV7du3fCorGVODvPy8sLatWstul+qPE5d1yIrrwAatQNCfBlEiYjIchaHnd69e8Pb2xuTJk3C//73P2nMG6LywH4dIiJ6Whb37HzyySfo2LEjFi5ciNDQULzyyitYsWIFzp07Vx71URVX9OSfRERET8LisDNhwgRs3LgRt2/fxtatW9GhQwds3boVTZo0Qa1atcqjRqqiCvQGxF2+C4DNyURE9OQs3o0FFPbSHDt2DLt378auXbuwf/9+GAwGeHt7l3V9VIWdvlHYr+PuZI8QP/brEBHRk7E47ERGRuLAgQPQarVo3rw5unXrhlGjRqFLly7s36Ey9aBfpxrs2K9DRERPyOKw06hRI4wZMwadO3eGRsPDgKn8GE/+2b4O+3WIiOjJWRx2Pv74Y+n33NxcODk5lWlBRADPh0VERGXH4gZlg8GAOXPmoGbNmnB1dcWlS5cAANOnT8c333xT5gVS1XT6hhaZeQVwY78OERE9JYvDzty5c7Fq1SosXLgQjo6O0vQmTZrg66+/LtPiqOqKvb8Lq12wF/t1iIjoqVgcdr777jusWLECUVFRsLOzk6Y3b94cf//9d5kWR1UXz4dFRERlxeKwc+3aNdSrV6/YdIPBAJ1OVyZFUdWmNwgcZr8OERGVEYvDTuPGjbFv375i0zds2ICWLVuWSVFUtZ2+zn4dIiIqOxYfjTVjxgwMHz4c165dg8FgwMaNG3H27Fl899132LJlS3nUSFWMNL5OEPt1iIjo6Vm8ZefFF1/E5s2bsX37dri4uGDGjBk4c+YMNm/ejOeee648aqQqJjaR/TpERFR2nuh0EZ07d0ZMTEyx6UeOHEHr1q2fuiiquvQGgVj26xARURmyeMtOVlYW7t27ZzItISEBkZGRaNeuXZkVRlXTmRtaZOYWwE1lj8b+7NchIqKnZ3bYSU5ORlhYGDQaDTQaDSZOnIicnBy89tpraNeuHVxcXPDnn3+WZ61UBRj7ddpwfB0iIiojZu/Gmjx5MnJzc7F48WJs3LgRixcvxr59+9CuXTtcvHgRtWrVKs86qYrg+bCIiKismR129u7di40bN6J9+/YYPHgwfH19ERUVhQkTJpRjeVSVFI6vw+ZkIiIqW2bvxrp58yaCg4MBADVq1ICzszMiIiLKrTCqes7c0EJr7Nfh+DpERFRGLGpQViqVJr8XPTcW0dMq2q9jb2dx7zwREVGJzN6NJYRAgwYNoFAUNo1mZWWhZcuWJgEIANLS0sq2QqoyDhU5+ScREVFZMTvsrFy5sjzroCrOYBCIu8zxdYiIqOyZHXaGDx9ennVQFXcmRYuMezq4quwRyvF1iIioDD3RCMpEj5NfYEDGPV2RS37hzxwdMu4VIOOeDun38qG9P/96ei4AoE2QJ/t1iIioTDHsUKkK9A8HliKXnAe/p9//aQwu6Tk63NPpn+g+e4X6lvGjICKiqo5hpwpKSE7HyWsZxYJL+r18ZNwrkEJLVl7BU92PQgG4qeyhcXaARu0AD7UjNGoHuKsLr2vUDvBwfvC7j7sKdb1dy+hREhERFWLYqWKup9/DS8v/RIFBmH0bN5W9SUApGlLcSwgtxoubkwNP+UBERLJj2KliDl68gwKDQA03Fbo28DYJKQ+Ci+ODaU727KEhIqJKzeKwo9frsWrVKuzYsQOpqakwGAwm83fu3FlmxVHZMx7e3f+ZmpgWESJzNUREROXP4rDz9ttvY9WqVejbty+aNGkiDTJIlcPh+2GnTSAH7iMioqrB4rDzww8/YP369ejTp0951EPl6HZWHi7dygYAtA7ylLkaIiKiimFxM4ajoyPq1atXHrVQOTty+S4AoKGPGzyceV4zIiKqGiwOO5MmTcLixYshhPlH85B1MPbrtAnmVh0iIqo6LN6NtX//fuzatQu///47QkND4eDgYDJ/48aNZVYclS0p7ASxX4eIiKoOi8OOh4cH+vfvXx61UDnKzivAqetaAAw7RERUtVgcdsry7Od79+7Fxx9/jKNHj+LGjRv45Zdf0K9fP2n+xo0bER0djaNHjyItLQ3Hjh1DixYtTNaRm5uLSZMm4YcffkBeXh569eqFL774Aj4+PmVWpy2IT7oLvUGgpoca/h5qucshIiKqMLKOFpednY3mzZvj888/L3V+p06dsGDBglLX8c4772Dz5s346aefsGfPHly/fh0DBgwor5Irrbj7zcltg7lVh4iIqpYnGkF5w4YNWL9+PZKSkpCfn28yLz4+3uz1REREICIiotT5w4YNAwBcvny5xPkZGRn45ptvsHbtWjz77LMACrc8hYSE4NChQ2jfvr3Ztdi6uET26xARUdVk8ZadJUuWYMSIEfDx8cGxY8fQtm1bVKtWDZcuXXpkcCkPR48ehU6nQ3h4uDStUaNGqF27Ng4ePFjq7fLy8qDVak0utiy/wIBjycYtOzwSi4iIqhaLw84XX3yBFStWYOnSpXB0dMSUKVMQExOD8ePHIyMjozxqLFVKSgocHR3h4eFhMt3HxwcpKSml3m7+/PnQaDTSJSAgoJwrldfJ6xnI1Rng6ezAs4oTEVGVY3HYSUpKQocOHQAAarUamZmZAAp3Oa1bt65sqysn06ZNQ0ZGhnRJTk6Wu6RyZdyF1TrIi6f3ICKiKsfisOPr64u0tMIvz9q1a+PQoUMAgMTExAofaNDX1xf5+flIT083mX7z5k34+vqWejuVSgV3d3eTiy2TmpPZr0NERFWQxWHn2WefxW+//QYAGDFiBN555x0899xzePnllyt8/J1WrVrBwcEBO3bskKadPXsWSUlJCAsLq9BarJXBIHDkinHkZIYdIiKqeiw+GmvFihUwGAwAgLfeegvVqlXDn3/+iRdeeAFjxoyxaF1ZWVm4cOGCdD0xMREJCQnw8vJC7dq1kZaWhqSkJFy/fh1AYZABCrfo+Pr6QqPR4I033sDEiRPh5eUFd3d3/Otf/0JYWBiPxLrvwq0spOfooHawQ6i/bW/BIiIiKolCyHiSq927d6N79+7Fpg8fPhyrVq3CqlWrMGLEiGLzZ86ciVmzZgF4MKjgunXrTAYVfNRurIdptVpoNBpkZGTY3C6t7w9dwb83nUSHutWwdhQDIBER2Q5zv7+fKOzs27cPX375JS5evIgNGzagZs2aWL16NYKDg9GpU6enKlwOthx2JvxwDJsSruPtHvXxznMN5C6HiIiozJj7/W1xz87PP/+MXr16Qa1W49ixY8jLywNQOMDfvHnznrxiKhccOZmIiKo6i8PO3LlzER0dja+++srkjOcdO3a0aPRkKn/X0u/hWvo92CkVaFnbQ+5yiIiIZGFx2Dl79iy6dOlSbLpGoyl2CDjJyzi+ThN/dzg7PtGZQYiIiCq9Jxpnp+gRVEb79+9HnTp1yqQoKhuHL/N8WERERBaHnVGjRuHtt99GbGwsFAoFrl+/jjVr1uDdd9/F2LFjy6NGekJHLnN8HSIiIov3bUydOhUGgwE9evRATk4OunTpApVKhXfffRf/+te/yqNGegJ3s/Nx7mYWAG7ZISKiqs3isKNQKPDBBx9g8uTJuHDhArKystC4cWO4uvIEk9bkyJXCo7Dq1XCFl4ujzNUQERHJ54m7Vh0dHdG4ceOyrIXKUJzUr+MpcyVERETyMjvsjBw50qzl/vvf/z5xMVR2DieyOZmIiAiwIOysWrUKgYGBaNmyZYWf3Zwscy9fj5PXMgAw7BAREZkddsaOHYt169YhMTERI0aMwKuvvgovL36RWqNjyXdRYBDw0zihlqda7nKIiIhkZfah559//jlu3LiBKVOmYPPmzQgICMDgwYOxbds2bumxMnGJhc3JrYO8oFAoZK6GiIhIXhaNs6NSqTB06FDExMTg9OnTCA0NxZtvvomgoCBkZWWVV41kIWNzcls2JxMREVk+qKB0Q6USCoUCQgjo9fqyrImeQoHegPikwi07HEyQiIjIwrCTl5eHdevW4bnnnkODBg1w4sQJLFu2DElJSRxnx0qcvqFFTr4eGrUDGtRwk7scIiIi2ZndoPzmm2/ihx9+QEBAAEaOHIl169ahevXq5VkbPQHjIeetAz2hVLJfh4iIyOywEx0djdq1a6NOnTrYs2cP9uzZU+JyGzduLLPiyHLGfp3WPOSciIgIgAVh57XXXuORPVZOCIEjlwv7ddoGszmZiIgIsHBQQbJul25n4052PlT2SjSt6SF3OURERFbhiY/GIusTd79fp0WABxzt+aclIiICGHZsymHj+Do85JyIiEjCsGND2JxMRERUHMOOjUjJyEVy2j0oFcAztT3kLoeIiMhqMOzYCONWncb+7nBzcpC5GiIiIuvBsGMjjGGnDXdhERERmWDYsRHGkZMZdoiIiEwx7NiAjHs6nL2ZCYBhh4iI6GEMOzbg6JU0CAEEV3eBt5tK7nKIiIisCsOODYi7f4qINkE8RQQREdHDGHZsQBz7dYiIiErFsFPJ5er0+OtqBgCGHSIiopIw7FRyx5PTka83wNtNhcBqznKXQ0REZHUYdio54/g6bYO8oFAoZK6GiIjI+jDsVHJsTiYiIno0hp1KTG8QiL9yP+zwTOdEREQlYtipxM7c0CIzrwBuKns08nWXuxwiIiKrJGvY2bt3LyIjI+Hv7w+FQoFNmzaZzBdCYMaMGfDz84NarUZ4eDjOnz9vskxaWhqioqLg7u4ODw8PvPHGG8jKyqrARyEfY7/OM4GesFOyX4eIiKgksoad7OxsNG/eHJ9//nmJ8xcuXIglS5YgOjoasbGxcHFxQa9evZCbmystExUVhVOnTiEmJgZbtmzB3r17MXr06Ip6CLI6cr9fpy13YREREZVKIYQQchcBAAqFAr/88gv69esHoHCrjr+/PyZNmoR3330XAJCRkQEfHx+sWrUKQ4YMwZkzZ9C4cWPExcWhdevWAICtW7eiT58+uHr1Kvz9/c26b61WC41Gg4yMDLi7V47dQUIItJ23A7cy87B+TBgDDxERVTnmfn9bbc9OYmIiUlJSEB4eLk3TaDRo164dDh48CAA4ePAgPDw8pKADAOHh4VAqlYiNjS113Xl5edBqtSaXyubKnRzcysyDo50SzWpp5C6HiIjIallt2ElJSQEA+Pj4mEz38fGR5qWkpKBGjRom8+3t7eHl5SUtU5L58+dDo9FIl4CAgDKuvvwdvt+v06yWBk4OdjJXQ0REZL2sNuyUp2nTpiEjI0O6JCcny12SxaTzYXH3FRER0SNZbdjx9fUFANy8edNk+s2bN6V5vr6+SE1NNZlfUFCAtLQ0aZmSqFQquLu7m1wqmyP3x9dpy/NhERERPZLVhp3g4GD4+vpix44d0jStVovY2FiEhYUBAMLCwpCeno6jR49Ky+zcuRMGgwHt2rWr8JorSmpmLhJvZ0OhKDzsnIiIiEpnL+edZ2Vl4cKFC9L1xMREJCQkwMvLC7Vr18aECRMwd+5c1K9fH8HBwZg+fTr8/f2lI7ZCQkLQu3dvjBo1CtHR0dDpdBg3bhyGDBli9pFYlZHxkPOGPm7QqB1kroaIiMi6yRp2jhw5gu7du0vXJ06cCAAYPnw4Vq1ahSlTpiA7OxujR49Geno6OnXqhK1bt8LJyUm6zZo1azBu3Dj06NEDSqUSAwcOxJIlSyr8sVSkw/f7dXi4ORER0eNZzTg7cqps4+z0XbIPp65rsXRoS0Q2t90tWERERI9S6cfZoZJl5upw5kbhuEDcskNERPR4DDuVTHxSOgwCqO3lDB93p8ffgIiIqIpj2KlkjOPrtA7iUVhERETmYNipZIwjJ3N8HSIiIvMw7FQieQV6JCSnA+DIyUREROZi2KlETl7LQH6BAdVcHFGnuovc5RAREVUKDDuVyOHEwsEEWwd5QqFQyFwNERFR5cCwU4nE3e/XacN+HSIiIrMx7FQSBoPAkcscOZmIiMhSDDuVxLnUTGhzC+DiaIfGftY/yjMREZG1YNipJIzj6zwT6Al7O/7ZiIiIzMVvzUri8P0znbcO5C4sIiIiSzDsVAJCCGnLTptgjpxMRERkCYadSuDq3XtI0ebCwU6BlgEMO0RERJZg2KkEjIecN6mpgdrRTuZqiIiIKheGnUogjufDIiIiemIMO5XAYelM5ww7RERElmLYsXJ3svJw8VY2AKB1IPt1iIiILMWwY+Xi7h9y3sDHFZ4ujjJXQ0REVPkw7Fi5IzwfFhER0VNh2LFyPPknERHR02HYsWLZeQU4eV0LAGjDk38SERE9EYYdK3YsKR16g0BNDzVqeqjlLoeIiKhSYtixYoelXVg8CouIiOhJMexYMak5mbuwiIiInhjDjpXS6Q04lpQOgM3JRERET4Nhx0qdvJaBezo9PJwdUM/bVe5yiIiIKi2GHStlPOS8daAXlEqFzNUQERFVXgw7Vso4cnLbYDYnExERPQ2GHStkMAiOnExERFRGGHas0MVbWbibo4OTgxKh/hq5yyEiIqrUGHaskHF8nZYBnnC055+IiIjoafCb1ArFJXJ8HSIiorLCsGOFpOZk9usQERE9NYYdK3M9/R6upd+DnVKBlrU95C6HiIio0mPYsTLG8XVC/d3horKXuRoiIqLKz+rDTmZmJiZMmIDAwECo1Wp06NABcXFx0nwhBGbMmAE/Pz+o1WqEh4fj/PnzMlb8dA4n8pBzIiKismT1Yecf//gHYmJisHr1apw4cQI9e/ZEeHg4rl27BgBYuHAhlixZgujoaMTGxsLFxQW9evVCbm6uzJU/mTiOr0NERFSmFEIIIXcRpbl37x7c3Nzw66+/om/fvtL0Vq1aISIiAnPmzIG/vz8mTZqEd999FwCQkZEBHx8frFq1CkOGDDHrfrRaLTQaDTIyMuDu7l4uj8Uc6Tn5aPFhDADg6L/DUc1VJVstRERE1s7c72+r3rJTUFAAvV4PJycnk+lqtRr79+9HYmIiUlJSEB4eLs3TaDRo164dDh48WOp68/LyoNVqTS7W4Mj9o7DqeLsw6BAREZURqw47bm5uCAsLw5w5c3D9+nXo9Xp8//33OHjwIG7cuIGUlBQAgI+Pj8ntfHx8pHklmT9/PjQajXQJCAgo18dhLuMuLB5yTkREVHasOuwAwOrVqyGEQM2aNaFSqbBkyRIMHToUSuWTlz5t2jRkZGRIl+Tk5DKs+MkdZr8OERFRmbP6sFO3bl3s2bMHWVlZSE5OxuHDh6HT6VCnTh34+voCAG7evGlym5s3b0rzSqJSqeDu7m5ykdu9fD1OXM0AALTlyMlERERlxurDjpGLiwv8/Pxw9+5dbNu2DS+++CKCg4Ph6+uLHTt2SMtptVrExsYiLCxMxmotl5CcjgKDgK+7E2p5quUuh4iIyGZY/ah127ZtgxACDRs2xIULFzB58mQ0atQII0aMgEKhwIQJEzB37lzUr18fwcHBmD59Ovz9/dGvXz+5S7eIsV+ndZAnFAqFzNUQERHZDqsPOxkZGZg2bRquXr0KLy8vDBw4EB999BEcHBwAAFOmTEF2djZGjx6N9PR0dOrUCVu3bi12BJe1k5qTuQuLiIioTFn1ODsVRe5xdgr0BjSf/Qey8/X4/e3OCPGTv4eIiIjI2tnEODtVxZkbmcjO18PNyR4NfdzkLoeIiMimMOxYAeMh560DPaFUsl+HiIioLDHsWIE448k/2a9DRERU5hh2ZCaE4MjJRERE5YhhR2aXbmfjTnY+HO2VaFpLI3c5RERENodhR2ZH7m/VaRHgAZW9nczVEBER2R6GHZkdTiw803mbIE+ZKyEiIrJNDDsyi+PJP4mIiMoVw46MbmpzkZSWA6UCaBXILTtERETlgWFHRofvH3Ie4ucONycHmashIiKyTQw7MjrCXVhERETljmFHRocvG5uTGXaIiIjKC8OOTDLu6fB3ihYA0CaY/TpERETlhWFHJvFX7kIIIKiaM2q4OcldDhERkc1i2JHJYfbrEBERVQiGHZmwOZmIiKhiMOzIIFenx/HkDAA80zkREVF5Y9iRwV9XM5CvN6C6qwpB1ZzlLoeIiMimMezIwHiKiLbBnlAoFDJXQ0REZNsYdmTA82ERERFVHIadCqY3CBzlYIJEREQVhmGngv2dokVmXgFcVfYI8XOXuxwiIiKbx7BTweLun/zzmUBP2CnZr0NERFTeGHYqWNz9XVhtg3iKCCIioorAsFOBhBBsTiYiIqpgDDsVKCktB6mZeXCwU6B5gIfc5RAREVUJDDsV6PD9fp1mtTzg5GAnczVERERVA8NOBeIuLCIioorHsFOBpObkYDYnExERVRSGnQpyKzMPibezoVAArWpzyw4REVFFYdipIEfu78Jq6OMGjbODzNUQERFVHQw7FeQw+3WIiIhkwbBTQaTm5GCGHSIioorEsFMBMnN1OH1dCwBow5GTiYiIKhTDTgU4lpQOgwBqearhp1HLXQ4REVGVwrBTAYy7sNqyX4eIiKjCWXXY0ev1mD59OoKDg6FWq1G3bl3MmTMHQghpGSEEZsyYAT8/P6jVaoSHh+P8+fMyVl2cceRk9usQERFVPKsOOwsWLMDy5cuxbNkynDlzBgsWLMDChQuxdOlSaZmFCxdiyZIliI6ORmxsLFxcXNCrVy/k5ubKWPkDeQV6JCSnA+CRWERERHKwl7uAR/nzzz/x4osvom/fvgCAoKAgrFu3DocPHwZQuFXns88+w7///W+8+OKLAIDvvvsOPj4+2LRpE4YMGSJb7UYnr2mRV2CAl4sj6nq7yF0OERFRlWPVW3Y6dOiAHTt24Ny5cwCA48ePY//+/YiIiAAAJCYmIiUlBeHh4dJtNBoN2rVrh4MHD5a63ry8PGi1WpNLeTH267QO9IRCoSi3+yEiIqKSWfWWnalTp0Kr1aJRo0aws7ODXq/HRx99hKioKABASkoKAMDHx8fkdj4+PtK8ksyfPx+zZ88uv8KLiLvfr9OW/TpERESysOotO+vXr8eaNWuwdu1axMfH49tvv8WiRYvw7bffPtV6p02bhoyMDOmSnJxcRhWbMhgEjlwpPPkn+3WIiIjkYdVbdiZPnoypU6dKvTdNmzbFlStXMH/+fAwfPhy+vr4AgJs3b8LPz0+63c2bN9GiRYtS16tSqaBSqcq1dgA4l5qJjHs6ODvaIdTfvdzvj4iIiIqz6i07OTk5UCpNS7Szs4PBYAAABAcHw9fXFzt27JDma7VaxMbGIiwsrEJrLUnc5cKtOi1re8DezqqfaiIiIptl1Vt2IiMj8dFHH6F27doIDQ3FsWPH8Mknn2DkyJEAAIVCgQkTJmDu3LmoX78+goODMX36dPj7+6Nfv37yFo8H/TrchUVERCQfqw47S5cuxfTp0/Hmm28iNTUV/v7+GDNmDGbMmCEtM2XKFGRnZ2P06NFIT09Hp06dsHXrVjg5OclYeaGcfD2UCo6cTEREJCeFKDoccRWl1Wqh0WiQkZEBd/ey7a3JyiuAyl4JB+7GIiIiKlPmfn9b9ZYdW+Cq4lNMREQkJ25uICIiIpvGsENEREQ2jWGHiIiIbBrDDhEREdk0hh0iIiKyaQw7REREZNMYdoiIiMimMewQERGRTWPYISIiIpvGsENEREQ2jWGHiIiIbBrDDhEREdk0hh0iIiKyaTwlNwAhBIDCU8UTERFR5WD83jZ+j5eGYQdAZmYmACAgIEDmSoiIiMhSmZmZ0Gg0pc5XiMfFoSrAYDDg+vXrcHNzg0KhKLP1arVaBAQEIDk5Ge7u7mW2XlvE58oyfL7Mx+fKfHyuzMfnynzl+VwJIZCZmQl/f38olaV35nDLDgClUolatWqV2/rd3d35ZjATnyvL8PkyH58r8/G5Mh+fK/OV13P1qC06RmxQJiIiIpvGsENEREQ2jWGnHKlUKsycORMqlUruUqwenyvL8PkyH58r8/G5Mh+fK/NZw3PFBmUiIiKyadyyQ0RERDaNYYeIiIhsGsMOERER2TSGHSIiIrJpDDvl6PPPP0dQUBCcnJzQrl07HD58WO6SrM78+fPRpk0buLm5oUaNGujXrx/Onj0rd1mVwv/7f/8PCoUCEyZMkLsUq3Tt2jW8+uqrqFatGtRqNZo2bYojR47IXZbV0ev1mD59OoKDg6FWq1G3bl3MmTPnsecaqir27t2LyMhI+Pv7Q6FQYNOmTSbzhRCYMWMG/Pz8oFarER4ejvPnz8tTrMwe9VzpdDq89957aNq0KVxcXODv74/XXnsN169fr5DaGHbKyY8//oiJEydi5syZiI+PR/PmzdGrVy+kpqbKXZpV2bNnD9566y0cOnQIMTEx0Ol06NmzJ7Kzs+UuzarFxcXhyy+/RLNmzeQuxSrdvXsXHTt2hIODA37//XecPn0a//nPf+Dp6Sl3aVZnwYIFWL58OZYtW4YzZ85gwYIFWLhwIZYuXSp3aVYhOzsbzZs3x+eff17i/IULF2LJkiWIjo5GbGwsXFxc0KtXL+Tm5lZwpfJ71HOVk5OD+Ph4TJ8+HfHx8di4cSPOnj2LF154oWKKE1Qu2rZtK9566y3pul6vF/7+/mL+/PkyVmX9UlNTBQCxZ88euUuxWpmZmaJ+/foiJiZGdO3aVbz99ttyl2R13nvvPdGpUye5y6gU+vbtK0aOHGkybcCAASIqKkqmiqwXAPHLL79I1w0Gg/D19RUff/yxNC09PV2oVCqxbt06GSq0Hg8/VyU5fPiwACCuXLlS7vVwy045yM/Px9GjRxEeHi5NUyqVCA8Px8GDB2WszPplZGQAALy8vGSuxHq99dZb6Nu3r8nri0z99ttvaN26NQYNGoQaNWqgZcuW+Oqrr+Quyyp16NABO3bswLlz5wAAx48fx/79+xERESFzZdYvMTERKSkpJu9FjUaDdu3a8bPeDBkZGVAoFPDw8Cj3++KJQMvB7du3odfr4ePjYzLdx8cHf//9t0xVWT+DwYAJEyagY8eOaNKkidzlWKUffvgB8fHxiIuLk7sUq3bp0iUsX74cEydOxPvvv4+4uDiMHz8ejo6OGD58uNzlWZWpU6dCq9WiUaNGsLOzg16vx0cffYSoqCi5S7N6KSkpAFDiZ71xHpUsNzcX7733HoYOHVohJ1Jl2CGr8dZbb+HkyZPYv3+/3KVYpeTkZLz99tuIiYmBk5OT3OVYNYPBgNatW2PevHkAgJYtW+LkyZOIjo5m2HnI+vXrsWbNGqxduxahoaFISEjAhAkT4O/vz+eKyoVOp8PgwYMhhMDy5csr5D65G6scVK9eHXZ2drh586bJ9Js3b8LX11emqqzbuHHjsGXLFuzatQu1atWSuxyrdPToUaSmpuKZZ56Bvb097O3tsWfPHixZsgT29vbQ6/Vyl2g1/Pz80LhxY5NpISEhSEpKkqki6zV58mRMnToVQ4YMQdOmTTFs2DC88847mD9/vtylWT3j5zk/681nDDpXrlxBTExMhWzVARh2yoWjoyNatWqFHTt2SNMMBgN27NiBsLAwGSuzPkIIjBs3Dr/88gt27tyJ4OBguUuyWj169MCJEyeQkJAgXVq3bo2oqCgkJCTAzs5O7hKtRseOHYsNYXDu3DkEBgbKVJH1ysnJgVJp+lVgZ2cHg8EgU0WVR3BwMHx9fU0+67VaLWJjY/lZXwJj0Dl//jy2b9+OatWqVdh9czdWOZk4cSKGDx+O1q1bo23btvjss8+QnZ2NESNGyF2aVXnrrbewdu1a/Prrr3Bzc5P2c2s0GqjVapmrsy5ubm7FeplcXFxQrVo19jg95J133kGHDh0wb948DB48GIcPH8aKFSuwYsUKuUuzOpGRkfjoo49Qu3ZthIaG4tixY/jkk08wcuRIuUuzCllZWbhw4YJ0PTExEQkJCfDy8kLt2rUxYcIEzJ07F/Xr10dwcDCmT58Of39/9OvXT76iZfKo58rPzw8vvfQS4uPjsWXLFuj1eunz3svLC46OjuVbXLkf71WFLV26VNSuXVs4OjqKtm3bikOHDsldktUBUOJl5cqVcpdWKfDQ89Jt3rxZNGnSRKhUKtGoUSOxYsUKuUuySlqtVrz99tuidu3awsnJSdSpU0d88MEHIi8vT+7SrMKuXbtK/IwaPny4EKLw8PPp06cLHx8foVKpRI8ePcTZs2flLVomj3quEhMTS/2837VrV7nXphCCw2QSERGR7WLPDhEREdk0hh0iIiKyaQw7REREZNMYdoiIiMimMewQERGRTWPYISIiIpvGsENEREQ2jWGHiCrc5cuXoVAokJCQUO73tWrVKnh4eJT7/RCR9WLYISITr7/+OhQKRbFL79695S7tsYKCgvDZZ5+ZTHv55Zdx7tw5eQq6r1u3bpgwYYKsNRBVZTw3FhEV07t3b6xcudJkmkqlkqmap6NWq3meNaIqjlt2iKgYlUoFX19fk4unpycA4JVXXsHLL79ssrxOp0P16tXx3XffAQC2bt2KTp06wcPDA9WqVcPzzz+Pixcvlnp/Je1q2rRpExQKhXT94sWLePHFF+Hj4wNXV1e0adMG27dvl+Z369YNV65cwTvvvCNtjSpt3cuXL0fdunXh6OiIhg0bYvXq1SbzFQoFvv76a/Tv3x/Ozs6oX78+fvvtt0c+Z1988QXq168PJycn+Pj44KWXXgJQuKVsz549WLx4sVTX5cuXAQAnT55EREQEXF1d4ePjg2HDhuH27dsmj2ncuHEYN24cNBoNqlevjunTp4Nn+SGyDMMOEVkkKioKmzdvRlZWljRt27ZtyMnJQf/+/QEA2dnZmDhxIo4cOYIdO3ZAqVSif//+MBgMT3y/WVlZ6NOnD3bs2IFjx46hd+/eiIyMRFJSEgBg48aNqFWrFj788EPcuHEDN27cKHE9v/zyC95++21MmjQJJ0+exJgxYzBixAjs2rXLZLnZs2dj8ODB+Ouvv9CnTx9ERUUhLS2txHUeOXIE48ePx4cffoizZ89i69at6NKlCwBg8eLFCAsLw6hRo6S6AgICkJ6ejmeffRYtW7bEkSNHsHXrVty8eRODBw82Wfe3334Le3t7HD58GIsXL8Ynn3yCr7/++omfR6IqqdxPNUpElcrw4cOFnZ2dcHFxMbl89NFHQgghdDqdqF69uvjuu++k2wwdOlS8/PLLpa7z1q1bAoA4ceKEEEJIZ0A+duyYEEKIlStXCo1GY3KbX375RTzuIyo0NFQsXbpUuh4YGCg+/fRTk2UeXneHDh3EqFGjTJYZNGiQ6NOnj3QdgPj3v/8tXc/KyhIAxO+//15iHT///LNwd3cXWq22xPklnZ1+zpw5omfPnibTkpOTBQDprNldu3YVISEhwmAwSMu89957IiQkpMT7IaKSccsOERXTvXt3JCQkmFz++c9/AgDs7e0xePBgrFmzBkDhVpxff/0VUVFR0u3Pnz+PoUOHok6dOnB3d0dQUBAASFthnkRWVhbeffddhISEwMPDA66urjhz5ozF6zxz5gw6duxoMq1jx444c+aMybRmzZpJv7u4uMDd3R2pqaklrvO5555DYGAg6tSpg2HDhmHNmjXIycl5ZB3Hjx/Hrl274OrqKl0aNWoEACa7/Nq3b2+yOy8sLAznz5+HXq837wETERuUiag4FxcX1KtXr9T5UVFR6Nq1K1JTUxETEwO1Wm1ytFZkZCQCAwPx1Vdfwd/fHwaDAU2aNEF+fn6J61MqlcX6UHQ6ncn1d999FzExMVi0aBHq1asHtVqNl156qdR1Pi0HBweT6wqFotTdcG5uboiPj8fu3bvxxx9/YMaMGZg1axbi4uJKPew9KysLkZGRWLBgQbF5fn5+T10/ET3AsENEFuvQoQMCAgLw448/4vfff8egQYOkcHDnzh2cPXsWX331FTp37gwA2L9//yPX5+3tjczMTGRnZ8PFxQUAio3Bc+DAAbz++utSX1BWVpbU6Gvk6Oj42C0eISEhOHDgAIYPH26y7saNGz/2cT+Kvb09wsPDER4ejpkzZ8LDwwM7d+7EgAEDSqzrmWeewc8//4ygoCDY25f+URwbG2ty/dChQ6hfvz7s7Oyeql6iqoRhh4iKycvLQ0pKisk0e3t7VK9eXbr+yiuvIDo6GufOnTNp7vX09ES1atWwYsUK+Pn5ISkpCVOnTn3k/bVr1w7Ozs54//33MX78eMTGxmLVqlUmy9SvXx8bN25EZGQkFAoFpk+fXmxLS1BQEPbu3YshQ4ZApVKZ1Gs0efJkDB48GC1btkR4eDg2b96MjRs3mhzZZaktW7bg0qVL6NKlCzw9PfG///0PBoMBDRs2lOqKjY3F5cuX4erqCi8vL7z11lv46quvMHToUEyZMgVeXl64cOECfvjhB3z99ddSmElKSsLEiRMxZswYxMfHY+nSpfjPf/7zxLUSVUlyNw0RkXUZPny4AFDs0rBhQ5PlTp8+LQCIwMBAkwZaIYSIiYkRISEhQqVSiWbNmondu3cLAOKXX34RQhRvUBaisCG5Xr16Qq1Wi+eff16sWLHCpEE5MTFRdO/eXajVahEQECCWLVtWrPH34MGDolmzZkKlUkm3Lan5+YsvvhB16tQRDg4OokGDBibN1kIIk1qNNBqNWLlyZYnP2b59+0TXrl2Fp6enUKvVolmzZuLHH3+U5p89e1a0b99eqNVqAUAkJiYKIYQ4d+6c6N+/v/Dw8BBqtVo0atRITJgwQXo+u3btKt58803xz3/+U7i7uwtPT0/x/vvvF3u+iejRFEJwwAYiImvUrVs3tGjRotio0ERkGR6NRURERDaNYYeIiIhsGndjERERkU3jlh0iIiKyaQw7REREZNMYdoiIiMimMewQERGRTWPYISIiIpvGsENEREQ2jWGHiIiIbBrDDhEREdk0hh0iIiKyaf8fU5iqYZRPOecAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create and train\n",
        "ent_coef=0.1\n",
        "eval_freq=2000\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef)\n",
        "model.learn(total_timesteps=100000, callback=eval_callback, progress_bar=False)\n",
        "\n",
        "# Save the model\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mL_LYMHeejpF",
        "outputId": "bf7e5c71-7b5f-40e4-b095-5ffad2515431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=150.27 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 150      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward 150.27\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 137      |\n",
            "|    ep_rew_mean     | 295      |\n",
            "| time/              |          |\n",
            "|    fps             | 1196     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=150.16 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010891594 |\n",
            "|    clip_fraction        | 0.071       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.73       |\n",
            "|    explained_variance   | 0.687       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.216      |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | 0.000695    |\n",
            "|    std                  | 2.65        |\n",
            "|    value_loss           | 0.503       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 4000: mean reward 150.16\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 137      |\n",
            "|    ep_rew_mean     | 318      |\n",
            "| time/              |          |\n",
            "|    fps             | 822      |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=150.36 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014330605 |\n",
            "|    clip_fraction        | 0.0839      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.86       |\n",
            "|    explained_variance   | 0.793       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.404      |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | 0.000798    |\n",
            "|    std                  | 2.85        |\n",
            "|    value_loss           | 0.242       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward 150.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 142      |\n",
            "|    ep_rew_mean     | 345      |\n",
            "| time/              |          |\n",
            "|    fps             | 746      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=150.32 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 150          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 32000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073479274 |\n",
            "|    clip_fraction        | 0.0558       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.96        |\n",
            "|    explained_variance   | 0.798        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.355       |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.000181    |\n",
            "|    std                  | 2.97         |\n",
            "|    value_loss           | 0.25         |\n",
            "------------------------------------------\n",
            "Evaluation at step 8000: mean reward 150.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 431      |\n",
            "| time/              |          |\n",
            "|    fps             | 713      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=150.39 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009534074 |\n",
            "|    clip_fraction        | 0.0779      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.04       |\n",
            "|    explained_variance   | 0.834       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.424      |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00213    |\n",
            "|    std                  | 3.12        |\n",
            "|    value_loss           | 0.195       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 10000: mean reward 150.39\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 473      |\n",
            "| time/              |          |\n",
            "|    fps             | 696      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 58       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=150.26 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 48000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014464151 |\n",
            "|    clip_fraction        | 0.0928      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.15       |\n",
            "|    explained_variance   | 0.841       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.418      |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00168    |\n",
            "|    std                  | 3.34        |\n",
            "|    value_loss           | 0.189       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 12000: mean reward 150.26\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 522      |\n",
            "| time/              |          |\n",
            "|    fps             | 685      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 71       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=150.49 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009365779 |\n",
            "|    clip_fraction        | 0.0729      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.25       |\n",
            "|    explained_variance   | 0.819       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.411      |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0023     |\n",
            "|    std                  | 3.51        |\n",
            "|    value_loss           | 0.228       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 14000: mean reward 150.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 164      |\n",
            "|    ep_rew_mean     | 628      |\n",
            "| time/              |          |\n",
            "|    fps             | 678      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 84       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=150.51 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 151         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013514825 |\n",
            "|    clip_fraction        | 0.0857      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.36       |\n",
            "|    explained_variance   | 0.854       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.421      |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.000585   |\n",
            "|    std                  | 3.76        |\n",
            "|    value_loss           | 0.2         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 16000: mean reward 150.51\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 190      |\n",
            "|    ep_rew_mean     | 758      |\n",
            "| time/              |          |\n",
            "|    fps             | 672      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=150.41 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 72000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013556324 |\n",
            "|    clip_fraction        | 0.0821      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.46       |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.469      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.000628   |\n",
            "|    std                  | 3.99        |\n",
            "|    value_loss           | 0.179       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 18000: mean reward 150.41\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 205      |\n",
            "|    ep_rew_mean     | 908      |\n",
            "| time/              |          |\n",
            "|    fps             | 667      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 110      |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=150.41 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 150        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 80000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01400767 |\n",
            "|    clip_fraction        | 0.0889     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.58      |\n",
            "|    explained_variance   | 0.864      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.443     |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.000469  |\n",
            "|    std                  | 4.29       |\n",
            "|    value_loss           | 0.198      |\n",
            "----------------------------------------\n",
            "Evaluation at step 20000: mean reward 150.41\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 989      |\n",
            "| time/              |          |\n",
            "|    fps             | 664      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 123      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=150.36 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 88000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014362032 |\n",
            "|    clip_fraction        | 0.0887      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.7        |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.454      |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | 0.00137     |\n",
            "|    std                  | 4.59        |\n",
            "|    value_loss           | 0.152       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 22000: mean reward 150.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 987      |\n",
            "| time/              |          |\n",
            "|    fps             | 662      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 136      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=154.08 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 154         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 96000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013676247 |\n",
            "|    clip_fraction        | 0.0875      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.81       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.553      |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00102    |\n",
            "|    std                  | 4.95        |\n",
            "|    value_loss           | 0.179       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 24000: mean reward 154.08\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 233      |\n",
            "|    ep_rew_mean     | 1.02e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 659      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 149      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=113.34 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 113         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 104000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008051613 |\n",
            "|    clip_fraction        | 0.0703      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.91       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.515      |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.000169   |\n",
            "|    std                  | 5.21        |\n",
            "|    value_loss           | 0.171       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 26000: mean reward 113.34\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 265      |\n",
            "|    ep_rew_mean     | 1.25e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 656      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 162      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=112000, episode_reward=150.37 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 112000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011890643 |\n",
            "|    clip_fraction        | 0.0755      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.02       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.54       |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | 0.00181     |\n",
            "|    std                  | 5.64        |\n",
            "|    value_loss           | 0.128       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 28000: mean reward 150.37\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 275      |\n",
            "|    ep_rew_mean     | 1.33e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 655      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 174      |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=149.84 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 120000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009819005 |\n",
            "|    clip_fraction        | 0.0672      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.13       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.451      |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.00223     |\n",
            "|    std                  | 6.04        |\n",
            "|    value_loss           | 0.15        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 30000: mean reward 149.84\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 310      |\n",
            "|    ep_rew_mean     | 1.59e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 654      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 187      |\n",
            "|    total_timesteps | 122880   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=128000, episode_reward=150.18 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 128000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010991115 |\n",
            "|    clip_fraction        | 0.0735      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.24       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.618      |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | 0.000186    |\n",
            "|    std                  | 6.44        |\n",
            "|    value_loss           | 0.15        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 32000: mean reward 150.18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 320      |\n",
            "|    ep_rew_mean     | 1.64e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 652      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 200      |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=136000, episode_reward=150.09 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 150         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 136000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013617264 |\n",
            "|    clip_fraction        | 0.0779      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.35       |\n",
            "|    explained_variance   | 0.901       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.618      |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | 0.0019      |\n",
            "|    std                  | 7.01        |\n",
            "|    value_loss           | 0.145       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 34000: mean reward 150.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 302      |\n",
            "|    ep_rew_mean     | 1.57e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 651      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 213      |\n",
            "|    total_timesteps | 139264   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=144000, episode_reward=120.69 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 121         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 144000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012827511 |\n",
            "|    clip_fraction        | 0.0767      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.48       |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.585      |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.000715    |\n",
            "|    std                  | 7.57        |\n",
            "|    value_loss           | 0.148       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 36000: mean reward 120.69\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 316      |\n",
            "|    ep_rew_mean     | 1.65e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 650      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 226      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=152000, episode_reward=3491.49 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3.49e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 152000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008072709 |\n",
            "|    clip_fraction        | 0.0583      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.59       |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.598      |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | 0.00253     |\n",
            "|    std                  | 8.09        |\n",
            "|    value_loss           | 0.106       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 38000: mean reward 3491.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 309      |\n",
            "|    ep_rew_mean     | 1.64e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 649      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 239      |\n",
            "|    total_timesteps | 155648   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=4528.96 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 4.53e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 160000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013888851 |\n",
            "|    clip_fraction        | 0.0828      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.71       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.612      |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | 0.000301    |\n",
            "|    std                  | 8.8         |\n",
            "|    value_loss           | 0.159       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 40000: mean reward 4528.96\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 326      |\n",
            "|    ep_rew_mean     | 1.72e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 648      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 252      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=168000, episode_reward=3540.46 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 3.54e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 168000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01433811 |\n",
            "|    clip_fraction        | 0.0851     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -6.84      |\n",
            "|    explained_variance   | 0.898      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.657     |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | 0.00166    |\n",
            "|    std                  | 9.66       |\n",
            "|    value_loss           | 0.121      |\n",
            "----------------------------------------\n",
            "Evaluation at step 42000: mean reward 3540.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 341      |\n",
            "|    ep_rew_mean     | 1.81e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 648      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 265      |\n",
            "|    total_timesteps | 172032   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=176000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 176000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009702887 |\n",
            "|    clip_fraction        | 0.0604      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.98       |\n",
            "|    explained_variance   | 0.887       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.657      |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | 0.00519     |\n",
            "|    std                  | 10.4        |\n",
            "|    value_loss           | 0.119       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 44000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 333      |\n",
            "|    ep_rew_mean     | 1.76e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 647      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 278      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=184000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 184000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011888519 |\n",
            "|    clip_fraction        | 0.0822      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.1        |\n",
            "|    explained_variance   | 0.878       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.659      |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | 7.14e-05    |\n",
            "|    std                  | 11.3        |\n",
            "|    value_loss           | 0.155       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 46000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 321      |\n",
            "|    ep_rew_mean     | 1.68e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 646      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 291      |\n",
            "|    total_timesteps | 188416   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=192000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 115        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 192000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01631188 |\n",
            "|    clip_fraction        | 0.102      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.24      |\n",
            "|    explained_variance   | 0.891      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.634     |\n",
            "|    n_updates            | 360        |\n",
            "|    policy_gradient_loss | -0.000364  |\n",
            "|    std                  | 12.4       |\n",
            "|    value_loss           | 0.149      |\n",
            "----------------------------------------\n",
            "Evaluation at step 48000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 273      |\n",
            "|    ep_rew_mean     | 1.38e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 646      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 304      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 200000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010574965 |\n",
            "|    clip_fraction        | 0.0681      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.36       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.698      |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | 0.00011     |\n",
            "|    std                  | 13.4        |\n",
            "|    value_loss           | 0.155       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 50000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 267      |\n",
            "|    ep_rew_mean     | 1.35e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 645      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 317      |\n",
            "|    total_timesteps | 204800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=208000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 208000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059473324 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -7.45        |\n",
            "|    explained_variance   | 0.864        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.645       |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | 0.00107      |\n",
            "|    std                  | 14.2         |\n",
            "|    value_loss           | 0.143        |\n",
            "------------------------------------------\n",
            "Evaluation at step 52000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 283      |\n",
            "|    ep_rew_mean     | 1.47e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 645      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 329      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=216000, episode_reward=3569.60 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3.57e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 216000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010355056 |\n",
            "|    clip_fraction        | 0.0634      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.56       |\n",
            "|    explained_variance   | 0.899       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.651      |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | 0.00355     |\n",
            "|    std                  | 15.5        |\n",
            "|    value_loss           | 0.114       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 54000: mean reward 3569.60\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 296      |\n",
            "|    ep_rew_mean     | 1.57e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 645      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 342      |\n",
            "|    total_timesteps | 221184   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=224000, episode_reward=3847.92 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3.85e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 224000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009991419 |\n",
            "|    clip_fraction        | 0.0738      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.66       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.699      |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | 0.000328    |\n",
            "|    std                  | 16.5        |\n",
            "|    value_loss           | 0.163       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 56000: mean reward 3847.92\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 295      |\n",
            "|    ep_rew_mean     | 1.53e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 644      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 355      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=232000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 232000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012544295 |\n",
            "|    clip_fraction        | 0.0756      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -7.77       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.692      |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.000202   |\n",
            "|    std                  | 17.9        |\n",
            "|    value_loss           | 0.154       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 58000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 294      |\n",
            "|    ep_rew_mean     | 1.56e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 644      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 368      |\n",
            "|    total_timesteps | 237568   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 240000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0097151855 |\n",
            "|    clip_fraction        | 0.0604       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -7.88        |\n",
            "|    explained_variance   | 0.882        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.766       |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | 0.0029       |\n",
            "|    std                  | 19.4         |\n",
            "|    value_loss           | 0.109        |\n",
            "------------------------------------------\n",
            "Evaluation at step 60000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 299      |\n",
            "|    ep_rew_mean     | 1.66e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 644      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 381      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=248000, episode_reward=4833.99 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 4.83e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 248000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016043838 |\n",
            "|    clip_fraction        | 0.0898      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.01       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.706      |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.000481   |\n",
            "|    std                  | 21.5        |\n",
            "|    value_loss           | 0.126       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 62000: mean reward 4833.99\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 322      |\n",
            "|    ep_rew_mean     | 1.79e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 643      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 394      |\n",
            "|    total_timesteps | 253952   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=256000, episode_reward=115.09 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 256000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010166329 |\n",
            "|    clip_fraction        | 0.0641      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.13       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.788      |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | 0.0017      |\n",
            "|    std                  | 23.3        |\n",
            "|    value_loss           | 0.125       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 64000: mean reward 115.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 340      |\n",
            "|    ep_rew_mean     | 1.92e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 643      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 407      |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=264000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 264000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067406334 |\n",
            "|    clip_fraction        | 0.0497       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -8.22        |\n",
            "|    explained_variance   | 0.88         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.774       |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | 0.000926     |\n",
            "|    std                  | 24.7         |\n",
            "|    value_loss           | 0.13         |\n",
            "------------------------------------------\n",
            "Evaluation at step 66000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 382      |\n",
            "|    ep_rew_mean     | 2.13e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 642      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 420      |\n",
            "|    total_timesteps | 270336   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=272000, episode_reward=6410.53 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.41e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 272000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010941563 |\n",
            "|    clip_fraction        | 0.0561      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.33       |\n",
            "|    explained_variance   | 0.878       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.766      |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | 0.00353     |\n",
            "|    std                  | 27          |\n",
            "|    value_loss           | 0.103       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 68000: mean reward 6410.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 343      |\n",
            "|    ep_rew_mean     | 1.9e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 642      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 433      |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=6939.19 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.94e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 280000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013218263 |\n",
            "|    clip_fraction        | 0.0808      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.46       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.8        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.000277   |\n",
            "|    std                  | 29.6        |\n",
            "|    value_loss           | 0.151       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 70000: mean reward 6939.19\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 349      |\n",
            "|    ep_rew_mean     | 1.93e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 642      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 446      |\n",
            "|    total_timesteps | 286720   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=288000, episode_reward=6907.83 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.91e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 288000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007697548 |\n",
            "|    clip_fraction        | 0.0569      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.57       |\n",
            "|    explained_variance   | 0.878       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.833      |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | 0.00148     |\n",
            "|    std                  | 31.8        |\n",
            "|    value_loss           | 0.12        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 72000: mean reward 6907.83\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 362      |\n",
            "|    ep_rew_mean     | 1.99e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 459      |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=296000, episode_reward=6531.77 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.53e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 296000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010489313 |\n",
            "|    clip_fraction        | 0.0657      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.68       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.81       |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | 0.00187     |\n",
            "|    std                  | 34.4        |\n",
            "|    value_loss           | 0.137       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 74000: mean reward 6531.77\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 365      |\n",
            "|    ep_rew_mean     | 1.99e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 472      |\n",
            "|    total_timesteps | 303104   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=304000, episode_reward=6035.65 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.04e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 304000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010919319 |\n",
            "|    clip_fraction        | 0.0592      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.8        |\n",
            "|    explained_variance   | 0.864       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.805      |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | 0.00209     |\n",
            "|    std                  | 37.4        |\n",
            "|    value_loss           | 0.141       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 76000: mean reward 6035.65\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 363      |\n",
            "|    ep_rew_mean     | 2.01e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 485      |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=312000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 151         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 312000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006727661 |\n",
            "|    clip_fraction        | 0.056       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -8.89       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.826      |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | 0.000882    |\n",
            "|    std                  | 39.8        |\n",
            "|    value_loss           | 0.118       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 78000: mean reward 150.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 376      |\n",
            "|    ep_rew_mean     | 2.03e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 498      |\n",
            "|    total_timesteps | 319488   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 151        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 320000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00882284 |\n",
            "|    clip_fraction        | 0.0555     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.99      |\n",
            "|    explained_variance   | 0.873      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.788     |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | 0.00341    |\n",
            "|    std                  | 43.1       |\n",
            "|    value_loss           | 0.123      |\n",
            "----------------------------------------\n",
            "Evaluation at step 80000: mean reward 150.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 367      |\n",
            "|    ep_rew_mean     | 1.96e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 641      |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 511      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=328000, episode_reward=4803.43 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 4.8e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 328000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012495583 |\n",
            "|    clip_fraction        | 0.0724      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.11       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.87       |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | 0.000577    |\n",
            "|    std                  | 47.5        |\n",
            "|    value_loss           | 0.146       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 82000: mean reward 4803.43\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 331      |\n",
            "|    ep_rew_mean     | 1.78e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 640      |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 523      |\n",
            "|    total_timesteps | 335872   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=336000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 151         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 336000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013979707 |\n",
            "|    clip_fraction        | 0.0911      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.24       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.839      |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | 0.000605    |\n",
            "|    std                  | 52.1        |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 84000: mean reward 150.75\n",
            "Eval num_timesteps=344000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 151      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 344000   |\n",
            "---------------------------------\n",
            "Evaluation at step 86000: mean reward 150.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 355      |\n",
            "|    ep_rew_mean     | 1.84e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 540      |\n",
            "|    total_timesteps | 344064   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=352000, episode_reward=6488.76 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+03     |\n",
            "|    mean_reward          | 6.49e+03  |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 352000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0051346 |\n",
            "|    clip_fraction        | 0.0422    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -9.33     |\n",
            "|    explained_variance   | 0.886     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.896    |\n",
            "|    n_updates            | 550       |\n",
            "|    policy_gradient_loss | 0.00113   |\n",
            "|    std                  | 54.5      |\n",
            "|    value_loss           | 0.116     |\n",
            "---------------------------------------\n",
            "Evaluation at step 88000: mean reward 6488.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 359      |\n",
            "|    ep_rew_mean     | 1.88e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 553      |\n",
            "|    total_timesteps | 352256   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=6794.50 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.79e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 360000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013897024 |\n",
            "|    clip_fraction        | 0.0833      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.42       |\n",
            "|    explained_variance   | 0.882       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.884      |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.000124   |\n",
            "|    std                  | 59.7        |\n",
            "|    value_loss           | 0.134       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 90000: mean reward 6794.50\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 356      |\n",
            "|    ep_rew_mean     | 1.84e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 566      |\n",
            "|    total_timesteps | 360448   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=368000, episode_reward=7167.52 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.17e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 368000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066177202 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -9.52        |\n",
            "|    explained_variance   | 0.902        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.869       |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | 0.00256      |\n",
            "|    std                  | 63.4         |\n",
            "|    value_loss           | 0.105        |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 92000: mean reward 7167.52\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 371      |\n",
            "|    ep_rew_mean     | 1.96e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 579      |\n",
            "|    total_timesteps | 368640   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=376000, episode_reward=7354.79 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.35e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 376000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008396348 |\n",
            "|    clip_fraction        | 0.06        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.61       |\n",
            "|    explained_variance   | 0.878       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.859      |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | 0.000221    |\n",
            "|    std                  | 67.6        |\n",
            "|    value_loss           | 0.139       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 94000: mean reward 7354.79\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 372      |\n",
            "|    ep_rew_mean     | 1.93e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 592      |\n",
            "|    total_timesteps | 376832   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=384000, episode_reward=7159.12 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.16e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 384000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013218088 |\n",
            "|    clip_fraction        | 0.0677      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.72       |\n",
            "|    explained_variance   | 0.887       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.923      |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | 0.00166     |\n",
            "|    std                  | 74          |\n",
            "|    value_loss           | 0.121       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 96000: mean reward 7159.12\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 370      |\n",
            "|    ep_rew_mean     | 2.01e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 605      |\n",
            "|    total_timesteps | 385024   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=392000, episode_reward=7287.12 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 7.29e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 392000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01073712 |\n",
            "|    clip_fraction        | 0.0801     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.84      |\n",
            "|    explained_variance   | 0.87       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.919     |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | 9.48e-06   |\n",
            "|    std                  | 80.6       |\n",
            "|    value_loss           | 0.146      |\n",
            "----------------------------------------\n",
            "Evaluation at step 98000: mean reward 7287.12\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 348      |\n",
            "|    ep_rew_mean     | 1.86e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 636      |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 618      |\n",
            "|    total_timesteps | 393216   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=7362.30 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.36e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 400000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014448138 |\n",
            "|    clip_fraction        | 0.0739      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -9.97       |\n",
            "|    explained_variance   | 0.891       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.962      |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | 0.00146     |\n",
            "|    std                  | 90.2        |\n",
            "|    value_loss           | 0.129       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 100000: mean reward 7362.30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 360      |\n",
            "|    ep_rew_mean     | 1.93e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 631      |\n",
            "|    total_timesteps | 401408   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=408000, episode_reward=7345.83 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.35e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 408000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007554958 |\n",
            "|    clip_fraction        | 0.057       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.1       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.902      |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | 0.00184     |\n",
            "|    std                  | 96.8        |\n",
            "|    value_loss           | 0.127       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 102000: mean reward 7345.83\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 341      |\n",
            "|    ep_rew_mean     | 1.86e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 50       |\n",
            "|    time_elapsed    | 644      |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=416000, episode_reward=6993.51 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 6.99e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 416000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01239376 |\n",
            "|    clip_fraction        | 0.0783     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.2      |\n",
            "|    explained_variance   | 0.879      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.947     |\n",
            "|    n_updates            | 630        |\n",
            "|    policy_gradient_loss | -0.000524  |\n",
            "|    std                  | 106        |\n",
            "|    value_loss           | 0.141      |\n",
            "----------------------------------------\n",
            "Evaluation at step 104000: mean reward 6993.51\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 355      |\n",
            "|    ep_rew_mean     | 1.92e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 51       |\n",
            "|    time_elapsed    | 657      |\n",
            "|    total_timesteps | 417792   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=424000, episode_reward=7052.20 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 1e+03     |\n",
            "|    mean_reward          | 7.05e+03  |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 424000    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0097681 |\n",
            "|    clip_fraction        | 0.0658    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -10.3     |\n",
            "|    explained_variance   | 0.885     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -1        |\n",
            "|    n_updates            | 640       |\n",
            "|    policy_gradient_loss | 0.00134   |\n",
            "|    std                  | 115       |\n",
            "|    value_loss           | 0.12      |\n",
            "---------------------------------------\n",
            "Evaluation at step 106000: mean reward 7052.20\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 376      |\n",
            "|    ep_rew_mean     | 2.03e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 52       |\n",
            "|    time_elapsed    | 670      |\n",
            "|    total_timesteps | 425984   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=432000, episode_reward=6469.21 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 6.47e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 432000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071254186 |\n",
            "|    clip_fraction        | 0.0563       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -10.4        |\n",
            "|    explained_variance   | 0.847        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.992       |\n",
            "|    n_updates            | 650          |\n",
            "|    policy_gradient_loss | 0.000598     |\n",
            "|    std                  | 122          |\n",
            "|    value_loss           | 0.127        |\n",
            "------------------------------------------\n",
            "Evaluation at step 108000: mean reward 6469.21\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 408      |\n",
            "|    ep_rew_mean     | 2.23e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 53       |\n",
            "|    time_elapsed    | 683      |\n",
            "|    total_timesteps | 434176   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=6437.18 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.44e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 440000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008824713 |\n",
            "|    clip_fraction        | 0.0579      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.5       |\n",
            "|    explained_variance   | 0.844       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.958      |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | 0.00211     |\n",
            "|    std                  | 133         |\n",
            "|    value_loss           | 0.125       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 110000: mean reward 6437.18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 385      |\n",
            "|    ep_rew_mean     | 2.12e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 54       |\n",
            "|    time_elapsed    | 695      |\n",
            "|    total_timesteps | 442368   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=448000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 151         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 448000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014911578 |\n",
            "|    clip_fraction        | 0.0743      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.6       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.01       |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | 0.00101     |\n",
            "|    std                  | 146         |\n",
            "|    value_loss           | 0.128       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 112000: mean reward 150.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 417      |\n",
            "|    ep_rew_mean     | 2.29e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 55       |\n",
            "|    time_elapsed    | 708      |\n",
            "|    total_timesteps | 450560   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=456000, episode_reward=4770.93 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 4.77e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 456000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011057999 |\n",
            "|    clip_fraction        | 0.0615      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.8       |\n",
            "|    explained_variance   | 0.863       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1          |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | 0.00332     |\n",
            "|    std                  | 161         |\n",
            "|    value_loss           | 0.121       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 114000: mean reward 4770.93\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 407      |\n",
            "|    ep_rew_mean     | 2.24e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 56       |\n",
            "|    time_elapsed    | 721      |\n",
            "|    total_timesteps | 458752   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=464000, episode_reward=6902.24 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 6.9e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 464000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009708094 |\n",
            "|    clip_fraction        | 0.0624      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -10.9       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.982      |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | 0.00263     |\n",
            "|    std                  | 174         |\n",
            "|    value_loss           | 0.128       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 116000: mean reward 6902.24\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 388      |\n",
            "|    ep_rew_mean     | 2.16e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 57       |\n",
            "|    time_elapsed    | 734      |\n",
            "|    total_timesteps | 466944   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=472000, episode_reward=7213.56 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.21e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 472000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009662561 |\n",
            "|    clip_fraction        | 0.0748      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11         |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.03       |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | -0.00124    |\n",
            "|    std                  | 187         |\n",
            "|    value_loss           | 0.144       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 118000: mean reward 7213.56\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 357      |\n",
            "|    ep_rew_mean     | 1.99e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 58       |\n",
            "|    time_elapsed    | 747      |\n",
            "|    total_timesteps | 475136   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=7592.74 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.59e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 480000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010959163 |\n",
            "|    clip_fraction        | 0.0693      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.1       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.04       |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | -0.000677   |\n",
            "|    std                  | 205         |\n",
            "|    value_loss           | 0.157       |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 120000: mean reward 7592.74\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 350      |\n",
            "|    ep_rew_mean     | 1.91e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 760      |\n",
            "|    total_timesteps | 483328   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=488000, episode_reward=7629.11 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.63e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 488000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077343397 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.2        |\n",
            "|    explained_variance   | 0.896        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.08        |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | 0.00203      |\n",
            "|    std                  | 221          |\n",
            "|    value_loss           | 0.115        |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 122000: mean reward 7629.11\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 358      |\n",
            "|    ep_rew_mean     | 1.98e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 60       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=496000, episode_reward=7629.11 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.63e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 496000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007378298 |\n",
            "|    clip_fraction        | 0.0471      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.851       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.07       |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | 0.00198     |\n",
            "|    std                  | 236         |\n",
            "|    value_loss           | 0.104       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 124000: mean reward 7629.11\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 384      |\n",
            "|    ep_rew_mean     | 2.11e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 61       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 499712   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=504000, episode_reward=7480.95 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.48e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 504000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076438393 |\n",
            "|    clip_fraction        | 0.0536       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.898        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.06        |\n",
            "|    n_updates            | 740          |\n",
            "|    policy_gradient_loss | 0.00188      |\n",
            "|    std                  | 256          |\n",
            "|    value_loss           | 0.118        |\n",
            "------------------------------------------\n",
            "Evaluation at step 126000: mean reward 7480.95\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 372      |\n",
            "|    ep_rew_mean     | 2.02e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 62       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 507904   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=512000, episode_reward=7584.55 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.58e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 512000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0099991355 |\n",
            "|    clip_fraction        | 0.07         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.5        |\n",
            "|    explained_variance   | 0.88         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.13        |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.000684    |\n",
            "|    std                  | 274          |\n",
            "|    value_loss           | 0.159        |\n",
            "------------------------------------------\n",
            "Evaluation at step 128000: mean reward 7584.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 402      |\n",
            "|    ep_rew_mean     | 2.2e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 63       |\n",
            "|    time_elapsed    | 812      |\n",
            "|    total_timesteps | 516096   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=7577.39 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.58e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 520000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0090713985 |\n",
            "|    clip_fraction        | 0.0589       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.6        |\n",
            "|    explained_variance   | 0.877        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.09        |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | 0.000857     |\n",
            "|    std                  | 294          |\n",
            "|    value_loss           | 0.127        |\n",
            "------------------------------------------\n",
            "Evaluation at step 130000: mean reward 7577.39\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 398      |\n",
            "|    ep_rew_mean     | 2.2e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 64       |\n",
            "|    time_elapsed    | 825      |\n",
            "|    total_timesteps | 524288   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=528000, episode_reward=7455.74 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.46e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 528000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008653885 |\n",
            "|    clip_fraction        | 0.0577      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.888       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.12       |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | 0.00187     |\n",
            "|    std                  | 316         |\n",
            "|    value_loss           | 0.117       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 132000: mean reward 7455.74\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 415      |\n",
            "|    ep_rew_mean     | 2.29e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 65       |\n",
            "|    time_elapsed    | 838      |\n",
            "|    total_timesteps | 532480   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=536000, episode_reward=7060.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 7.06e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 536000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01594374 |\n",
            "|    clip_fraction        | 0.076      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.8      |\n",
            "|    explained_variance   | 0.842      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.13      |\n",
            "|    n_updates            | 780        |\n",
            "|    policy_gradient_loss | 0.000868   |\n",
            "|    std                  | 352        |\n",
            "|    value_loss           | 0.11       |\n",
            "----------------------------------------\n",
            "Evaluation at step 134000: mean reward 7060.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 392      |\n",
            "|    ep_rew_mean     | 2.14e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 66       |\n",
            "|    time_elapsed    | 851      |\n",
            "|    total_timesteps | 540672   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=544000, episode_reward=7182.53 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.18e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 544000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013455603 |\n",
            "|    clip_fraction        | 0.0786      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.88        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.07       |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.000225    |\n",
            "|    std                  | 390         |\n",
            "|    value_loss           | 0.141       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 136000: mean reward 7182.53\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 427      |\n",
            "|    ep_rew_mean     | 2.36e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 67       |\n",
            "|    time_elapsed    | 864      |\n",
            "|    total_timesteps | 548864   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=552000, episode_reward=5413.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 5.41e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 552000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0106190415 |\n",
            "|    clip_fraction        | 0.0604       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.1        |\n",
            "|    explained_variance   | 0.861        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.08        |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00286      |\n",
            "|    std                  | 427          |\n",
            "|    value_loss           | 0.128        |\n",
            "------------------------------------------\n",
            "Evaluation at step 138000: mean reward 5413.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 428      |\n",
            "|    ep_rew_mean     | 2.33e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 635      |\n",
            "|    iterations      | 68       |\n",
            "|    time_elapsed    | 877      |\n",
            "|    total_timesteps | 557056   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=150.75 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 151         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 560000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010980855 |\n",
            "|    clip_fraction        | 0.0636      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.2       |\n",
            "|    explained_variance   | 0.864       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.18       |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | 0.001       |\n",
            "|    std                  | 468         |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 140000: mean reward 150.75\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 429      |\n",
            "|    ep_rew_mean     | 2.3e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 69       |\n",
            "|    time_elapsed    | 890      |\n",
            "|    total_timesteps | 565248   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=568000, episode_reward=5780.56 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 5.78e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 568000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01389138 |\n",
            "|    clip_fraction        | 0.0757     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.3      |\n",
            "|    explained_variance   | 0.898      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.21      |\n",
            "|    n_updates            | 820        |\n",
            "|    policy_gradient_loss | 0.00092    |\n",
            "|    std                  | 518        |\n",
            "|    value_loss           | 0.124      |\n",
            "----------------------------------------\n",
            "Evaluation at step 142000: mean reward 5780.56\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 397      |\n",
            "|    ep_rew_mean     | 2.12e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 70       |\n",
            "|    time_elapsed    | 903      |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=576000, episode_reward=5253.09 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 5.25e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 576000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009948589 |\n",
            "|    clip_fraction        | 0.0551      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.4       |\n",
            "|    explained_variance   | 0.838       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.19       |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | 0.00227     |\n",
            "|    std                  | 565         |\n",
            "|    value_loss           | 0.129       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 144000: mean reward 5253.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 423      |\n",
            "|    ep_rew_mean     | 2.28e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 71       |\n",
            "|    time_elapsed    | 916      |\n",
            "|    total_timesteps | 581632   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=584000, episode_reward=6704.17 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 6.7e+03    |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 584000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00907784 |\n",
            "|    clip_fraction        | 0.0581     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.5      |\n",
            "|    explained_variance   | 0.866      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.21      |\n",
            "|    n_updates            | 840        |\n",
            "|    policy_gradient_loss | 0.00328    |\n",
            "|    std                  | 610        |\n",
            "|    value_loss           | 0.121      |\n",
            "----------------------------------------\n",
            "Evaluation at step 146000: mean reward 6704.17\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 408      |\n",
            "|    ep_rew_mean     | 2.2e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 72       |\n",
            "|    time_elapsed    | 928      |\n",
            "|    total_timesteps | 589824   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=592000, episode_reward=7115.59 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 7.12e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 592000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00923766 |\n",
            "|    clip_fraction        | 0.0601     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.6      |\n",
            "|    explained_variance   | 0.881      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.21      |\n",
            "|    n_updates            | 850        |\n",
            "|    policy_gradient_loss | 0.000599   |\n",
            "|    std                  | 661        |\n",
            "|    value_loss           | 0.117      |\n",
            "----------------------------------------\n",
            "Evaluation at step 148000: mean reward 7115.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 397      |\n",
            "|    ep_rew_mean     | 2.15e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 73       |\n",
            "|    time_elapsed    | 941      |\n",
            "|    total_timesteps | 598016   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=7554.79 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.55e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 600000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008572947 |\n",
            "|    clip_fraction        | 0.0566      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.7       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.2        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | 0.00125     |\n",
            "|    std                  | 717         |\n",
            "|    value_loss           | 0.147       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 150000: mean reward 7554.79\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 361      |\n",
            "|    ep_rew_mean     | 1.9e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 74       |\n",
            "|    time_elapsed    | 954      |\n",
            "|    total_timesteps | 606208   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=608000, episode_reward=7554.55 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.55e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 608000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011495396 |\n",
            "|    clip_fraction        | 0.0692      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.9       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.2        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.000887   |\n",
            "|    std                  | 786         |\n",
            "|    value_loss           | 0.152       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 152000: mean reward 7554.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 306      |\n",
            "|    ep_rew_mean     | 1.55e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 75       |\n",
            "|    time_elapsed    | 967      |\n",
            "|    total_timesteps | 614400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=616000, episode_reward=7304.90 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.3e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 616000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010696812 |\n",
            "|    clip_fraction        | 0.0695      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13         |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.2        |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.000484   |\n",
            "|    std                  | 854         |\n",
            "|    value_loss           | 0.15        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 154000: mean reward 7304.90\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 335      |\n",
            "|    ep_rew_mean     | 1.68e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 76       |\n",
            "|    time_elapsed    | 980      |\n",
            "|    total_timesteps | 622592   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=624000, episode_reward=7506.11 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.51e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 624000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009354075 |\n",
            "|    clip_fraction        | 0.0669      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.1       |\n",
            "|    explained_variance   | 0.863       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.25       |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | -0.000169   |\n",
            "|    std                  | 914         |\n",
            "|    value_loss           | 0.114       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 156000: mean reward 7506.11\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 352      |\n",
            "|    ep_rew_mean     | 1.79e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 77       |\n",
            "|    time_elapsed    | 993      |\n",
            "|    total_timesteps | 630784   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=632000, episode_reward=7402.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.4e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 632000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008620599 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.2       |\n",
            "|    explained_variance   | 0.808       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.28       |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.00275     |\n",
            "|    std                  | 988         |\n",
            "|    value_loss           | 0.121       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 158000: mean reward 7402.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 374      |\n",
            "|    ep_rew_mean     | 1.93e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 78       |\n",
            "|    time_elapsed    | 1006     |\n",
            "|    total_timesteps | 638976   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=7488.61 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.49e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 640000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011547892 |\n",
            "|    clip_fraction        | 0.0699      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.3       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.28       |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | 0.000671    |\n",
            "|    std                  | 1.08e+03    |\n",
            "|    value_loss           | 0.14        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 160000: mean reward 7488.61\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 380      |\n",
            "|    ep_rew_mean     | 1.99e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 79       |\n",
            "|    time_elapsed    | 1019     |\n",
            "|    total_timesteps | 647168   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=648000, episode_reward=7302.43 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.3e+03      |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 648000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072597326 |\n",
            "|    clip_fraction        | 0.0488       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -13.4        |\n",
            "|    explained_variance   | 0.891        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.27        |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | 0.00149      |\n",
            "|    std                  | 1.15e+03     |\n",
            "|    value_loss           | 0.124        |\n",
            "------------------------------------------\n",
            "Evaluation at step 162000: mean reward 7302.43\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 418      |\n",
            "|    ep_rew_mean     | 2.2e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 80       |\n",
            "|    time_elapsed    | 1032     |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=656000, episode_reward=7549.23 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.55e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 656000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009796913 |\n",
            "|    clip_fraction        | 0.0582      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.5       |\n",
            "|    explained_variance   | 0.882       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.27       |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | 0.00316     |\n",
            "|    std                  | 1.24e+03    |\n",
            "|    value_loss           | 0.109       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 164000: mean reward 7549.23\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 420      |\n",
            "|    ep_rew_mean     | 2.22e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 81       |\n",
            "|    time_elapsed    | 1045     |\n",
            "|    total_timesteps | 663552   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=664000, episode_reward=7067.73 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.07e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 664000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0104625365 |\n",
            "|    clip_fraction        | 0.0689       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -13.6        |\n",
            "|    explained_variance   | 0.875        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.23        |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.000369    |\n",
            "|    std                  | 1.34e+03     |\n",
            "|    value_loss           | 0.135        |\n",
            "------------------------------------------\n",
            "Evaluation at step 166000: mean reward 7067.73\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 401      |\n",
            "|    ep_rew_mean     | 2.1e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 82       |\n",
            "|    time_elapsed    | 1058     |\n",
            "|    total_timesteps | 671744   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=672000, episode_reward=7021.96 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 7.02e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 672000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073860353 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -13.7        |\n",
            "|    explained_variance   | 0.879        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.24        |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | 0.00148      |\n",
            "|    std                  | 1.43e+03     |\n",
            "|    value_loss           | 0.115        |\n",
            "------------------------------------------\n",
            "Evaluation at step 168000: mean reward 7021.96\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 422      |\n",
            "|    ep_rew_mean     | 2.18e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 634      |\n",
            "|    iterations      | 83       |\n",
            "|    time_elapsed    | 1071     |\n",
            "|    total_timesteps | 679936   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008920469 |\n",
            "|    clip_fraction        | 0.0574      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.8       |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.38       |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | 0.00229     |\n",
            "|    std                  | 1.54e+03    |\n",
            "|    value_loss           | 0.121       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 170000: mean reward 115.10\n",
            "Eval num_timesteps=688000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 115      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 688000   |\n",
            "---------------------------------\n",
            "Evaluation at step 172000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 435      |\n",
            "|    ep_rew_mean     | 2.23e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 84       |\n",
            "|    time_elapsed    | 1088     |\n",
            "|    total_timesteps | 688128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=696000, episode_reward=5790.16 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 5.79e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 696000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008082359 |\n",
            "|    clip_fraction        | 0.0486      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -13.9       |\n",
            "|    explained_variance   | 0.864       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.34       |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | 0.00145     |\n",
            "|    std                  | 1.68e+03    |\n",
            "|    value_loss           | 0.122       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 174000: mean reward 5790.16\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 404      |\n",
            "|    ep_rew_mean     | 2.1e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 85       |\n",
            "|    time_elapsed    | 1101     |\n",
            "|    total_timesteps | 696320   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=704000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 704000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010741845 |\n",
            "|    clip_fraction        | 0.0661      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -14         |\n",
            "|    explained_variance   | 0.882       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.35       |\n",
            "|    n_updates            | 980         |\n",
            "|    policy_gradient_loss | 0.000378    |\n",
            "|    std                  | 1.86e+03    |\n",
            "|    value_loss           | 0.139       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 176000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 385      |\n",
            "|    ep_rew_mean     | 2e+03    |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 86       |\n",
            "|    time_elapsed    | 1114     |\n",
            "|    total_timesteps | 704512   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=712000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 712000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070703803 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -14.1        |\n",
            "|    explained_variance   | 0.86         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.3         |\n",
            "|    n_updates            | 990          |\n",
            "|    policy_gradient_loss | 0.00139      |\n",
            "|    std                  | 1.98e+03     |\n",
            "|    value_loss           | 0.141        |\n",
            "------------------------------------------\n",
            "Evaluation at step 178000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 381      |\n",
            "|    ep_rew_mean     | 2.03e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 87       |\n",
            "|    time_elapsed    | 1127     |\n",
            "|    total_timesteps | 712704   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=4981.19 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 4.98e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 720000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0128913075 |\n",
            "|    clip_fraction        | 0.0644       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -14.2        |\n",
            "|    explained_variance   | 0.865        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.35        |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | 0.00223      |\n",
            "|    std                  | 2.22e+03     |\n",
            "|    value_loss           | 0.117        |\n",
            "------------------------------------------\n",
            "Evaluation at step 180000: mean reward 4981.19\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 339      |\n",
            "|    ep_rew_mean     | 1.82e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 88       |\n",
            "|    time_elapsed    | 1140     |\n",
            "|    total_timesteps | 720896   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=728000, episode_reward=7093.58 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.09e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 728000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008554487 |\n",
            "|    clip_fraction        | 0.0534      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -14.3       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.36       |\n",
            "|    n_updates            | 1010        |\n",
            "|    policy_gradient_loss | 3.17e-06    |\n",
            "|    std                  | 2.39e+03    |\n",
            "|    value_loss           | 0.149       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 182000: mean reward 7093.58\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 354      |\n",
            "|    ep_rew_mean     | 1.92e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 89       |\n",
            "|    time_elapsed    | 1153     |\n",
            "|    total_timesteps | 729088   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=736000, episode_reward=7226.38 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 7.23e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 736000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008496239 |\n",
            "|    clip_fraction        | 0.0528      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -14.4       |\n",
            "|    explained_variance   | 0.849       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.35       |\n",
            "|    n_updates            | 1020        |\n",
            "|    policy_gradient_loss | 0.000907    |\n",
            "|    std                  | 2.59e+03    |\n",
            "|    value_loss           | 0.119       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 184000: mean reward 7226.38\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 376      |\n",
            "|    ep_rew_mean     | 2.02e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 90       |\n",
            "|    time_elapsed    | 1166     |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=744000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 744000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071010874 |\n",
            "|    clip_fraction        | 0.0497       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -14.5        |\n",
            "|    explained_variance   | 0.879        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.38        |\n",
            "|    n_updates            | 1030         |\n",
            "|    policy_gradient_loss | 0.00214      |\n",
            "|    std                  | 2.75e+03     |\n",
            "|    value_loss           | 0.108        |\n",
            "------------------------------------------\n",
            "Evaluation at step 186000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 398      |\n",
            "|    ep_rew_mean     | 2.15e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 91       |\n",
            "|    time_elapsed    | 1179     |\n",
            "|    total_timesteps | 745472   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=752000, episode_reward=6526.23 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 6.53e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 752000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058534998 |\n",
            "|    clip_fraction        | 0.0399       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -14.6        |\n",
            "|    explained_variance   | 0.868        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.34        |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | 0.00372      |\n",
            "|    std                  | 2.94e+03     |\n",
            "|    value_loss           | 0.118        |\n",
            "------------------------------------------\n",
            "Evaluation at step 188000: mean reward 6526.23\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 387      |\n",
            "|    ep_rew_mean     | 2.08e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 92       |\n",
            "|    time_elapsed    | 1192     |\n",
            "|    total_timesteps | 753664   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=5944.90 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 5.94e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 760000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008395819 |\n",
            "|    clip_fraction        | 0.0502      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -14.7       |\n",
            "|    explained_variance   | 0.861       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.37       |\n",
            "|    n_updates            | 1050        |\n",
            "|    policy_gradient_loss | 0.000806    |\n",
            "|    std                  | 3.18e+03    |\n",
            "|    value_loss           | 0.142       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 190000: mean reward 5944.90\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 413      |\n",
            "|    ep_rew_mean     | 2.23e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 93       |\n",
            "|    time_elapsed    | 1205     |\n",
            "|    total_timesteps | 761856   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=768000, episode_reward=115.09 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 768000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010672189 |\n",
            "|    clip_fraction        | 0.0683      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -14.8       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.38       |\n",
            "|    n_updates            | 1060        |\n",
            "|    policy_gradient_loss | -0.000363   |\n",
            "|    std                  | 3.46e+03    |\n",
            "|    value_loss           | 0.122       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 192000: mean reward 115.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 436      |\n",
            "|    ep_rew_mean     | 2.29e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 94       |\n",
            "|    time_elapsed    | 1218     |\n",
            "|    total_timesteps | 770048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=776000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 115        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 776000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01008368 |\n",
            "|    clip_fraction        | 0.055      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -14.9      |\n",
            "|    explained_variance   | 0.879      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.47      |\n",
            "|    n_updates            | 1070       |\n",
            "|    policy_gradient_loss | 0.0019     |\n",
            "|    std                  | 3.81e+03   |\n",
            "|    value_loss           | 0.134      |\n",
            "----------------------------------------\n",
            "Evaluation at step 194000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 397      |\n",
            "|    ep_rew_mean     | 2.13e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 95       |\n",
            "|    time_elapsed    | 1230     |\n",
            "|    total_timesteps | 778240   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=784000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 784000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009319837 |\n",
            "|    clip_fraction        | 0.0649      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15         |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.39       |\n",
            "|    n_updates            | 1080        |\n",
            "|    policy_gradient_loss | -0.00121    |\n",
            "|    std                  | 4.11e+03    |\n",
            "|    value_loss           | 0.133       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 196000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 408      |\n",
            "|    ep_rew_mean     | 2.21e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 96       |\n",
            "|    time_elapsed    | 1244     |\n",
            "|    total_timesteps | 786432   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=792000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 792000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011046648 |\n",
            "|    clip_fraction        | 0.0557      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.1       |\n",
            "|    explained_variance   | 0.827       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.45       |\n",
            "|    n_updates            | 1090        |\n",
            "|    policy_gradient_loss | 0.00415     |\n",
            "|    std                  | 4.52e+03    |\n",
            "|    value_loss           | 0.111       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 198000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 430      |\n",
            "|    ep_rew_mean     | 2.31e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 97       |\n",
            "|    time_elapsed    | 1256     |\n",
            "|    total_timesteps | 794624   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 800000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011966423 |\n",
            "|    clip_fraction        | 0.0679      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.2       |\n",
            "|    explained_variance   | 0.862       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.47       |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | 0.000327    |\n",
            "|    std                  | 4.96e+03    |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 200000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 417      |\n",
            "|    ep_rew_mean     | 2.23e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 98       |\n",
            "|    time_elapsed    | 1269     |\n",
            "|    total_timesteps | 802816   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=808000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 808000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011044274 |\n",
            "|    clip_fraction        | 0.0603      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.3       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.54       |\n",
            "|    n_updates            | 1110        |\n",
            "|    policy_gradient_loss | 0.000738    |\n",
            "|    std                  | 5.43e+03    |\n",
            "|    value_loss           | 0.137       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 202000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 401      |\n",
            "|    ep_rew_mean     | 2.14e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 99       |\n",
            "|    time_elapsed    | 1282     |\n",
            "|    total_timesteps | 811008   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=816000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 816000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011042697 |\n",
            "|    clip_fraction        | 0.0671      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.4       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.52       |\n",
            "|    n_updates            | 1120        |\n",
            "|    policy_gradient_loss | 0.000882    |\n",
            "|    std                  | 5.91e+03    |\n",
            "|    value_loss           | 0.123       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 204000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 434      |\n",
            "|    ep_rew_mean     | 2.33e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 100      |\n",
            "|    time_elapsed    | 1295     |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=824000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 824000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067328764 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -15.5        |\n",
            "|    explained_variance   | 0.827        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.51        |\n",
            "|    n_updates            | 1130         |\n",
            "|    policy_gradient_loss | 0.00248      |\n",
            "|    std                  | 6.34e+03     |\n",
            "|    value_loss           | 0.128        |\n",
            "------------------------------------------\n",
            "Evaluation at step 206000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 419      |\n",
            "|    ep_rew_mean     | 2.22e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 101      |\n",
            "|    time_elapsed    | 1308     |\n",
            "|    total_timesteps | 827392   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=832000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 832000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009868728 |\n",
            "|    clip_fraction        | 0.0567      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.7       |\n",
            "|    explained_variance   | 0.817       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.53       |\n",
            "|    n_updates            | 1140        |\n",
            "|    policy_gradient_loss | 0.00392     |\n",
            "|    std                  | 6.88e+03    |\n",
            "|    value_loss           | 0.128       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 208000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 412      |\n",
            "|    ep_rew_mean     | 2.2e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 102      |\n",
            "|    time_elapsed    | 1321     |\n",
            "|    total_timesteps | 835584   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 840000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010188075 |\n",
            "|    clip_fraction        | 0.0571      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -15.8       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.5        |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | 0.000702    |\n",
            "|    std                  | 7.44e+03    |\n",
            "|    value_loss           | 0.123       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 210000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 426      |\n",
            "|    ep_rew_mean     | 2.26e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 103      |\n",
            "|    time_elapsed    | 1334     |\n",
            "|    total_timesteps | 843776   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=848000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 848000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083880685 |\n",
            "|    clip_fraction        | 0.0565       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -15.9        |\n",
            "|    explained_variance   | 0.895        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.51        |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | 8.17e-05     |\n",
            "|    std                  | 7.95e+03     |\n",
            "|    value_loss           | 0.123        |\n",
            "------------------------------------------\n",
            "Evaluation at step 212000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 423      |\n",
            "|    ep_rew_mean     | 2.26e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 104      |\n",
            "|    time_elapsed    | 1347     |\n",
            "|    total_timesteps | 851968   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=856000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 115        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 856000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01612189 |\n",
            "|    clip_fraction        | 0.0707     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -16        |\n",
            "|    explained_variance   | 0.896      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.57      |\n",
            "|    n_updates            | 1170       |\n",
            "|    policy_gradient_loss | 0.0019     |\n",
            "|    std                  | 8.87e+03   |\n",
            "|    value_loss           | 0.128      |\n",
            "----------------------------------------\n",
            "Evaluation at step 214000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 407      |\n",
            "|    ep_rew_mean     | 2.13e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 105      |\n",
            "|    time_elapsed    | 1360     |\n",
            "|    total_timesteps | 860160   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=864000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 864000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012361737 |\n",
            "|    clip_fraction        | 0.0756      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.1       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.58       |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.000685   |\n",
            "|    std                  | 9.74e+03    |\n",
            "|    value_loss           | 0.145       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 216000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 406      |\n",
            "|    ep_rew_mean     | 2.16e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 106      |\n",
            "|    time_elapsed    | 1373     |\n",
            "|    total_timesteps | 868352   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=872000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 872000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014048444 |\n",
            "|    clip_fraction        | 0.0704      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.2       |\n",
            "|    explained_variance   | 0.85        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.56       |\n",
            "|    n_updates            | 1190        |\n",
            "|    policy_gradient_loss | 0.00188     |\n",
            "|    std                  | 1.09e+04    |\n",
            "|    value_loss           | 0.113       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 218000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 393      |\n",
            "|    ep_rew_mean     | 2.04e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 107      |\n",
            "|    time_elapsed    | 1386     |\n",
            "|    total_timesteps | 876544   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 880000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013253216 |\n",
            "|    clip_fraction        | 0.0687      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.4       |\n",
            "|    explained_variance   | 0.891       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.54       |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | 0.000277    |\n",
            "|    std                  | 1.2e+04     |\n",
            "|    value_loss           | 0.15        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 220000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 417      |\n",
            "|    ep_rew_mean     | 2.15e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 108      |\n",
            "|    time_elapsed    | 1399     |\n",
            "|    total_timesteps | 884736   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=888000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 888000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010134002 |\n",
            "|    clip_fraction        | 0.0552      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.5       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.6        |\n",
            "|    n_updates            | 1210        |\n",
            "|    policy_gradient_loss | 0.0015      |\n",
            "|    std                  | 1.32e+04    |\n",
            "|    value_loss           | 0.126       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 222000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 394      |\n",
            "|    ep_rew_mean     | 2.06e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 109      |\n",
            "|    time_elapsed    | 1412     |\n",
            "|    total_timesteps | 892928   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=896000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 896000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007922547 |\n",
            "|    clip_fraction        | 0.0513      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.6       |\n",
            "|    explained_variance   | 0.894       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.6        |\n",
            "|    n_updates            | 1220        |\n",
            "|    policy_gradient_loss | 0.00084     |\n",
            "|    std                  | 1.41e+04    |\n",
            "|    value_loss           | 0.133       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 224000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 397      |\n",
            "|    ep_rew_mean     | 2.11e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 110      |\n",
            "|    time_elapsed    | 1424     |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=904000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 904000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0080104135 |\n",
            "|    clip_fraction        | 0.0489       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -16.7        |\n",
            "|    explained_variance   | 0.848        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.64        |\n",
            "|    n_updates            | 1230         |\n",
            "|    policy_gradient_loss | 0.00423      |\n",
            "|    std                  | 1.52e+04     |\n",
            "|    value_loss           | 0.119        |\n",
            "------------------------------------------\n",
            "Evaluation at step 226000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 399      |\n",
            "|    ep_rew_mean     | 2.08e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 111      |\n",
            "|    time_elapsed    | 1437     |\n",
            "|    total_timesteps | 909312   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=912000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 912000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008559946 |\n",
            "|    clip_fraction        | 0.0502      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.8       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.64       |\n",
            "|    n_updates            | 1240        |\n",
            "|    policy_gradient_loss | 0.00285     |\n",
            "|    std                  | 1.64e+04    |\n",
            "|    value_loss           | 0.15        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 228000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 373      |\n",
            "|    ep_rew_mean     | 1.99e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 112      |\n",
            "|    time_elapsed    | 1450     |\n",
            "|    total_timesteps | 917504   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 920000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009016846 |\n",
            "|    clip_fraction        | 0.0599      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -16.9       |\n",
            "|    explained_variance   | 0.889       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.59       |\n",
            "|    n_updates            | 1250        |\n",
            "|    policy_gradient_loss | -0.00173    |\n",
            "|    std                  | 1.74e+04    |\n",
            "|    value_loss           | 0.164       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 230000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 368      |\n",
            "|    ep_rew_mean     | 1.97e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 113      |\n",
            "|    time_elapsed    | 1463     |\n",
            "|    total_timesteps | 925696   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=928000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | 115        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 928000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00922954 |\n",
            "|    clip_fraction        | 0.0508     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -17        |\n",
            "|    explained_variance   | 0.866      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.62      |\n",
            "|    n_updates            | 1260       |\n",
            "|    policy_gradient_loss | 0.00151    |\n",
            "|    std                  | 1.91e+04   |\n",
            "|    value_loss           | 0.151      |\n",
            "----------------------------------------\n",
            "Evaluation at step 232000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 395      |\n",
            "|    ep_rew_mean     | 2.1e+03  |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 114      |\n",
            "|    time_elapsed    | 1476     |\n",
            "|    total_timesteps | 933888   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=936000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 936000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0117684025 |\n",
            "|    clip_fraction        | 0.0639       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -17.1        |\n",
            "|    explained_variance   | 0.888        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.64        |\n",
            "|    n_updates            | 1270         |\n",
            "|    policy_gradient_loss | 0.00303      |\n",
            "|    std                  | 2.12e+04     |\n",
            "|    value_loss           | 0.12         |\n",
            "------------------------------------------\n",
            "Evaluation at step 234000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 381      |\n",
            "|    ep_rew_mean     | 2.04e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 115      |\n",
            "|    time_elapsed    | 1489     |\n",
            "|    total_timesteps | 942080   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=944000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 944000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010085571 |\n",
            "|    clip_fraction        | 0.0588      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -17.2       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.48       |\n",
            "|    n_updates            | 1280        |\n",
            "|    policy_gradient_loss | 6.1e-05     |\n",
            "|    std                  | 2.31e+04    |\n",
            "|    value_loss           | 0.141       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 236000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 387      |\n",
            "|    ep_rew_mean     | 2.09e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 116      |\n",
            "|    time_elapsed    | 1502     |\n",
            "|    total_timesteps | 950272   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=952000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 952000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065487744 |\n",
            "|    clip_fraction        | 0.0386       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -17.3        |\n",
            "|    explained_variance   | 0.863        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.68        |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | 0.00141      |\n",
            "|    std                  | 2.47e+04     |\n",
            "|    value_loss           | 0.145        |\n",
            "------------------------------------------\n",
            "Evaluation at step 238000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 420      |\n",
            "|    ep_rew_mean     | 2.23e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 117      |\n",
            "|    time_elapsed    | 1515     |\n",
            "|    total_timesteps | 958464   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 960000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007111702 |\n",
            "|    clip_fraction        | 0.0394      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -17.4       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.67       |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | 0.00186     |\n",
            "|    std                  | 2.65e+04    |\n",
            "|    value_loss           | 0.132       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 240000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 430      |\n",
            "|    ep_rew_mean     | 2.26e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 118      |\n",
            "|    time_elapsed    | 1528     |\n",
            "|    total_timesteps | 966656   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=968000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 968000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011320766 |\n",
            "|    clip_fraction        | 0.0628      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -17.5       |\n",
            "|    explained_variance   | 0.888       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.67       |\n",
            "|    n_updates            | 1310        |\n",
            "|    policy_gradient_loss | 0.000638    |\n",
            "|    std                  | 2.92e+04    |\n",
            "|    value_loss           | 0.125       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 242000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 442      |\n",
            "|    ep_rew_mean     | 2.31e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 119      |\n",
            "|    time_elapsed    | 1541     |\n",
            "|    total_timesteps | 974848   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=976000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 976000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073776077 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -17.6        |\n",
            "|    explained_variance   | 0.868        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.69        |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | 0.00326      |\n",
            "|    std                  | 3.12e+04     |\n",
            "|    value_loss           | 0.114        |\n",
            "------------------------------------------\n",
            "Evaluation at step 244000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 447      |\n",
            "|    ep_rew_mean     | 2.37e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 120      |\n",
            "|    time_elapsed    | 1553     |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=984000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 115          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 984000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064058737 |\n",
            "|    clip_fraction        | 0.0431       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -17.7        |\n",
            "|    explained_variance   | 0.846        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.7         |\n",
            "|    n_updates            | 1330         |\n",
            "|    policy_gradient_loss | 0.00291      |\n",
            "|    std                  | 3.32e+04     |\n",
            "|    value_loss           | 0.135        |\n",
            "------------------------------------------\n",
            "Evaluation at step 246000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 471      |\n",
            "|    ep_rew_mean     | 2.49e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 121      |\n",
            "|    time_elapsed    | 1566     |\n",
            "|    total_timesteps | 991232   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=992000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 992000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010627419 |\n",
            "|    clip_fraction        | 0.0619      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -17.8       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.62       |\n",
            "|    n_updates            | 1340        |\n",
            "|    policy_gradient_loss | 0.00164     |\n",
            "|    std                  | 3.66e+04    |\n",
            "|    value_loss           | 0.135       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 248000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 444      |\n",
            "|    ep_rew_mean     | 2.38e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 122      |\n",
            "|    time_elapsed    | 1579     |\n",
            "|    total_timesteps | 999424   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=115.10 +/- 0.00\n",
            "Episode length: 1001.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 115         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 1000000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006081367 |\n",
            "|    clip_fraction        | 0.0395      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -17.9       |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.69       |\n",
            "|    n_updates            | 1350        |\n",
            "|    policy_gradient_loss | 0.00167     |\n",
            "|    std                  | 3.87e+04    |\n",
            "|    value_loss           | 0.168       |\n",
            "-----------------------------------------\n",
            "Evaluation at step 250000: mean reward 115.10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 431      |\n",
            "|    ep_rew_mean     | 2.28e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 632      |\n",
            "|    iterations      | 123      |\n",
            "|    time_elapsed    | 1593     |\n",
            "|    total_timesteps | 1007616  |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load and train\n",
        "ent_coef=0.1\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=1000000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "71c7d9ee-a0bc-4a9a-81d3-3515602701b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: -78.79866129999999 +/- 133.82650038538142\n"
          ]
        }
      ],
      "source": [
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "882ed470-5bd7-48e2-ba77-872bba1eb3e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PPO' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7e30db269de0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the trained model and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a new environment for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCustomPongEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputer_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mComputerPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PPO' is not defined"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer())])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 2:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    # print(obs, action)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "      env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbn1ZY4GGMZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-hPY2KxRxsE",
        "outputId": "409b1ca5-e574-479e-c62f-84f66515be4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Actions: [[ 0.05661448 -0.16774732]]\n",
            "ONNX Values: [[-0.05518252]]\n",
            "ONNX Log Prob: [-3.9599395]\n",
            "PyTorch Actions: [[ 0.05661447 -0.1677474 ]]\n",
            "PyTorch Values: [[-0.0551825]]\n",
            "PyTorch Log Prob: [-3.9599395]\n",
            "Actions match: True\n",
            "Values match: True\n",
            "Log prob match: True\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        # Run the policy in deterministic mode\n",
        "        actions, values, log_prob = self.policy(observation, deterministic=True)\n",
        "        return actions, values, log_prob\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\"\n",
        "model = PPO.load(model_path)\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "\n",
        "# Define dummy input based on the observation space shape\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1.onnx\"\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Create an ONNX runtime session\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "ort_inputs = {\"input\": observation}\n",
        "ort_outputs = ort_sess.run(None, ort_inputs)\n",
        "\n",
        "# Output from ONNX\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_outputs\n",
        "\n",
        "# Print ONNX outputs\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "# Check that the predictions are the same in PyTorch\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "\n",
        "# Print PyTorch outputs\n",
        "print(\"PyTorch Actions:\", pytorch_outputs[0].numpy())\n",
        "print(\"PyTorch Values:\", pytorch_outputs[1].numpy())\n",
        "print(\"PyTorch Log Prob:\", pytorch_outputs[2].numpy())\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = [output.numpy() for output in pytorch_outputs]\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "\n",
        "    actions_match = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match: {actions_match}\")\n",
        "    print(f\"Values match: {values_match}\")\n",
        "    print(f\"Log prob match: {log_prob_match}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(pytorch_outputs, ort_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx2tf import convert\n",
        "\n",
        "# Define paths\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Convert ONNX to TensorFlow SavedModel\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPSegjRq1cOI",
        "outputId": "fa8d3822-4eed-4746-bd7e-f9a0b9a6c98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnx2tf/onnx2tf.py\", line 631, in convert\n",
            "    result = subprocess.check_output(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
            "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n",
            "\n",
            "\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Cast\u001b[35m onnx_op_name\u001b[0m: wa/policy/Cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.dtype\u001b[0m: \u001b[34mname\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Exp\u001b[35m onnx_op_name\u001b[0m: wa/policy/Exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: policy.log_std \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/policy/features_extractor/flatten/Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 8] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_22/Reshape:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Constant_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_66/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_67/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_66/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_44/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_67/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_45/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_68/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_69/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_68/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_46/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_69/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_47/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/value_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.value_net.weight \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.value_net.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: values \u001b[36mshape\u001b[0m: [1, 1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_70/AddV2:0 \u001b[34mshape\u001b[0m: (1, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_33/wa/policy/Shape:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ConstantOfShape\u001b[35m onnx_op_name\u001b[0m: wa/policy/ConstantOfShape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: constant\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.value\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_11/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_11/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_32/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_32/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_34/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m27 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_33/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_35/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m28 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_71/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_34/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m29 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_35/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m30 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_22/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m31 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Log\u001b[35m onnx_op_name\u001b[0m: wa/policy/Log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_215/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_11/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m32 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_33/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m33 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_5_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_214/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_23/Reshape:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m34 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_2_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_33/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_23/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m35 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_3_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_22/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_218/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m36 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Neg\u001b[35m onnx_op_name\u001b[0m: wa/policy/Neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_23/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_11/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m37 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/policy/Div\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_11/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_218/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_11/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m38 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_11/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_11/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_34/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m39 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_4_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_34/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_35/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m40 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceSum\u001b[35m onnx_op_name\u001b[0m: wa/policy/ReduceSum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: log_prob \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_sum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_35/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_sum_23/Sum:0 \u001b[34mshape\u001b[0m: (None,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.engine.functional.Functional at 0x7d035f42fdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnaMw0pAURsM",
        "outputId": "fb2b8e87-dc43-497e-f332-98dd0f8f6b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow.js model saved at: /content/drive/MyDrive/wimblepong/24-monday/ppo_custom_pong_1_tfjs\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tfjs\"\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow.js\n",
        "subprocess.run([\n",
        "    'tensorflowjs_converter',\n",
        "    '--input_format', 'tf_saved_model',\n",
        "    '--output_format', 'tfjs_graph_model',\n",
        "    \"--signature_name\", \"serving_default\",\n",
        "    tf_model_path,\n",
        "    tfjs_model_path\n",
        "])\n",
        "\n",
        "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1.onnx\"\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        return self.policy(observation, deterministic=True)\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Convert ONNX to TensorFlow\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True\n",
        ")\n",
        "\n",
        "# Load the TensorFlow model\n",
        "tf_model = tf.saved_model.load(tf_model_path)\n",
        "\n",
        "# Function to run inference on TensorFlow model\n",
        "def run_tf_model(tf_model, input_data):\n",
        "    concrete_func = tf_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "    input_tensor = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
        "    output_dict = concrete_func(input_tensor)\n",
        "    actions = output_dict['actions'].numpy()\n",
        "    values = output_dict['values'].numpy()\n",
        "    log_prob = output_dict['log_prob'].numpy()\n",
        "    return actions, values, log_prob\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Run inference with TensorFlow model\n",
        "tf_actions, tf_values, tf_log_prob = run_tf_model(tf_model, observation)\n",
        "\n",
        "# Run inference with ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_sess.run(None, {\"input\": observation})\n",
        "\n",
        "# Run inference with PyTorch model\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "    pytorch_actions = pytorch_outputs[0].numpy()\n",
        "    pytorch_values = pytorch_outputs[1].numpy()\n",
        "    pytorch_log_prob = pytorch_outputs[2].numpy()\n",
        "\n",
        "# Print Outputs\n",
        "print(\"PyTorch Actions:\", pytorch_actions)\n",
        "print(\"PyTorch Values:\", pytorch_values)\n",
        "print(\"PyTorch Log Prob:\", pytorch_log_prob)\n",
        "\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "print(\"TensorFlow Actions:\", tf_actions)\n",
        "print(\"TensorFlow Values:\", tf_values)\n",
        "print(\"TensorFlow Log Prob:\", tf_log_prob)\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs, tf_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = pytorch_outputs\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "    tf_actions, tf_values, tf_log_prob = tf_outputs\n",
        "\n",
        "    actions_match_pytorch_onnx = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match_pytorch_onnx = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match_pytorch_onnx = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    actions_match_pytorch_tf = np.allclose(pytorch_actions, tf_actions, atol=1e-5)\n",
        "    values_match_pytorch_tf = np.allclose(pytorch_values, tf_values, atol=1e-5)\n",
        "    log_prob_match_pytorch_tf = np.allclose(pytorch_log_prob, tf_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match (PyTorch vs ONNX): {actions_match_pytorch_onnx}\")\n",
        "    print(f\"Values match (PyTorch vs ONNX): {values_match_pytorch_onnx}\")\n",
        "    print(f\"Log prob match (PyTorch vs ONNX): {log_prob_match_pytorch_onnx}\")\n",
        "\n",
        "    print(f\"Actions match (PyTorch vs TensorFlow): {actions_match_pytorch_tf}\")\n",
        "    print(f\"Values match (PyTorch vs TensorFlow): {values_match_pytorch_tf}\")\n",
        "    print(f\"Log prob match (PyTorch vs TensorFlow): {log_prob_match_pytorch_tf}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(\n",
        "    (pytorch_actions, pytorch_values, pytorch_log_prob),\n",
        "    (onnx_actions, onnx_values, onnx_log_prob),\n",
        "    (tf_actions, tf_values, tf_log_prob)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UazEUD5K0s1",
        "outputId": "d436ce8f-1f2b-439d-b70f-2df9ab7909ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnx2tf/onnx2tf.py\", line 631, in convert\n",
            "    result = subprocess.check_output(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
            "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n",
            "\n",
            "\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 8] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Cast\u001b[35m onnx_op_name\u001b[0m: wa/policy/Cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: cast\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.dtype\u001b[0m: \u001b[34mname\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Exp\u001b[35m onnx_op_name\u001b[0m: wa/policy/Exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: policy.log_std \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: exp\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/policy/features_extractor/flatten/Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Cast_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 8] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_26/Reshape:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Constant_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Exp_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_78/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/features_extractor/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.0.weight \u001b[36mshape\u001b[0m: [64, 8] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.0.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 8) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (8, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_79/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_78/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_52/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.0/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_79/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_53/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.policy_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.policy_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_80/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.1/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.mlp_extractor.value_net.2.weight \u001b[36mshape\u001b[0m: [64, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.mlp_extractor.value_net.2.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_81/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_80/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_54/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Tanh\u001b[35m onnx_op_name\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.2/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: tanh\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_81/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.tanh_55/Tanh:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/value_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/value_net/value_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.value_net.weight \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.value_net.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: values \u001b[36mshape\u001b[0m: [1, 1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_82/AddV2:0 \u001b[34mshape\u001b[0m: (1, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/policy/action_net/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/mlp_extractor/policy_net/policy_net.3/Tanh_output_0 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: policy.action_net.weight \u001b[36mshape\u001b[0m: [2, 64] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: policy.action_net.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_39/wa/policy/Shape:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ConstantOfShape\u001b[35m onnx_op_name\u001b[0m: wa/policy/ConstantOfShape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Shape_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: constant\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.value\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_13/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.fill_13/Fill:0 \u001b[34mshape\u001b[0m: (None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_36/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/policy/Add_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_36/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_40/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m27 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Shape_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_37/Add:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_41/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m28 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/action_net/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_1_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_83/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_40/wa/policy/Shape_1:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m29 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/policy/Expand_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Mul_output_0 \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Shape_2_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_41/wa/policy/Shape_2:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m30 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_26/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m31 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Log\u001b[35m onnx_op_name\u001b[0m: wa/policy/Log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: log\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_247/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_13/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m32 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_39/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m33 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/policy/Reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_5_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: actions \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_246/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_27/Reshape:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m34 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/policy/Pow_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_2_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_39/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_27/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m35 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/policy/Mul_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_3_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_26/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_250/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m36 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Neg\u001b[35m onnx_op_name\u001b[0m: wa/policy/Neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Pow_1_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: neg\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.pow_27/Pow:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_13/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m37 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/policy/Div\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Neg_output_0 \u001b[36mshape\u001b[0m: ['unk__2', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Mul_1_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.negative_13/Neg:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_250/Mul:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_13/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m38 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_1\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__5', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Log_output_0 \u001b[36mshape\u001b[0m: ['unk__3', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide_13/truediv:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.log_13/Log:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_40/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m39 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/policy/Sub_2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_1_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/policy/Constant_4_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_40/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_41/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m40 / 40\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceSum\u001b[35m onnx_op_name\u001b[0m: wa/policy/ReduceSum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/policy/Sub_2_output_0 \u001b[36mshape\u001b[0m: ['unk__6', 2] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: log_prob \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_sum\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract_41/Sub:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_sum_27/Sum:0 \u001b[34mshape\u001b[0m: (None,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n",
            "PyTorch Actions: [[ 0.05661447 -0.1677474 ]]\n",
            "PyTorch Values: [[-0.0551825]]\n",
            "PyTorch Log Prob: [-3.9599395]\n",
            "ONNX Actions: [[ 0.05661448 -0.16774732]]\n",
            "ONNX Values: [[-0.05518252]]\n",
            "ONNX Log Prob: [-3.9599395]\n",
            "TensorFlow Actions: [[ 0.05661446 -0.16774729]]\n",
            "TensorFlow Values: [[-0.05518243]]\n",
            "TensorFlow Log Prob: [-3.9599395]\n",
            "Actions match (PyTorch vs ONNX): True\n",
            "Values match (PyTorch vs ONNX): True\n",
            "Log prob match (PyTorch vs ONNX): True\n",
            "Actions match (PyTorch vs TensorFlow): True\n",
            "Values match (PyTorch vs TensorFlow): True\n",
            "Log prob match (PyTorch vs TensorFlow): True\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}