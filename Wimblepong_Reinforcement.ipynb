{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "29d77db7-cb90-4f77-bddf-3fb0ed776ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=9b5dd6d2cf658b5b1ac0a5459c001c486daacf751e7e2cee348c0e8438ef0c52\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.1)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.0 ydf-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup Google Colab Environment\n",
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install imageio pillow\n",
        "%pip install tensorflowjs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYCJyx3kJikU",
        "outputId": "f807ed84-6dbc-4f72-d3e7-c8c74dd7cdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"6-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "87028a1b-c5cd-4a72-a4cd-7974c84dc714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/skimage/util/dtype.py:27: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'blue', 'Player2': 'red'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    # \"conceed_point\": lambda ball, paddle, rally_length: -0.1,\n",
        "    \"serve\": lambda ball_speed: ball_speed / 10,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -60]), high=np.array([1, 60]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -np.inf, -np.inf, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([COURT_WIDTH, COURT_HEIGHT, np.inf, np.inf, COURT_WIDTH, COURT_HEIGHT, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "          #  {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "\n",
        "        self.starting_state_index = 0\n",
        "        self.serve_delay = 50\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 15\n",
        "\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "\n",
        "        self.is_done = False\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"Environment reset\")\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        computer = starting_state['opponent']\n",
        "        player = starting_state['player']\n",
        "\n",
        "        self.game_state = {\n",
        "            'server': server,\n",
        "            'positions_reversed': positions_reversed,\n",
        "            'opponent': computer,\n",
        "            'player': player,\n",
        "            Player.Player1: {'x': 0, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "            Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "            'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "            'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 30 * np.random.rand()\n",
        "        self.serve_delay = 100 * np.random.rand()\n",
        "        self.direction = self.direction if np.random.rand() > 0.5 else -self.direction\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.is_done = False\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "            self.game_state[Player.Player2]['x'] = 0\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = 0\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius']\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction}\n",
        "        computer_player_actions = self.get_computer_player_actions(self.game_state['opponent'])\n",
        "        actions = {self.game_state['opponent']: computer_player_actions, self.game_state['player']: model_player_actions}\n",
        "        reward = self.update_game_state(actions, 2.5)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        terminated = self.check_done()\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def update_game_state(self, actions, delta_time):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        model_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            self.is_done = True\n",
        "            return 0.01\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            if serving_from_left:\n",
        "                ball['x'] = game_state[server]['width'] + ball['radius']\n",
        "            else:\n",
        "                ball['x'] = COURT_WIDTH - game_state[server]['width'] - ball['radius']\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if model_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward\n",
        "\n",
        "    def get_computer_player_actions(self, player):\n",
        "        state = self.game_state\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4 ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "\n",
        "    def bounded_value(self, value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x']),\n",
        "            float(state['ball']['y']),\n",
        "            float(state['ball']['dx']),\n",
        "            float(state['ball']['dy']),\n",
        "            float(paddle['x']),\n",
        "            float(paddle['y']),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(is_server),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def check_done(self):\n",
        "        if self.game_state['stats']['rally_length'] > 100:\n",
        "            return True\n",
        "        if self.step_count > 1000:\n",
        "            return True\n",
        "        return self.is_done\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "outputId": "f38b3a31-3151-4c4c-a9f5-2efb05f4b089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: [[0.01       0.00999997 0.00995036 0.00995036 0.         0.00999996\n",
            "  0.00707071 0.        ]]\n",
            "Action taken: [ 0.8282336 18.14778  ]\n",
            "Observation: [[ 0.00707107  0.00707105  0.00703597 -0.999949    0.         -0.99397427\n",
            "   0.00499969  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [  0.5639987 -57.280445 ]\n",
            "Observation: [[ 0.0057735   0.8229626   0.00574484 -0.52322984  0.          1.3439206\n",
            "   0.00408214  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [  0.5399142 -27.077219 ]\n",
            "Observation: [[ 0.005       1.4517182   0.00497518 -0.1971278   0.          1.3474191\n",
            "   0.00353516  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [ 0.9643562 23.3147   ]\n",
            "Observation: [[0.00447213 1.6718895  0.00444993 0.07512181 0.         0.5504365\n",
            "  0.00316187 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [  0.19304404 -49.277435  ]\n",
            "Observation: [[0.00408248 1.7642368  0.00406221 0.3046841  0.         1.477877\n",
            "  0.00288631 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [ 0.32973075 37.437447  ]\n",
            "Observation: [[0.00377964 1.8123935  0.00376087 0.49109027 0.         0.4322442\n",
            "  0.00267214 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [ 0.64543164 54.60348   ]\n",
            "Observation: [[ 0.00353553  1.8402551   0.00351797  0.6354942   0.         -0.97703457\n",
            "   0.00249949  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [  0.0585899 -45.339985 ]\n",
            "Observation: [[0.00333333 1.8568907  0.00331678 0.7423917  0.         0.28767502\n",
            "  0.00235649 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [ 0.7185063 -5.436562 ]\n",
            "Observation: [[0.00316228 1.8666123  0.00314657 0.8181428  0.         0.422738\n",
            "  0.0022355  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [  0.46407577 -45.299137  ]\n",
            "Observation: [[0.00301511 1.8717965  0.00300013 0.86931175 0.         1.5309436\n",
            "  0.00213142 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [  0.9164338 -10.333415 ]\n",
            "Observation: [[0.00288675 1.8738937  0.00287241 0.90170836 0.         1.5469556\n",
            "  0.00204063 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [ 0.59532756 -7.257145  ]\n",
            "Observation: [[0.0027735  1.8738408  0.00275972 0.9200606  0.         1.50474\n",
            "  0.00196052 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [ 0.28073373 51.465664  ]\n",
            "Observation: [[0.00267261 1.8445687  0.00265933 0.6171377  0.         0.22054602\n",
            "  0.00188916 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [ 0.4463708 26.944227 ]\n",
            "Observation: [[ 0.00258199  1.7441883   0.00256916 -0.40554076  0.         -0.4517995\n",
            "   0.00182505  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [ 0.35567147 16.388573  ]\n",
            "Observation: [[ 0.0025      1.6131577   0.00248757 -1.1944416   0.         -0.8358247\n",
            "   0.00176706  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [  0.12320609 -16.457907  ]\n",
            "Observation: [[ 0.00242536  1.4665315   0.0024133  -1.691565    0.         -0.38438338\n",
            "   0.00171425  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [ 0.24042319 19.351013  ]\n",
            "Observation: [[ 2.3570217e-03  1.3092027e+00  2.3453042e-03 -1.9381622e+00\n",
            "   0.0000000e+00 -8.6695898e-01  1.6659149e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [  0.18655924 -17.959087  ]\n",
            "Observation: [[ 2.2941565e-03  1.1418592e+00  2.2827503e-03 -2.0253127e+00\n",
            "   0.0000000e+00 -3.6259809e-01  1.6214420e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [  0.96790594 -41.0301    ]\n",
            "Observation: [[ 2.2360671e-03  9.6338445e-01  2.2249487e-03 -2.0267417e+00\n",
            "   0.0000000e+00  7.5871706e-01  1.5803468e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [0.7017081 3.5854235]\n",
            "Observation: [[ 2.1821782e-03  7.7192587e-01  2.1713264e-03 -1.9861789e+00\n",
            "   0.0000000e+00  6.3459569e-01  1.5422222e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [  0.77030826 -41.90408   ]\n",
            "Observation: [[ 2.1320065e-03  5.6550777e-01  2.1214033e-03 -1.9266965e+00\n",
            "   0.0000000e+00  1.6746700e+00  1.5067265e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [  0.3094432 -45.981106 ]\n",
            "Observation: [[ 2.0851435e-03  3.4255925e-01  2.0747723e-03 -1.8599910e+00\n",
            "   0.0000000e+00  2.1400669e+00  1.4735709e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [ 0.7942574 32.457573 ]\n",
            "Observation: [[ 2.0412407e-03  1.0249945e-01  2.0310869e-03 -1.7919060e+00\n",
            "   0.0000000e+00  1.2562977e+00  1.4425089e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [ 0.6075863 26.30358  ]\n",
            "Observation: [[ 1.9999992e-03 -1.5356228e-01  1.9900496e-03 -1.7253144e+00\n",
            "   0.0000000e+00  5.9224224e-01  1.4133290e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [  0.8884083 -36.810303 ]\n",
            "Observation: [[ 1.9611607e-03 -4.2213637e-01  1.9514033e-03 -1.6615701e+00\n",
            "   0.0000000e+00  1.4198260e+00  1.3858486e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [0.0472354 3.3264544]\n",
            "Observation: [[ 1.9245002e-03 -6.9692588e-01  1.9149244e-03 -1.6012443e+00\n",
            "   0.0000000e+00  1.2721534e+00  1.3599087e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [  0.43109235 -19.13167   ]\n",
            "Observation: [[ 1.8898217e-03 -9.6905267e-01  1.8804175e-03 -1.5445025e+00\n",
            "   0.0000000e+00  1.6168070e+00  1.3353706e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [ 0.8802128 59.96401  ]\n",
            "Observation: [[ 1.8569527e-03 -1.2281874e+00  1.8477112e-03 -1.4913013e+00\n",
            "   0.0000000e+00  2.1646821e-01  1.3121122e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [ 0.66132665 24.251528  ]\n",
            "Observation: [[ 1.8257411e-03 -1.4644380e+00  1.8166540e-03 -1.4414941e+00\n",
            "   0.0000000e+00 -3.5100642e-01  1.2900262e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [ 0.7641296 47.238796 ]\n",
            "Observation: [[ 1.7960523e-03 -1.6703099e+00  1.7871121e-03 -1.3948866e+00\n",
            "   0.0000000e+00 -1.4126148e+00  1.2690171e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [  0.50430167 -59.638405  ]\n",
            "Observation: [[ 1.7677663e-03 -1.8419286e+00  1.7589660e-03 -1.3512671e+00\n",
            "   0.0000000e+00 -4.9078935e-03  1.2490002e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [ 0.73018974 20.362211  ]\n",
            "Observation: [[ 1.7407759e-03 -1.9791149e+00  1.7321091e-03 -1.3104233e+00\n",
            "   0.0000000e+00 -4.8389071e-01  1.2298997e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [ 0.1464558 39.35491  ]\n",
            "Observation: [[ 1.7149852e-03 -2.0845344e+00  1.7064459e-03 -1.2721496e+00\n",
            "   0.0000000e+00 -1.3761669e+00  1.2116478e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [ 0.8413804 37.923004 ]\n",
            "Observation: [[ 1.6903079e-03 -2.1624827e+00  1.6818907e-03 -1.2362522e+00\n",
            "   0.0000000e+00 -2.0992098e+00  1.1941832e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [  0.5088845 -49.014156 ]\n",
            "Observation: [[ 1.6666660e-03 -2.2177980e+00  1.6583657e-03 -1.2025506e+00\n",
            "   0.0000000e+00 -9.6300542e-01  1.1774512e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [ 0.19393054 45.637917  ]\n",
            "Observation: [[ 1.6439892e-03 -2.2551239e+00  1.6358010e-03 -1.1708773e+00\n",
            "   0.0000000e+00 -1.8712208e+00  1.1614017e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [  0.3590923 -38.29839  ]\n",
            "Observation: [[ 1.6222136e-03 -2.2785258e+00  1.6141331e-03 -1.1410784e+00\n",
            "   0.0000000e+00 -1.0078856e+00  1.1459898e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [ 0.6982691 16.93639  ]\n",
            "Observation: [[ 1.6012810e-03 -2.2913666e+00  1.5933039e-03 -1.1130118e+00\n",
            "   0.0000000e+00 -1.3274773e+00  1.1311739e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [  0.10004698 -10.078791  ]\n",
            "Observation: [[ 1.5811382e-03 -2.2963235e+00  1.5732608e-03 -1.0865481e+00\n",
            "   0.0000000e+00 -1.0796171e+00  1.1169169e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [ 0.05138843 24.712902  ]\n",
            "Observation: [[ 1.5617370e-03 -2.2861414e+00  1.5539555e-03 -9.8343801e-01\n",
            "   0.0000000e+00 -1.5434289e+00  1.1031844e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [ 0.08013506 -9.784258  ]\n",
            "Observation: [[ 1.5430329e-03 -2.2244804e+00  1.5353438e-03 -5.2116776e-01\n",
            "   0.0000000e+00 -1.2935085e+00  1.0899450e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [ 0.85945714 -1.3182862 ]\n",
            "Observation: [[ 1.5249852e-03 -2.1328459e+00  1.5173851e-03 -1.3769242e-01\n",
            "   0.0000000e+00 -1.2284176e+00  1.0771698e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [ 0.85410506 12.912695  ]\n",
            "Observation: [[ 1.5075562e-03 -2.0242293e+00 -6.5573616e+00  1.8200399e-01\n",
            "   0.0000000e+00 -1.4436206e+00 -6.5567737e+00  0.0000000e+00]]\n",
            "Reward: [4.6589456]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [  0.3407247 -27.913404 ]\n",
            "Observation: [[-5.992712   -1.9328806  -4.636784    0.17990416  0.         -0.8537511\n",
            "  -4.6365714   0.        ]]\n",
            "Reward: [2.7635415]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [ 0.48002842 33.299244  ]\n",
            "Observation: [[-5.8542175  -1.8548477  -3.7859263   0.17787538  0.         -1.484683\n",
            "  -3.785808    0.        ]]\n",
            "Reward: [1.7930562]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [  0.75772536 -23.637781  ]\n",
            "Observation: [[-5.3685966  -1.7873394  -3.2787118   0.17591372  0.         -0.9836325\n",
            "  -3.2786334   0.        ]]\n",
            "Reward: [1.2775941]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [  0.66033006 -20.706667  ]\n",
            "Observation: [[-4.952422   -1.7283216  -2.9325707   0.17401555  0.         -0.5555515\n",
            "  -2.9325135   0.        ]]\n",
            "Reward: [0.9727384]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [ 0.5552527 58.528923 ]\n",
            "Observation: [[-4.61782    -1.6762712  -2.6770597   0.17217754  0.         -1.682437\n",
            "  -2.6770153   0.        ]]\n",
            "Reward: [0.77609366]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [  0.05660375 -40.66336   ]\n",
            "Observation: [[-4.3466907  -1.6300225  -2.4784765   0.17039657  0.         -0.85239094\n",
            "  -2.4784405   0.        ]]\n",
            "Reward: [0.6407326]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [  0.83044255 -12.6361265 ]\n",
            "Observation: [[-4.1231008  -1.5886662  -2.318403    0.16866975  0.         -0.590516\n",
            "  -2.318373    0.        ]]\n",
            "Reward: [0.542851]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [0.05897643 9.613934  ]\n",
            "Observation: [[-3.9354851  -1.5514809  -2.1858118   0.16699438  0.         -0.77278876\n",
            "  -2.185786    0.        ]]\n",
            "Reward: [0.469309]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [  0.5007581 -20.218449 ]\n",
            "Observation: [[-3.7756262  -1.5178858  -2.0736432   0.16536796  0.         -0.35846978\n",
            "  -2.073621    0.        ]]\n",
            "Reward: [0.41234434]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [ 0.09584906 42.801594  ]\n",
            "Observation: [[-3.637612   -1.4874077  -1.9771415   0.16378815  0.         -1.2061964\n",
            "  -1.9771218   0.        ]]\n",
            "Reward: [0.36711305]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [  0.91847754 -54.702038  ]\n",
            "Observation: [[-3.5171072  -1.4596565  -1.8929691   0.16225277  0.         -0.09075116\n",
            "  -1.8929514   0.        ]]\n",
            "Reward: [0.3304536]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [  0.19899096 -15.028609  ]\n",
            "Observation: [[-3.410864   -1.4343067  -1.818706    0.16075978  0.          0.2162192\n",
            "  -1.8186901   0.        ]]\n",
            "Reward: [0.30022427]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [ 0.9264303 14.446539 ]\n",
            "Observation: [[-3.3164053  -1.4110855  -1.752549    0.15930726  0.         -0.08265302\n",
            "  -1.7525345   0.        ]]\n",
            "Reward: [0.27492782]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [  0.6612774 -13.120953 ]\n",
            "Observation: [[-3.2318053  -1.3897609  -1.6931233   0.15789342  0.          0.19014776\n",
            "  -1.69311     0.        ]]\n",
            "Reward: [0.25348917]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [  0.20632106 -23.957212  ]\n",
            "Observation: [[-3.1555455  -1.3701348  -1.6393597   0.15651655  0.          0.68687737\n",
            "  -1.6393474   0.        ]]\n",
            "Reward: [0.2351183]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [ 0.74001575 20.40171   ]\n",
            "Observation: [[-3.0864103  -1.3520375  -1.5904126   0.15517509  0.          0.25224108\n",
            "  -1.5904012   0.        ]]\n",
            "Reward: [0.21922284]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [ 0.25063646 -8.79547   ]\n",
            "Observation: [[-3.023415   -1.3353214  -1.5456034   0.15386754  0.          0.4358002\n",
            "  -1.5455927   0.        ]]\n",
            "Reward: [0.20535074]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [  0.4487116 -49.94898  ]\n",
            "Observation: [[-2.9657524  -1.3198583  -1.5043799   0.15259251  0.          1.4708469\n",
            "  -1.5043699   0.        ]]\n",
            "Reward: [0.1931515]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [ 0.92130417 55.149216  ]\n",
            "Observation: [[-2.9127536  -1.305536   -1.4662882   0.15134865  0.          0.2940967\n",
            "  -1.4662788   0.        ]]\n",
            "Reward: [0.1823496]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [3.0233907e-02 3.7347672e+01]\n",
            "Observation: [[-2.8638601  -1.2922556  -1.4309508   0.15013471  0.         -0.5019172\n",
            "  -1.4309419   0.        ]]\n",
            "Reward: [0.1727257]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.5127765 35.97593  ]\n",
            "Observation: [[-2.8186011  -1.27993    -1.398051    0.14894953  0.         -1.252528\n",
            "  -1.3980426   0.        ]]\n",
            "Reward: [0.16410322]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [7.467630e-03 2.049722e+01]\n",
            "Observation: [[-2.7765768  -1.2684816  -1.3673209   0.14779198  0.         -1.644479\n",
            "  -1.3673129   0.        ]]\n",
            "Reward: [0.15633848]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [ 0.3406736 48.578114 ]\n",
            "Observation: [[-2.737445   -1.2578413  -1.338532    0.14666101  0.         -2.5223362\n",
            "  -1.3385243   0.        ]]\n",
            "Reward: [0.14931355]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [ 0.88884884 11.406866  ]\n",
            "Observation: [[-2.7009118  -1.2479476  -1.3114882   0.14555562  0.         -2.5926893\n",
            "  -1.3114809   0.        ]]\n",
            "Reward: [0.14293076]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [  0.13509543 -27.95238   ]\n",
            "Observation: [[-2.666722   -1.2387446  -1.28602     0.14447483  0.         -1.9765341\n",
            "  -1.286013    0.        ]]\n",
            "Reward: [0.13710856]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [  0.3464731 -29.829815 ]\n",
            "Observation: [[-2.6346538  -1.2301828  -1.2619802   0.14341778  0.         -1.3813677\n",
            "  -1.2619735   0.        ]]\n",
            "Reward: [0.13177843]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [  0.74946994 -53.61061   ]\n",
            "Observation: [[-2.6045132  -1.2222167  -1.2392399   0.14238359  0.         -0.3688892\n",
            "  -1.2392335   0.        ]]\n",
            "Reward: [0.1268824]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [ 0.03119264 -2.1429412 ]\n",
            "Observation: [[-2.5761294  -1.2148055  -1.2176863   0.14137146  0.         -0.32575852\n",
            "  -1.2176801   0.        ]]\n",
            "Reward: [0.12237103]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [  0.73449945 -42.097706  ]\n",
            "Observation: [[-2.549352   -1.2079121  -1.1972195   0.1403806   0.          0.47279352\n",
            "  -1.1972136   0.        ]]\n",
            "Reward: [0.11820205]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [0.8618101 6.5389543]\n",
            "Observation: [[-2.5240467  -1.2015024  -1.1777513   0.1394103   0.          0.34482813\n",
            "  -1.1777456   0.        ]]\n",
            "Reward: [0.11433902]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [ 0.6805953 -4.9536405]\n",
            "Observation: [[-2.5000951  -1.1955457  -1.1592028   0.13845985  0.          0.43683243\n",
            "  -1.1591973   0.        ]]\n",
            "Reward: [0.11075043]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [  0.6678308 -30.724606 ]\n",
            "Observation: [[-2.477391   -1.1900133  -1.141504    0.13752857  0.          1.018537\n",
            "  -1.1414987   0.        ]]\n",
            "Reward: [0.10740885]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [ 0.78770196 -0.68542796]\n",
            "Observation: [[-2.455839   -1.1848794  -1.124592    0.13661583  0.          1.0180842\n",
            "  -1.1245868   0.        ]]\n",
            "Reward: [0.10429037]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [ 0.22559123 13.623952  ]\n",
            "Observation: [[-2.4353535  -1.18012    -1.10841     0.13572103  0.          0.74657613\n",
            "  -1.108405    0.        ]]\n",
            "Reward: [0.10137396]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [0.10612041 8.124462  ]\n",
            "Observation: [[-2.4158573  -1.1757131  -1.092907    0.13484357  0.          0.5837167\n",
            "  -1.0929021   0.        ]]\n",
            "Reward: [0.09864117]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [ 0.6005148 44.95156  ]\n",
            "Observation: [[-2.397281   -1.1716381  -1.0780368   0.13398293  0.         -0.2912669\n",
            "  -1.078032    0.        ]]\n",
            "Reward: [0.09607567]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [ 0.50768405 52.67766   ]\n",
            "Observation: [[-2.3795607  -1.1678764  -1.0637575   0.13313855  0.         -1.3026628\n",
            "  -1.0637529   0.        ]]\n",
            "Reward: [0.09366297]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [  0.4495603 -57.753292 ]\n",
            "Observation: [[-2.3626394  -1.1644104  -1.0500311   0.13230994  0.         -0.17342803\n",
            "  -1.0500265   0.        ]]\n",
            "Reward: [0.0913902]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [ 0.71540385 33.31872   ]\n",
            "Observation: [[-2.3464644  -1.1612236  -1.0368227   0.13149662  0.         -0.8197654\n",
            "  -1.0368183   0.        ]]\n",
            "Reward: [0.08924587]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [  0.3913889 -28.316359 ]\n",
            "Observation: [[-2.330988  -1.1583014 -1.0241004  0.1306981  0.        -0.2605854\n",
            "  -1.0240961  0.       ]]\n",
            "Reward: [0.08721969]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [ 0.7028882 20.431341 ]\n",
            "Observation: [[-2.316166   -1.1556293  -1.0118353   0.12991397  0.         -0.6594615\n",
            "  -1.0118312   0.        ]]\n",
            "Reward: [0.08530243]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [ 0.0446762 -6.669329 ]\n",
            "Observation: [[-2.3019586  -1.1531943  -1.0000006   0.12914377  0.         -0.52306145\n",
            "  -0.9999965   0.        ]]\n",
            "Reward: [0.08348577]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 5.7550524e-03 -4.1565945e+01]\n",
            "Observation: [[-2.288329   -1.1509842  -0.98857164  0.12838712  0.          0.3042045\n",
            "  -0.98856765  0.        ]]\n",
            "Reward: [0.08176224]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [  0.32494295 -14.228297  ]\n",
            "Observation: [[-2.2752426  -1.1489874  -0.9775258   0.12764362  0.          0.5850216\n",
            "  -0.9775219   0.        ]]\n",
            "Reward: [0.08012503]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [  0.5666315 -15.881701 ]\n",
            "Observation: [[-2.2626686  -1.1471933  -0.9668421   0.12691286  0.          0.8954983\n",
            "  -0.9668383   0.        ]]\n",
            "Reward: [0.07856803]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [ 0.72231734 39.90718   ]\n",
            "Observation: [[-2.250578   -1.1455917  -0.9565013   0.12619454  0.          0.09083375\n",
            "  -0.95649755  0.        ]]\n",
            "Reward: [0.07708562]\n",
            "iteration: 88\n",
            "Done: [False]\n",
            "Action taken: [  0.4329661 -57.633133 ]\n",
            "Observation: [[-2.2389436  -1.1441733  -0.9464853   0.12548827  0.          1.2409357\n",
            "  -0.94648165  0.        ]]\n",
            "Reward: [-9.546293]\n",
            "iteration: 89\n",
            "Done: [False]\n",
            "Action taken: [0.66747737 5.939556  ]\n",
            "Observation: [[ 0.79486847  0.8188046   1.0444665   2.496789    0.         -0.62170136\n",
            "   1.0444647   0.        ]]\n",
            "Reward: [0.00017334]\n",
            "iteration: 90\n",
            "Done: [ True]\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0') for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv())\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "R172XbXX5Am5",
        "outputId": "1fd607dc-9abd-4c62-b583-d42697c3e55a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([1.173e+03, 4.450e+02, 1.000e+01, 1.000e+01, 0.000e+00, 3.550e+02,\n",
            "       1.000e+00, 0.000e+00], dtype=float32), {})\n",
            "Action taken: [ 0.234057 45.87425 ]\n",
            "Observation: [1.1730000e+03 4.4500000e+02 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
            " 2.4031438e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.26528174 37.21364   ]\n",
            "Observation: [1.1730000e+03 4.5313940e+02 1.0000000e+01 3.2557576e+00 0.0000000e+00\n",
            " 1.4728029e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [ 0.0913955 41.281696 ]\n",
            "Observation: [1.1730000e+03 4.6806161e+02 1.0000000e+01 5.9688888e+00 0.0000000e+00\n",
            " 4.4076042e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [  0.6630683 -22.510214 ]\n",
            "Observation: [1.1730000e+03 4.8863620e+02 1.0000000e+01 8.2298317e+00 0.0000000e+00\n",
            " 1.0035158e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [  0.08742023 -34.556503  ]\n",
            "Observation: [1.1730000e+03 5.1392108e+02 1.0000000e+01 1.0113951e+01 0.0000000e+00\n",
            " 1.8674283e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [ 0.9018553 -4.5977783]\n",
            "Observation: [1.1730000e+03 5.4313116e+02 1.0000000e+01 1.1684050e+01 0.0000000e+00\n",
            " 1.9823727e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [0.97900766 0.48201385]\n",
            "Observation: [1.1730000e+03 5.7094269e+02 1.0000000e+01 1.1124587e+01 0.0000000e+00\n",
            " 1.9703224e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [ 0.04582943 22.273016  ]\n",
            "Observation: [1.173000e+03 5.859795e+02 1.000000e+01 6.014732e+00 0.000000e+00\n",
            " 1.413497e+02 1.000000e+00 0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.4233536 46.110336 ]\n",
            "Observation: [1.1730000e+03 5.9037079e+02 1.0000000e+01 1.7565190e+00 0.0000000e+00\n",
            " 2.6073866e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [  0.70262975 -28.789707  ]\n",
            "Observation: [ 1.1730000e+03  5.8589081e+02  1.0000000e+01 -1.7919917e+00\n",
            "  0.0000000e+00  9.8048134e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [ 0.09675319 42.07306   ]\n",
            "Observation: [ 1.173000e+03  5.740181e+02  1.000000e+01 -4.749084e+00  0.000000e+00\n",
            "  0.000000e+00  1.000000e+00  0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [  0.3145742 -28.139328 ]\n",
            "Observation: [ 1.1730000e+03  5.5598480e+02  1.0000000e+01 -7.2133274e+00\n",
            "  0.0000000e+00  7.0348320e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [  0.6253395 -40.030106 ]\n",
            "Observation: [ 1.1730000e+03  5.3281763e+02  1.0000000e+01 -9.2668638e+00\n",
            "  0.0000000e+00  1.7042358e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [ 0.97345054 43.98384   ]\n",
            "Observation: [ 1.1730000e+03  5.0537225e+02  1.0000000e+01 -1.0978144e+01\n",
            "  0.0000000e+00  6.0463982e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [  0.0929045 -30.55654  ]\n",
            "Observation: [ 1.1730000e+03  4.7436172e+02  1.0000000e+01 -1.2404211e+01\n",
            "  0.0000000e+00  1.3685533e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [ 0.4364077 10.453006 ]\n",
            "Observation: [ 1.1730000e+03  4.4038025e+02  1.0000000e+01 -1.3592600e+01\n",
            "  0.0000000e+00  1.1072282e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [ 0.31397223 -6.173403  ]\n",
            "Observation: [ 1.1730000e+03  4.0392291e+02  1.0000000e+01 -1.4582924e+01\n",
            "  0.0000000e+00  1.2615633e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [  0.6513571 -41.07938  ]\n",
            "Observation: [ 1.1730000e+03  3.6540244e+02  1.0000000e+01 -1.5408195e+01\n",
            "  0.0000000e+00  2.2885477e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [  0.29675186 -25.692406  ]\n",
            "Observation: [ 1.1730000e+03  3.2516263e+02  1.0000000e+01 -1.6095919e+01\n",
            "  0.0000000e+00  2.9308578e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [  0.4319154 -59.5608   ]\n",
            "Observation: [ 1.1730000e+03  2.8596887e+02  1.0000000e+01 -1.5677509e+01\n",
            "  0.0000000e+00  4.4198779e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [  0.688301 -39.008343]\n",
            "Observation: [ 1.1730000e+03  2.6144678e+02  1.0000000e+01 -9.8088341e+00\n",
            "  0.0000000e+00  5.3950867e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [ 0.6205133 44.979626 ]\n",
            "Observation: [ 1.1730000e+03  2.4915111e+02  1.0000000e+01 -4.9182706e+00\n",
            "  0.0000000e+00  4.2705957e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [  0.54617685 -23.69454   ]\n",
            "Observation: [ 1.1730000e+03  2.4704410e+02  1.0000000e+01 -8.4280145e-01\n",
            "  0.0000000e+00  4.8629593e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [0.25126496 2.4174113 ]\n",
            "Observation: [1.1730000e+03 2.5342766e+02 1.0000000e+01 2.5534229e+00 0.0000000e+00\n",
            " 4.8025241e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [ 0.55161  -9.533233]\n",
            "Observation: [1.1730000e+03 2.6688669e+02 1.0000000e+01 5.3836098e+00 0.0000000e+00\n",
            " 5.0408548e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.64824355 52.338158  ]\n",
            "Observation: [1.1730000e+03 2.8624194e+02 1.0000000e+01 7.7420993e+00 0.0000000e+00\n",
            " 3.7324008e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [  0.29730737 -40.74307   ]\n",
            "Observation: [1.1730000e+03 3.1051071e+02 1.0000000e+01 9.7075071e+00 0.0000000e+00\n",
            " 4.7509775e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [2.4531027e-02 5.5766312e+01]\n",
            "Observation: [1.1730000e+03 3.3887405e+02 1.0000000e+01 1.1345346e+01 0.0000000e+00\n",
            " 3.3568198e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [  0.34874013 -53.26894   ]\n",
            "Observation: [1.1730000e+03 3.7064960e+02 1.0000000e+01 1.2710213e+01 0.0000000e+00\n",
            " 4.6885434e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [ 0.45820168 39.529835  ]\n",
            "Observation: [1.1730000e+03 4.0526862e+02 1.0000000e+01 1.3847602e+01 0.0000000e+00\n",
            " 3.7002975e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [ 3.8828617e-03 -1.6415422e+01]\n",
            "Observation: [1.1730000e+03 4.4225717e+02 1.0000000e+01 1.4795425e+01 0.0000000e+00\n",
            " 4.1106830e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [  0.24395275 -54.82277   ]\n",
            "Observation: [1.17300000e+03 4.81220367e+02 1.00000000e+01 1.55852785e+01\n",
            " 0.00000000e+00 5.48125244e+02 1.00000000e+00 0.00000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [ 0.24472997 41.994846  ]\n",
            "Observation: [1.1730000e+03 5.1935028e+02 1.0000000e+01 1.5251976e+01 0.0000000e+00\n",
            " 4.4313809e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [ 0.7380201 -4.9652643]\n",
            "Observation: [1.1730000e+03 5.4298584e+02 1.0000000e+01 9.4542227e+00 0.0000000e+00\n",
            " 4.5555127e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [ 0.52688885 -6.416888  ]\n",
            "Observation: [1.1730000e+03 5.5454279e+02 1.0000000e+01 4.6227612e+00 0.0000000e+00\n",
            " 4.7159348e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [ 0.68212384 55.346672  ]\n",
            "Observation: [1.1730000e+03 5.5603412e+02 1.0000000e+01 5.9654343e-01 0.0000000e+00\n",
            " 3.3322681e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [ 0.7268645 -5.0075088]\n",
            "Observation: [ 1.1730000e+03  5.4913751e+02  1.0000000e+01 -2.7586379e+00\n",
            "  0.0000000e+00  3.4574557e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [  0.20876469 -48.328552  ]\n",
            "Observation: [ 1.1730000e+03  5.3525098e+02  1.0000000e+01 -5.5546227e+00\n",
            "  0.0000000e+00  4.6656696e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [  0.9426219 -21.034786 ]\n",
            "Observation: [ 1.1730000e+03  5.1553943e+02  1.0000000e+01 -7.8846097e+00\n",
            "  0.0000000e+00  5.1915393e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [ 0.3167385 -0.7490832]\n",
            "Observation: [ 1.173000e+03  4.909738e+02  1.000000e+01 -9.826265e+00  0.000000e+00\n",
            "  5.210266e+02  1.000000e+00  0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [ 0.86510795 -7.6591144 ]\n",
            "Observation: [ 1.1730000e+03  4.6236301e+02  1.0000000e+01 -1.1444312e+01\n",
            "  0.0000000e+00  5.4017444e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [ 0.6418892 -9.598403 ]\n",
            "Observation: [ 1.1730000e+03  4.3038129e+02  1.0000000e+01 -1.2792685e+01\n",
            "  0.0000000e+00  5.6417041e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [ 0.21642834 10.212973  ]\n",
            "Observation: [ 1.1730000e+03  3.9559048e+02  1.0000000e+01 -1.3916327e+01\n",
            "  0.0000000e+00  5.3863800e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [  0.74950457 -52.52396   ]\n",
            "Observation: [ 1.1730000e+03  3.5845874e+02  1.0000000e+01 -1.4852697e+01\n",
            "  0.0000000e+00  6.6994788e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [  0.09870762 -43.586304  ]\n",
            "Observation: [ 1.1730000e+03  3.1937622e+02  1.0000000e+01 -1.5633005e+01\n",
            "  0.0000000e+00  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [  0.84622943 -19.821566  ]\n",
            "Observation: [ 1.1730000e+03  2.8114685e+02  1.0000000e+01 -1.5291748e+01\n",
            "  0.0000000e+00  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [ 8.411657e-03 -3.719542e+01]\n",
            "Observation: [ 1.1730000e+03  2.5742844e+02  1.0000000e+01 -9.4873657e+00\n",
            "  0.0000000e+00  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [  0.89201856 -43.783962  ]\n",
            "Observation: [ 1.1730000e+03  2.4580247e+02  1.0000000e+01 -4.6503806e+00\n",
            "  0.0000000e+00  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [  0.30499986 -47.625027  ]\n",
            "Observation: [ 1.1730000e+03  2.4425359e+02  1.0000000e+01 -6.1955965e-01\n",
            "  0.0000000e+00  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [ 0.01355899 -4.1042867 ]\n",
            "Observation: [1.1730000e+03 2.5110223e+02 1.0000000e+01 2.7394578e+00 0.0000000e+00\n",
            " 7.1000000e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [ 0.5379197 38.519886 ]\n",
            "Observation: [1.1730000e+03 2.6494882e+02 1.0000000e+01 5.5386391e+00 0.0000000e+00\n",
            " 6.1370026e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [ 0.9529561 -5.3133445]\n",
            "Observation: [1.1730000e+03 2.8462704e+02 1.0000000e+01 7.8712897e+00 0.0000000e+00\n",
            " 6.2698364e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [  0.8588825 -13.858188 ]\n",
            "Observation: [1.1730000e+03 3.0916498e+02 1.0000000e+01 9.8151655e+00 0.0000000e+00\n",
            " 6.6162909e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [ 0.41669962 37.556435  ]\n",
            "Observation: [1.1730000e+03 3.3775262e+02 1.0000000e+01 1.1435062e+01 0.0000000e+00\n",
            " 5.6773804e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.46282312 47.49416   ]\n",
            "Observation: [1.1730000e+03 3.6971506e+02 1.0000000e+01 1.2784976e+01 0.0000000e+00\n",
            " 4.4900262e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [  0.8190639 -59.939823 ]\n",
            "Observation: [1.17300000e+03 4.04489807e+02 1.00000000e+01 1.39099045e+01\n",
            " 0.00000000e+00 5.98852173e+02 1.00000000e+00 0.00000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [ 0.5596851 43.953453 ]\n",
            "Observation: [1.1730000e+03 4.4160818e+02 1.0000000e+01 1.4847344e+01 0.0000000e+00\n",
            " 4.8896857e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [ 0.67490166 20.670023  ]\n",
            "Observation: [1.1730000e+03 4.8067953e+02 1.0000000e+01 1.5628545e+01 0.0000000e+00\n",
            " 4.3729349e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [  0.03372375 -13.158146  ]\n",
            "Observation: [1.1730000e+03 5.1889960e+02 1.0000000e+01 1.5288031e+01 0.0000000e+00\n",
            " 4.7018887e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [ 0.89924544 30.628098  ]\n",
            "Observation: [1.1730000e+03 5.4261029e+02 1.0000000e+01 9.4842682e+00 0.0000000e+00\n",
            " 3.9361862e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [ 0.65785116 14.269365  ]\n",
            "Observation: [1.173000e+03 5.542298e+02 1.000000e+01 4.647799e+00 0.000000e+00\n",
            " 3.579452e+02 1.000000e+00 0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [ 0.56450284 -4.757083  ]\n",
            "Observation: [1.1730000e+03 5.5577332e+02 1.0000000e+01 6.1740851e-01 0.0000000e+00\n",
            " 3.6983792e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [ 0.6934047 13.024266 ]\n",
            "Observation: [ 1.1730000e+03  5.4892017e+02  1.0000000e+01 -2.7412505e+00\n",
            "  0.0000000e+00  3.3727725e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [  0.45484373 -25.436724  ]\n",
            "Observation: [ 1.1730000e+03  5.3506982e+02  1.0000000e+01 -5.5401330e+00\n",
            "  0.0000000e+00  4.0086905e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [ 0.32969996 34.091553  ]\n",
            "Observation: [ 1.1730000e+03  5.1538849e+02  1.0000000e+01 -7.8725348e+00\n",
            "  0.0000000e+00  3.1564017e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [  0.12400325 -25.776272  ]\n",
            "Observation: [ 1.1730000e+03  4.9084799e+02  1.0000000e+01 -9.8162031e+00\n",
            "  0.0000000e+00  3.8008084e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [  0.65223044 -31.009539  ]\n",
            "Observation: [ 1.1730000e+03  4.6225818e+02  1.0000000e+01 -1.1435926e+01\n",
            "  0.0000000e+00  4.5760471e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [  0.19525793 -17.11018   ]\n",
            "Observation: [ 1.1730000e+03  4.3029395e+02  1.0000000e+01 -1.2785696e+01\n",
            "  0.0000000e+00  5.0038016e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [ 0.91699713 -5.9514036 ]\n",
            "Observation: [ 1.1730000e+03  3.9551767e+02  1.0000000e+01 -1.3910504e+01\n",
            "  0.0000000e+00  5.1525867e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [ 0.74679023 -4.148079  ]\n",
            "Observation: [ 1.1730000e+03  3.5839807e+02  1.0000000e+01 -1.4847845e+01\n",
            "  0.0000000e+00  5.2562885e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [ 0.1438009 27.83644  ]\n",
            "Observation: [ 1.1730000e+03  3.1932568e+02  1.0000000e+01 -1.5628962e+01\n",
            "  0.0000000e+00  4.5603775e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [  0.6070081 -27.220078 ]\n",
            "Observation: [ 1.1730000e+03  2.8110474e+02  1.0000000e+01 -1.5288378e+01\n",
            "  0.0000000e+00  5.2408795e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 71\n",
            "Done: False\n",
            "Action taken: [0.4891285 9.341054 ]\n",
            "Observation: [ 1.1730000e+03  2.5739334e+02  1.0000000e+01 -9.4845572e+00\n",
            "  0.0000000e+00  5.0073532e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 72\n",
            "Done: False\n",
            "Action taken: [  0.6589693 -34.351288 ]\n",
            "Observation: [ 1.1730000e+03  2.4577322e+02  1.0000000e+01 -4.6480403e+00\n",
            "  0.0000000e+00  5.8661353e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 73\n",
            "Done: False\n",
            "Action taken: [ 0.24282742 22.144241  ]\n",
            "Observation: [ 1.1730000e+03  2.4422920e+02  1.0000000e+01 -6.1760956e-01\n",
            "  0.0000000e+00  5.3125293e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 74\n",
            "Done: False\n",
            "Action taken: [  0.19905494 -44.493446  ]\n",
            "Observation: [1.1730000e+03 2.5108191e+02 1.0000000e+01 2.7410829e+00 0.0000000e+00\n",
            " 6.4248657e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 75\n",
            "Done: False\n",
            "Action taken: [ 0.7969522 27.132347 ]\n",
            "Observation: [1.1730000e+03 2.6493188e+02 1.0000000e+01 5.5399933e+00 0.0000000e+00\n",
            " 5.7465570e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 76\n",
            "Done: False\n",
            "Action taken: [ 0.5815818 26.726704 ]\n",
            "Observation: [1.1730000e+03 2.8461295e+02 1.0000000e+01 7.8724184e+00 0.0000000e+00\n",
            " 5.0783893e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 77\n",
            "Done: False\n",
            "Action taken: [  0.4385067 -44.185062 ]\n",
            "Observation: [1.173000e+03 3.091532e+02 1.000000e+01 9.816106e+00 0.000000e+00\n",
            " 6.183016e+02 1.000000e+00 0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 78\n",
            "Done: False\n",
            "Action taken: [ 0.09446499 10.342937  ]\n",
            "Observation: [1.1730000e+03 3.3774283e+02 1.0000000e+01 1.1435846e+01 0.0000000e+00\n",
            " 5.9244421e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 79\n",
            "Done: False\n",
            "Action taken: [  0.8594641 -34.838093 ]\n",
            "Observation: [1.1730000e+03 3.6970691e+02 1.0000000e+01 1.2785629e+01 0.0000000e+00\n",
            " 6.7953949e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 80\n",
            "Done: False\n",
            "Action taken: [  0.67889434 -48.252567  ]\n",
            "Observation: [1.1730000e+03 4.0448300e+02 1.0000000e+01 1.3910448e+01 0.0000000e+00\n",
            " 7.1000000e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 81\n",
            "Done: False\n",
            "Action taken: [ 0.58572054 21.905207  ]\n",
            "Observation: [1173.        441.6025    -10.         14.847797    0.        655.237\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 82\n",
            "Done: False\n",
            "Action taken: [  0.2078235 -16.948696 ]\n",
            "Observation: [1148.        478.72202   -10.         14.847797    0.        697.6087\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 83\n",
            "Done: False\n",
            "Action taken: [ 0.7350486 55.4653   ]\n",
            "Observation: [1123.        515.8415    -10.         14.847797    0.        558.9455\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 84\n",
            "Done: False\n",
            "Action taken: [  0.70830137 -20.277063  ]\n",
            "Observation: [1098.        552.961     -10.         14.847797    0.        609.6381\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 85\n",
            "Done: False\n",
            "Action taken: [  0.03071419 -28.976488  ]\n",
            "Observation: [1073.        590.0805    -10.         14.847797    0.        682.07935\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 86\n",
            "Done: False\n",
            "Action taken: [  0.426591 -10.120242]\n",
            "Observation: [1048.        627.2       -10.         14.847797    0.        707.37994\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 87\n",
            "Done: False\n",
            "Action taken: [  0.4091807 -24.110935 ]\n",
            "Observation: [1023.        664.31946   -10.         14.847797    0.        710.\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 88\n",
            "Done: False\n",
            "Action taken: [ 0.20019683 -7.6991196 ]\n",
            "Observation: [998.       701.43896  -10.        14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 89\n",
            "Done: False\n",
            "Action taken: [  0.14258537 -35.49047   ]\n",
            "Observation: [973.       738.5585   -10.        14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 90\n",
            "Done: False\n",
            "Action taken: [  0.45486295 -56.33638   ]\n",
            "Observation: [948.       775.678    -10.        14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 91\n",
            "Done: False\n",
            "Action taken: [  0.93728596 -43.096493  ]\n",
            "Observation: [923.       788.       -10.       -14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 92\n",
            "Done: False\n",
            "Action taken: [  0.3139198 -47.25712  ]\n",
            "Observation: [898.       750.8805   -10.       -14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 93\n",
            "Done: False\n",
            "Action taken: [ 0.7859401 31.323036 ]\n",
            "Observation: [873.       713.761    -10.       -14.847797   0.       631.6924\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 94\n",
            "Done: False\n",
            "Action taken: [  0.17702284 -37.22064   ]\n",
            "Observation: [848.       676.64154  -10.       -14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 95\n",
            "Done: False\n",
            "Action taken: [ 0.24363975 40.134388  ]\n",
            "Observation: [823.       639.52203  -10.       -14.847797   0.       609.664\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 96\n",
            "Done: False\n",
            "Action taken: [  0.31941402 -16.883236  ]\n",
            "Observation: [798.       602.4025   -10.       -14.847797   0.       651.87213\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 97\n",
            "Done: False\n",
            "Action taken: [ 0.13775599 44.015354  ]\n",
            "Observation: [773.       565.283    -10.       -14.847797   0.       541.83374\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 98\n",
            "Done: False\n",
            "Action taken: [  0.6064738 -20.366533 ]\n",
            "Observation: [748.       528.1635   -10.       -14.847797   0.       592.75006\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 99\n",
            "Done: False\n",
            "Action taken: [0.19820683 4.116676  ]\n",
            "Observation: [723.       491.04404  -10.       -14.847797   0.       582.4584\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 100\n",
            "Done: False\n",
            "Action taken: [0.6021964 5.817667 ]\n",
            "Observation: [698.       453.92456  -10.       -14.847797   0.       567.9142\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 101\n",
            "Done: False\n",
            "Action taken: [0.6812138 4.3221984]\n",
            "Observation: [673.       416.80505  -10.       -14.847797   0.       557.1087\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 102\n",
            "Done: False\n",
            "Action taken: [ 0.46166816 -2.9919355 ]\n",
            "Observation: [648.       379.68555  -10.       -14.847797   0.       564.58856\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 103\n",
            "Done: False\n",
            "Action taken: [ 0.625587 -8.666336]\n",
            "Observation: [623.       342.56607  -10.       -14.847797   0.       586.2544\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 104\n",
            "Done: False\n",
            "Action taken: [0.60309535 6.6970286 ]\n",
            "Observation: [598.       305.44656  -10.       -14.847797   0.       569.51184\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 105\n",
            "Done: False\n",
            "Action taken: [ 0.16074666 28.486008  ]\n",
            "Observation: [573.       268.3271   -10.       -14.847797   0.       498.2968\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 106\n",
            "Done: False\n",
            "Action taken: [  0.96750903 -42.440136  ]\n",
            "Observation: [548.       231.20758  -10.       -14.847797   0.       604.39716\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 107\n",
            "Done: False\n",
            "Action taken: [  0.643439 -44.30585 ]\n",
            "Observation: [523.       194.08809  -10.       -14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 108\n",
            "Done: False\n",
            "Action taken: [ 0.33765355 16.903362  ]\n",
            "Observation: [498.       156.9686   -10.       -14.847797   0.       667.7416\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 109\n",
            "Done: False\n",
            "Action taken: [ 0.6136493 14.006832 ]\n",
            "Observation: [473.       119.8491   -10.       -14.847797   0.       632.7245\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 110\n",
            "Done: False\n",
            "Action taken: [  0.5736644 -38.023052 ]\n",
            "Observation: [448.        82.7296   -10.       -14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 111\n",
            "Done: False\n",
            "Action taken: [ 0.32880142 21.667852  ]\n",
            "Observation: [423.        45.610107 -10.       -14.847797   0.       655.8304\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 112\n",
            "Done: False\n",
            "Action taken: [  0.39173952 -26.734013  ]\n",
            "Observation: [398.        12.       -10.        14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 113\n",
            "Done: False\n",
            "Action taken: [  0.7106791 -59.403854 ]\n",
            "Observation: [373.        49.119495 -10.        14.847797   0.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 114\n",
            "Done: False\n",
            "Action taken: [ 0.9517887 30.49339  ]\n",
            "Observation: [348.        86.23899  -10.        14.847797   0.       633.76654\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 115\n",
            "Done: False\n",
            "Action taken: [ 0.44250578 56.401382  ]\n",
            "Observation: [323.       123.35848  -10.        14.847797   0.       492.76306\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 116\n",
            "Done: False\n",
            "Action taken: [  0.14997184 -22.528296  ]\n",
            "Observation: [298.       160.47798  -10.        14.847797   0.       549.0838\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 117\n",
            "Done: False\n",
            "Action taken: [ 0.7059592 -4.408621 ]\n",
            "Observation: [273.       197.59747  -10.        14.847797   0.       560.10535\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 118\n",
            "Done: False\n",
            "Action taken: [  0.40901354 -16.850891  ]\n",
            "Observation: [248.       234.71696  -10.        14.847797   0.       602.2326\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 119\n",
            "Done: False\n",
            "Action taken: [ 0.248901 31.154112]\n",
            "Observation: [223.       271.83646  -10.        14.847797   0.       524.3473\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 120\n",
            "Done: False\n",
            "Action taken: [0.02686077 3.6731863 ]\n",
            "Observation: [198.       308.95596  -10.        14.847797   0.       515.16437\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 121\n",
            "Done: False\n",
            "Action taken: [ 0.20017715 -9.217131  ]\n",
            "Observation: [173.       346.07544  -10.        14.847797   0.       538.20715\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 122\n",
            "Done: False\n",
            "Action taken: [0.10941953 1.3128141 ]\n",
            "Observation: [148.       383.19495  -10.        14.847797   0.       534.9251\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 123\n",
            "Done: False\n",
            "Action taken: [ 0.95538217 45.246876  ]\n",
            "Observation: [123.       420.31445  -10.        14.847797   0.       421.80795\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 124\n",
            "Done: False\n",
            "Action taken: [  0.15671687 -14.029788  ]\n",
            "Observation: [ 98.       457.43393  -10.        14.847797   0.       456.88242\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 125\n",
            "Done: False\n",
            "Action taken: [0.37533814 7.766677  ]\n",
            "Observation: [ 73.       494.55344  -10.        14.847797   0.       437.46573\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 126\n",
            "Done: False\n",
            "Action taken: [0.11673827 1.3128437 ]\n",
            "Observation: [ 48.       531.6729   -10.        14.847797   0.       434.18362\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 127\n",
            "Done: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: [  0.9238107 -16.280554 ]\n",
            "Observation: [ 23.       568.7924   -10.        14.847797   0.       474.885\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 128\n",
            "Done: False\n",
            "Action taken: [  0.49105346 -54.15518   ]\n",
            "Observation: [ -2.       605.9119   -10.        14.847797   0.       610.27295\n",
            "   0.         0.      ]\n",
            "Reward: -131.01691455104856\n",
            "iteration: 129\n",
            "Done: False\n",
            "Action taken: [ 0.4890698 -0.569245 ]\n",
            "Observation: [ -2.       605.9119   -10.        14.847797   0.       610.27295\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 130\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ],
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and train\n",
        "ent_coef=0.5\n",
        "eval_freq=2000\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef)\n",
        "# model = PPO.load(\"ppo_custom_pong\")\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_sjyFuJcGT-w",
        "outputId": "740a35f5-26f4-45c7-f1ef-e2ef82a38594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Eval num_timesteps=8000, episode_reward=-2.54 +/- 0.68\n",
            "Episode length: 68.00 +/- 11.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 68       |\n",
            "|    mean_reward     | -2.54    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -2.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.17    |\n",
            "| time/              |          |\n",
            "|    fps             | 2147     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-1.71 +/- 0.92\n",
            "Episode length: 83.80 +/- 23.01\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 83.8       |\n",
            "|    mean_reward          | -1.71      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 16000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17352124 |\n",
            "|    clip_fraction        | 0.422      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.17      |\n",
            "|    explained_variance   | 0.244      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.28      |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | 0.0395     |\n",
            "|    std                  | 1.38       |\n",
            "|    value_loss           | 0.929      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -1.71\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | -1.06    |\n",
            "| time/              |          |\n",
            "|    fps             | 1165     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-0.57 +/- 1.86\n",
            "Episode length: 120.80 +/- 23.59\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 121        |\n",
            "|    mean_reward          | -0.572     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 24000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16630663 |\n",
            "|    clip_fraction        | 0.435      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.82      |\n",
            "|    explained_variance   | 0.568      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.74      |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | 0.0424     |\n",
            "|    std                  | 1.9        |\n",
            "|    value_loss           | 0.747      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -0.57\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -0.991   |\n",
            "| time/              |          |\n",
            "|    fps             | 1042     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-1.54 +/- 0.84\n",
            "Episode length: 101.20 +/- 24.84\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 101        |\n",
            "|    mean_reward          | -1.54      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 32000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16927126 |\n",
            "|    clip_fraction        | 0.476      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.5       |\n",
            "|    explained_variance   | 0.64       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.09      |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | 0.0363     |\n",
            "|    std                  | 2.71       |\n",
            "|    value_loss           | 0.547      |\n",
            "----------------------------------------\n",
            "Evaluation at step 8000: mean reward -1.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -1.22    |\n",
            "| time/              |          |\n",
            "|    fps             | 981      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1.46 +/- 1.18\n",
            "Episode length: 94.20 +/- 20.31\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 94.2       |\n",
            "|    mean_reward          | -1.46      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 40000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14823309 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.17      |\n",
            "|    explained_variance   | 0.675      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.56      |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | 0.0285     |\n",
            "|    std                  | 3.74       |\n",
            "|    value_loss           | 0.614      |\n",
            "----------------------------------------\n",
            "Evaluation at step 10000: mean reward -1.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -1.67    |\n",
            "| time/              |          |\n",
            "|    fps             | 937      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-0.91 +/- 1.39\n",
            "Episode length: 111.40 +/- 57.57\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 111        |\n",
            "|    mean_reward          | -0.906     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 48000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16196406 |\n",
            "|    clip_fraction        | 0.418      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.81      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.67      |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 5.12       |\n",
            "|    value_loss           | 0.672      |\n",
            "----------------------------------------\n",
            "Evaluation at step 12000: mean reward -0.91\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.16    |\n",
            "| time/              |          |\n",
            "|    fps             | 926      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=0.32 +/- 2.31\n",
            "Episode length: 158.60 +/- 35.51\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 159        |\n",
            "|    mean_reward          | 0.324      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 56000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13600472 |\n",
            "|    clip_fraction        | 0.411      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -6.43      |\n",
            "|    explained_variance   | 0.627      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.79      |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | 0.0288     |\n",
            "|    std                  | 7.02       |\n",
            "|    value_loss           | 0.766      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 14000: mean reward 0.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | -1.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 839      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-2.09 +/- 1.43\n",
            "Episode length: 92.00 +/- 25.58\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 92         |\n",
            "|    mean_reward          | -2.09      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 64000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14800479 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.08      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.49      |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | 0.0366     |\n",
            "|    std                  | 9.76       |\n",
            "|    value_loss           | 0.663      |\n",
            "----------------------------------------\n",
            "Evaluation at step 16000: mean reward -2.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | -0.0483  |\n",
            "| time/              |          |\n",
            "|    fps             | 834      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 78       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=1.36 +/- 5.39\n",
            "Episode length: 138.40 +/- 110.77\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 138        |\n",
            "|    mean_reward          | 1.36       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 72000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14038308 |\n",
            "|    clip_fraction        | 0.404      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.72      |\n",
            "|    explained_variance   | 0.689      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.63      |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | 0.0373     |\n",
            "|    std                  | 13.2       |\n",
            "|    value_loss           | 0.777      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 1.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -0.39    |\n",
            "| time/              |          |\n",
            "|    fps             | 826      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-0.64 +/- 0.66\n",
            "Episode length: 110.20 +/- 35.67\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 110        |\n",
            "|    mean_reward          | -0.645     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 80000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17711544 |\n",
            "|    clip_fraction        | 0.463      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.35      |\n",
            "|    explained_variance   | 0.595      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.05      |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | 0.0447     |\n",
            "|    std                  | 18.2       |\n",
            "|    value_loss           | 0.47       |\n",
            "----------------------------------------\n",
            "Evaluation at step 20000: mean reward -0.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 129      |\n",
            "|    ep_rew_mean     | 1.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=1.48 +/- 2.63\n",
            "Episode length: 168.40 +/- 81.02\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 168        |\n",
            "|    mean_reward          | 1.48       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 88000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16022202 |\n",
            "|    clip_fraction        | 0.45       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.99      |\n",
            "|    explained_variance   | 0.503      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.49      |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | 0.0404     |\n",
            "|    std                  | 25.4       |\n",
            "|    value_loss           | 0.546      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 22000: mean reward 1.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 137      |\n",
            "|    ep_rew_mean     | 4        |\n",
            "| time/              |          |\n",
            "|    fps             | 828      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=25.55 +/- 48.76\n",
            "Episode length: 236.80 +/- 128.23\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 237        |\n",
            "|    mean_reward          | 25.6       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 96000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17208788 |\n",
            "|    clip_fraction        | 0.462      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.68      |\n",
            "|    explained_variance   | 0.444      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.69      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | 0.0502     |\n",
            "|    std                  | 36.1       |\n",
            "|    value_loss           | 0.392      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 24000: mean reward 25.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 134      |\n",
            "|    ep_rew_mean     | 6.9      |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=1.97 +/- 2.30\n",
            "Episode length: 161.60 +/- 67.48\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 162        |\n",
            "|    mean_reward          | 1.97       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 104000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17116281 |\n",
            "|    clip_fraction        | 0.451      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.4      |\n",
            "|    explained_variance   | 0.562      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.87      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 51.4       |\n",
            "|    value_loss           | 0.395      |\n",
            "----------------------------------------\n",
            "Evaluation at step 26000: mean reward 1.97\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | 10.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 819      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "# Sync normalization statistics from the training environment to the evaluation environment\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "eval_env.ret_rms = train_env.ret_rms\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mL_LYMHeejpF",
        "outputId": "a0c1b264-c71d-45dc-9f35-6c430a93be40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=-84.46 +/- 115.27\n",
            "Episode length: 128.80 +/- 39.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 129      |\n",
            "|    mean_reward     | -84.5    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -84.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -275     |\n",
            "| time/              |          |\n",
            "|    fps             | 2629     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-55.49 +/- 114.04\n",
            "Episode length: 131.00 +/- 70.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -55.5       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004481554 |\n",
            "|    clip_fraction        | 0.0276      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.1       |\n",
            "|    explained_variance   | 0.559       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.941      |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.00016     |\n",
            "|    std                  | 2.25e+06    |\n",
            "|    value_loss           | 1.42        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -55.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 121      |\n",
            "|    ep_rew_mean     | -252     |\n",
            "| time/              |          |\n",
            "|    fps             | 1159     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-54.59 +/- 49.95\n",
            "Episode length: 149.80 +/- 69.42\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 150          |\n",
            "|    mean_reward          | -54.6        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 24000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036968566 |\n",
            "|    clip_fraction        | 0.0201       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.1        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.763       |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00063      |\n",
            "|    std                  | 2.35e+06     |\n",
            "|    value_loss           | 1.69         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -54.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -262     |\n",
            "| time/              |          |\n",
            "|    fps             | 1023     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-59.87 +/- 43.11\n",
            "Episode length: 162.80 +/- 36.45\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 163          |\n",
            "|    mean_reward          | -59.9        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 32000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034573479 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.2        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.578       |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | 0.000596     |\n",
            "|    std                  | 2.45e+06     |\n",
            "|    value_loss           | 1.48         |\n",
            "------------------------------------------\n",
            "Evaluation at step 8000: mean reward -59.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 111      |\n",
            "|    ep_rew_mean     | -241     |\n",
            "| time/              |          |\n",
            "|    fps             | 941      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-164.55 +/- 161.91\n",
            "Episode length: 116.60 +/- 45.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 117         |\n",
            "|    mean_reward          | -165        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004476109 |\n",
            "|    clip_fraction        | 0.0281      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.3       |\n",
            "|    explained_variance   | 0.511       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.654      |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.000234    |\n",
            "|    std                  | 2.58e+06    |\n",
            "|    value_loss           | 1.46        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward -164.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | -255     |\n",
            "| time/              |          |\n",
            "|    fps             | 889      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-150.29 +/- 81.91\n",
            "Episode length: 91.40 +/- 36.15\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 91.4         |\n",
            "|    mean_reward          | -150         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034466675 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.4        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.01        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | 0.000546     |\n",
            "|    std                  | 2.68e+06     |\n",
            "|    value_loss           | 1.75         |\n",
            "------------------------------------------\n",
            "Evaluation at step 12000: mean reward -150.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -267     |\n",
            "| time/              |          |\n",
            "|    fps             | 870      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-122.84 +/- 159.98\n",
            "Episode length: 131.20 +/- 30.50\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -123        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003678766 |\n",
            "|    clip_fraction        | 0.0178      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.5       |\n",
            "|    explained_variance   | 0.588       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.18       |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.000835    |\n",
            "|    std                  | 2.8e+06     |\n",
            "|    value_loss           | 2.03        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward -122.84\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 112      |\n",
            "|    ep_rew_mean     | -266     |\n",
            "| time/              |          |\n",
            "|    fps             | 867      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-128.49 +/- 138.29\n",
            "Episode length: 138.00 +/- 51.68\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -128         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 64000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025062133 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.6        |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.02        |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | 0.00116      |\n",
            "|    std                  | 2.91e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 16000: mean reward -128.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -273     |\n",
            "| time/              |          |\n",
            "|    fps             | 850      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 77       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-75.76 +/- 35.28\n",
            "Episode length: 150.60 +/- 84.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 151          |\n",
            "|    mean_reward          | -75.8        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 72000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035605398 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.7        |\n",
            "|    explained_variance   | 0.479        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.703       |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | 0.000628     |\n",
            "|    std                  | 3.02e+06     |\n",
            "|    value_loss           | 1.78         |\n",
            "------------------------------------------\n",
            "Evaluation at step 18000: mean reward -75.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -245     |\n",
            "| time/              |          |\n",
            "|    fps             | 836      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-183.29 +/- 78.86\n",
            "Episode length: 98.40 +/- 33.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 98.4        |\n",
            "|    mean_reward          | -183        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003738511 |\n",
            "|    clip_fraction        | 0.0235      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.7       |\n",
            "|    explained_variance   | 0.544       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.86       |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.000683    |\n",
            "|    std                  | 3.16e+06    |\n",
            "|    value_loss           | 1.7         |\n",
            "-----------------------------------------\n",
            "Evaluation at step 20000: mean reward -183.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -235     |\n",
            "| time/              |          |\n",
            "|    fps             | 840      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-188.48 +/- 100.36\n",
            "Episode length: 95.40 +/- 32.52\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 95.4         |\n",
            "|    mean_reward          | -188         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 88000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040115286 |\n",
            "|    clip_fraction        | 0.0256       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.8        |\n",
            "|    explained_variance   | 0.502        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.799       |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.000368     |\n",
            "|    std                  | 3.31e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 22000: mean reward -188.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -277     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-109.59 +/- 63.85\n",
            "Episode length: 115.40 +/- 55.33\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 115          |\n",
            "|    mean_reward          | -110         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 96000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031413306 |\n",
            "|    clip_fraction        | 0.0192       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.9        |\n",
            "|    explained_variance   | 0.577        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.2         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | 0.000992     |\n",
            "|    std                  | 3.45e+06     |\n",
            "|    value_loss           | 1.44         |\n",
            "------------------------------------------\n",
            "Evaluation at step 24000: mean reward -109.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 115      |\n",
            "|    ep_rew_mean     | -209     |\n",
            "| time/              |          |\n",
            "|    fps             | 825      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=-130.40 +/- 80.68\n",
            "Episode length: 138.00 +/- 56.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -130         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035240077 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -33          |\n",
            "|    explained_variance   | 0.537        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.348       |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | 0.000891     |\n",
            "|    std                  | 3.61e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 26000: mean reward -130.40\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -234     |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "71c7d9ee-a0bc-4a9a-81d3-3515602701b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -78.79866129999999 +/- 133.82650038538142\n"
          ]
        }
      ],
      "source": [
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "4442f8ca-5fd6-42d6-9825-8dd3e0e8861d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n",
            "reset\n",
            "reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "eval_env.norm_reward = False  # Disable reward normalization for evaluation\n",
        "\n",
        "# Load the normalization statistics\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "\n",
        "# Sync the observation normalization statistics\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the VecNormalize wrapper\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0')])\n",
        "env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", env)\n",
        "\n",
        "# Extract normalization parameters\n",
        "mean = env.obs_rms.mean\n",
        "std = env.obs_rms.var ** 0.5\n",
        "\n",
        "# Save the mean and std to a file\n",
        "import json\n",
        "normalization_params = {'mean': mean.tolist(), 'std': std.tolist()}\n",
        "with open('normalization_params.json', 'w') as f:\n",
        "    json.dump(normalization_params, f)"
      ],
      "metadata": {
        "id": "TCTA0XkE21Kd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.policy)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Print the state dictionary keys to understand the structure\n",
        "for name, param in policy.state_dict().items():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "id": "tOQDfwZkEce8",
        "outputId": "accd63ac-effb-45a9-8bf2-e3c63c8836ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorCriticPolicy(\n",
            "  (features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (pi_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (vf_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (mlp_extractor): MlpExtractor(\n",
            "    (policy_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (value_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "log_std torch.Size([2])\n",
            "mlp_extractor.policy_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.policy_net.0.bias torch.Size([64])\n",
            "mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.policy_net.2.bias torch.Size([64])\n",
            "mlp_extractor.value_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.value_net.0.bias torch.Size([64])\n",
            "mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.value_net.2.bias torch.Size([64])\n",
            "action_net.weight torch.Size([2, 64])\n",
            "action_net.bias torch.Size([2])\n",
            "value_net.weight torch.Size([1, 64])\n",
            "value_net.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the TensorFlow model structure\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class CustomPongModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomPongModel, self).__init__()\n",
        "        self.dense1_policy = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense2_policy = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense1_value = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.dense2_value = tf.keras.layers.Dense(64, activation='tanh')\n",
        "        self.action = tf.keras.layers.Dense(2)\n",
        "        self.value = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Policy network\n",
        "        pi_x = self.dense1_policy(x)\n",
        "        pi_x = self.dense2_policy(pi_x)\n",
        "        action = self.action(pi_x)\n",
        "\n",
        "        # Value network\n",
        "        vf_x = self.dense1_value(x)\n",
        "        vf_x = self.dense2_value(vf_x)\n",
        "        value = self.value(vf_x)\n",
        "\n",
        "        return action, value\n",
        "\n",
        "# Instantiate the TensorFlow model\n",
        "tf_model = CustomPongModel()\n"
      ],
      "metadata": {
        "id": "uI7i6NHBE6kO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# need to run once to initialise model\n",
        "dummy_input = np.random.randn(1, 8).astype(np.float32)  # Assuming input size is 8\n",
        "tf_model(dummy_input)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Extract the weights and biases\n",
        "weights = {}\n",
        "for name, param in policy.state_dict().items():\n",
        "    weights[name] = param.cpu().numpy()\n",
        "\n",
        "# Map the weights to the TensorFlow model\n",
        "# Note: Transpose the weights because PyTorch uses [out_features, in_features] and TensorFlow uses [in_features, out_features]\n",
        "\n",
        "# Policy network\n",
        "tf_model.dense1_policy.set_weights([weights['mlp_extractor.policy_net.0.weight'].T, weights['mlp_extractor.policy_net.0.bias']])\n",
        "tf_model.dense2_policy.set_weights([weights['mlp_extractor.policy_net.2.weight'].T, weights['mlp_extractor.policy_net.2.bias']])\n",
        "\n",
        "# Value network\n",
        "tf_model.dense1_value.set_weights([weights['mlp_extractor.value_net.0.weight'].T, weights['mlp_extractor.value_net.0.bias']])\n",
        "tf_model.dense2_value.set_weights([weights['mlp_extractor.value_net.2.weight'].T, weights['mlp_extractor.value_net.2.bias']])\n",
        "\n",
        "# Action and value outputs\n",
        "tf_model.action.set_weights([weights['action_net.weight'].T, weights['action_net.bias']])\n",
        "tf_model.value.set_weights([weights['value_net.weight'].T, weights['value_net.bias']])\n"
      ],
      "metadata": {
        "id": "46982YgHJNNr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf_model.save(\"tf_custom_pong_model.keras\")\n",
        "saved_model_path = \"./saved_model\"\n",
        "tf.saved_model.save(tf_model, saved_model_path)"
      ],
      "metadata": {
        "id": "4Yc_bcrpJwOJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeLcH84ysrNd",
        "outputId": "418eda87-ddda-4c3f-a291-7c8b09e24026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-07 08:00:36.654914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Convert TensorFlow to TensorFlow.js\n",
        "!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model ./saved_model ./tfjs_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}