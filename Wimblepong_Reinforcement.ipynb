{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkhwGXZyRfWZ",
        "outputId": "e30ae992-5aa7-460a-e18c-120e9b5d2c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446663 sha256=c3d48eaf412f2b4c3ec7c9408c0bede670ec75937636111498249a4d5cf311dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.2)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.1 ydf-0.5.0\n"
          ]
        }
      ],
      "source": [
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "vYCJyx3kJikU",
        "outputId": "38614f02-f310-4fc5-8655-061a7ed3f895"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3ea7cac5e07a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDRIVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/wimblepong\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDAY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"6-thursday\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"6-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRePUZ1QRh6C",
        "outputId": "c3cf0a47-5d59-4958-afc9-5624113d1b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/skimage/util/dtype.py:27: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym version: 0.29.1\n",
            "stable-baselines3 version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'blue', 'Player2': 'red'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    \"serve\": lambda ball_speed: ball_speed / 10,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ExmAn7VtUakq",
        "outputId": "c65f66cd-84cc-431a-86e2-a2cba4c94e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CustomPongEnv-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -60]), high=np.array([1, 60]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -np.inf, -np.inf, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([COURT_WIDTH, COURT_HEIGHT, np.inf, np.inf, COURT_WIDTH, COURT_HEIGHT, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "          #  {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "          #  {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "\n",
        "        self.starting_state_index = 0\n",
        "        self.serve_delay = 50\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 15\n",
        "\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "\n",
        "        self.is_done = False\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"Environment reset\")\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        computer = starting_state['opponent']\n",
        "        player = starting_state['player']\n",
        "\n",
        "        self.game_state = {\n",
        "            'server': server,\n",
        "            'positions_reversed': positions_reversed,\n",
        "            'opponent': computer,\n",
        "            'player': player,\n",
        "            Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "            Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "            'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "            'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = 30 * np.random.rand()\n",
        "        self.serve_delay = 100 * np.random.rand()\n",
        "        self.direction = self.direction if np.random.rand() > 0.5 else -self.direction\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.is_done = False\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction}\n",
        "        computer_player_actions = self.get_computer_player_actions(self.game_state['opponent'])\n",
        "        actions = {self.game_state['opponent']: computer_player_actions, self.game_state['player']: model_player_actions}\n",
        "        reward = self.update_game_state(actions, 2.5)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        terminated = self.check_done()\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def update_game_state(self, actions, delta_time):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        model_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            self.is_done = True\n",
        "            return 0.01\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "            # if serving_from_left:\n",
        "            #     ball['x'] = game_state[server]['width'] + ball['radius']\n",
        "            # else:\n",
        "            #     ball['x'] = COURT_WIDTH - game_state[server]['width'] - ball['radius']\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "                if game_state['player'] == server:\n",
        "                    reward += rewards_map['serve'](abs(ball['dy']) + abs(ball['dx']))\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward += rewards_map[\"hit_paddle\"](stats['rally_length'])\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "                if model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_left, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "                if not model_is_left:\n",
        "                    reward += rewards_map['conceed_point'](ball, paddle_right, stats['rally_length'])\n",
        "                else:\n",
        "                    reward += rewards_map['score_point'](stats['rally_length'])\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "        if model_is_left:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_left['dy']))\n",
        "        else:\n",
        "            reward += rewards_map['paddle_movement'](abs(paddle_right['dy']))\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "        reward += 0.01 * stats['rally_length']\n",
        "        return reward\n",
        "\n",
        "    def get_computer_player_actions(self, player):\n",
        "        state = self.game_state\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -self.bounded_value(\n",
        "                    paddle['y'] - state['ball']['y'] + paddle['height'] / 4 ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "\n",
        "    def bounded_value(self, value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x']),\n",
        "            float(state['ball']['y']),\n",
        "            float(state['ball']['dx']),\n",
        "            float(state['ball']['dy']),\n",
        "            float(paddle['x']),\n",
        "            float(paddle['y']),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(is_server),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def check_done(self):\n",
        "        if self.game_state['stats']['rally_length'] > 100:\n",
        "            return True\n",
        "        if self.step_count > 1000:\n",
        "            return True\n",
        "        return self.is_done\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w4LjxARS9zCF",
        "outputId": "f38b3a31-3151-4c4c-a9f5-2efb05f4b089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: [[0.01       0.00999997 0.00995036 0.00995036 0.         0.00999996\n",
            "  0.00707071 0.        ]]\n",
            "Action taken: [ 0.8282336 18.14778  ]\n",
            "Observation: [[ 0.00707107  0.00707105  0.00703597 -0.999949    0.         -0.99397427\n",
            "   0.00499969  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 0\n",
            "Done: [False]\n",
            "Action taken: [  0.5639987 -57.280445 ]\n",
            "Observation: [[ 0.0057735   0.8229626   0.00574484 -0.52322984  0.          1.3439206\n",
            "   0.00408214  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 1\n",
            "Done: [False]\n",
            "Action taken: [  0.5399142 -27.077219 ]\n",
            "Observation: [[ 0.005       1.4517182   0.00497518 -0.1971278   0.          1.3474191\n",
            "   0.00353516  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 2\n",
            "Done: [False]\n",
            "Action taken: [ 0.9643562 23.3147   ]\n",
            "Observation: [[0.00447213 1.6718895  0.00444993 0.07512181 0.         0.5504365\n",
            "  0.00316187 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 3\n",
            "Done: [False]\n",
            "Action taken: [  0.19304404 -49.277435  ]\n",
            "Observation: [[0.00408248 1.7642368  0.00406221 0.3046841  0.         1.477877\n",
            "  0.00288631 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 4\n",
            "Done: [False]\n",
            "Action taken: [ 0.32973075 37.437447  ]\n",
            "Observation: [[0.00377964 1.8123935  0.00376087 0.49109027 0.         0.4322442\n",
            "  0.00267214 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 5\n",
            "Done: [False]\n",
            "Action taken: [ 0.64543164 54.60348   ]\n",
            "Observation: [[ 0.00353553  1.8402551   0.00351797  0.6354942   0.         -0.97703457\n",
            "   0.00249949  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 6\n",
            "Done: [False]\n",
            "Action taken: [  0.0585899 -45.339985 ]\n",
            "Observation: [[0.00333333 1.8568907  0.00331678 0.7423917  0.         0.28767502\n",
            "  0.00235649 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 7\n",
            "Done: [False]\n",
            "Action taken: [ 0.7185063 -5.436562 ]\n",
            "Observation: [[0.00316228 1.8666123  0.00314657 0.8181428  0.         0.422738\n",
            "  0.0022355  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 8\n",
            "Done: [False]\n",
            "Action taken: [  0.46407577 -45.299137  ]\n",
            "Observation: [[0.00301511 1.8717965  0.00300013 0.86931175 0.         1.5309436\n",
            "  0.00213142 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 9\n",
            "Done: [False]\n",
            "Action taken: [  0.9164338 -10.333415 ]\n",
            "Observation: [[0.00288675 1.8738937  0.00287241 0.90170836 0.         1.5469556\n",
            "  0.00204063 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 10\n",
            "Done: [False]\n",
            "Action taken: [ 0.59532756 -7.257145  ]\n",
            "Observation: [[0.0027735  1.8738408  0.00275972 0.9200606  0.         1.50474\n",
            "  0.00196052 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 11\n",
            "Done: [False]\n",
            "Action taken: [ 0.28073373 51.465664  ]\n",
            "Observation: [[0.00267261 1.8445687  0.00265933 0.6171377  0.         0.22054602\n",
            "  0.00188916 0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 12\n",
            "Done: [False]\n",
            "Action taken: [ 0.4463708 26.944227 ]\n",
            "Observation: [[ 0.00258199  1.7441883   0.00256916 -0.40554076  0.         -0.4517995\n",
            "   0.00182505  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 13\n",
            "Done: [False]\n",
            "Action taken: [ 0.35567147 16.388573  ]\n",
            "Observation: [[ 0.0025      1.6131577   0.00248757 -1.1944416   0.         -0.8358247\n",
            "   0.00176706  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 14\n",
            "Done: [False]\n",
            "Action taken: [  0.12320609 -16.457907  ]\n",
            "Observation: [[ 0.00242536  1.4665315   0.0024133  -1.691565    0.         -0.38438338\n",
            "   0.00171425  0.        ]]\n",
            "Reward: [0.]\n",
            "iteration: 15\n",
            "Done: [False]\n",
            "Action taken: [ 0.24042319 19.351013  ]\n",
            "Observation: [[ 2.3570217e-03  1.3092027e+00  2.3453042e-03 -1.9381622e+00\n",
            "   0.0000000e+00 -8.6695898e-01  1.6659149e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 16\n",
            "Done: [False]\n",
            "Action taken: [  0.18655924 -17.959087  ]\n",
            "Observation: [[ 2.2941565e-03  1.1418592e+00  2.2827503e-03 -2.0253127e+00\n",
            "   0.0000000e+00 -3.6259809e-01  1.6214420e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 17\n",
            "Done: [False]\n",
            "Action taken: [  0.96790594 -41.0301    ]\n",
            "Observation: [[ 2.2360671e-03  9.6338445e-01  2.2249487e-03 -2.0267417e+00\n",
            "   0.0000000e+00  7.5871706e-01  1.5803468e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 18\n",
            "Done: [False]\n",
            "Action taken: [0.7017081 3.5854235]\n",
            "Observation: [[ 2.1821782e-03  7.7192587e-01  2.1713264e-03 -1.9861789e+00\n",
            "   0.0000000e+00  6.3459569e-01  1.5422222e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 19\n",
            "Done: [False]\n",
            "Action taken: [  0.77030826 -41.90408   ]\n",
            "Observation: [[ 2.1320065e-03  5.6550777e-01  2.1214033e-03 -1.9266965e+00\n",
            "   0.0000000e+00  1.6746700e+00  1.5067265e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 20\n",
            "Done: [False]\n",
            "Action taken: [  0.3094432 -45.981106 ]\n",
            "Observation: [[ 2.0851435e-03  3.4255925e-01  2.0747723e-03 -1.8599910e+00\n",
            "   0.0000000e+00  2.1400669e+00  1.4735709e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 21\n",
            "Done: [False]\n",
            "Action taken: [ 0.7942574 32.457573 ]\n",
            "Observation: [[ 2.0412407e-03  1.0249945e-01  2.0310869e-03 -1.7919060e+00\n",
            "   0.0000000e+00  1.2562977e+00  1.4425089e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 22\n",
            "Done: [False]\n",
            "Action taken: [ 0.6075863 26.30358  ]\n",
            "Observation: [[ 1.9999992e-03 -1.5356228e-01  1.9900496e-03 -1.7253144e+00\n",
            "   0.0000000e+00  5.9224224e-01  1.4133290e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 23\n",
            "Done: [False]\n",
            "Action taken: [  0.8884083 -36.810303 ]\n",
            "Observation: [[ 1.9611607e-03 -4.2213637e-01  1.9514033e-03 -1.6615701e+00\n",
            "   0.0000000e+00  1.4198260e+00  1.3858486e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 24\n",
            "Done: [False]\n",
            "Action taken: [0.0472354 3.3264544]\n",
            "Observation: [[ 1.9245002e-03 -6.9692588e-01  1.9149244e-03 -1.6012443e+00\n",
            "   0.0000000e+00  1.2721534e+00  1.3599087e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 25\n",
            "Done: [False]\n",
            "Action taken: [  0.43109235 -19.13167   ]\n",
            "Observation: [[ 1.8898217e-03 -9.6905267e-01  1.8804175e-03 -1.5445025e+00\n",
            "   0.0000000e+00  1.6168070e+00  1.3353706e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 26\n",
            "Done: [False]\n",
            "Action taken: [ 0.8802128 59.96401  ]\n",
            "Observation: [[ 1.8569527e-03 -1.2281874e+00  1.8477112e-03 -1.4913013e+00\n",
            "   0.0000000e+00  2.1646821e-01  1.3121122e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 27\n",
            "Done: [False]\n",
            "Action taken: [ 0.66132665 24.251528  ]\n",
            "Observation: [[ 1.8257411e-03 -1.4644380e+00  1.8166540e-03 -1.4414941e+00\n",
            "   0.0000000e+00 -3.5100642e-01  1.2900262e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 28\n",
            "Done: [False]\n",
            "Action taken: [ 0.7641296 47.238796 ]\n",
            "Observation: [[ 1.7960523e-03 -1.6703099e+00  1.7871121e-03 -1.3948866e+00\n",
            "   0.0000000e+00 -1.4126148e+00  1.2690171e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 29\n",
            "Done: [False]\n",
            "Action taken: [  0.50430167 -59.638405  ]\n",
            "Observation: [[ 1.7677663e-03 -1.8419286e+00  1.7589660e-03 -1.3512671e+00\n",
            "   0.0000000e+00 -4.9078935e-03  1.2490002e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 30\n",
            "Done: [False]\n",
            "Action taken: [ 0.73018974 20.362211  ]\n",
            "Observation: [[ 1.7407759e-03 -1.9791149e+00  1.7321091e-03 -1.3104233e+00\n",
            "   0.0000000e+00 -4.8389071e-01  1.2298997e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 31\n",
            "Done: [False]\n",
            "Action taken: [ 0.1464558 39.35491  ]\n",
            "Observation: [[ 1.7149852e-03 -2.0845344e+00  1.7064459e-03 -1.2721496e+00\n",
            "   0.0000000e+00 -1.3761669e+00  1.2116478e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 32\n",
            "Done: [False]\n",
            "Action taken: [ 0.8413804 37.923004 ]\n",
            "Observation: [[ 1.6903079e-03 -2.1624827e+00  1.6818907e-03 -1.2362522e+00\n",
            "   0.0000000e+00 -2.0992098e+00  1.1941832e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 33\n",
            "Done: [False]\n",
            "Action taken: [  0.5088845 -49.014156 ]\n",
            "Observation: [[ 1.6666660e-03 -2.2177980e+00  1.6583657e-03 -1.2025506e+00\n",
            "   0.0000000e+00 -9.6300542e-01  1.1774512e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 34\n",
            "Done: [False]\n",
            "Action taken: [ 0.19393054 45.637917  ]\n",
            "Observation: [[ 1.6439892e-03 -2.2551239e+00  1.6358010e-03 -1.1708773e+00\n",
            "   0.0000000e+00 -1.8712208e+00  1.1614017e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 35\n",
            "Done: [False]\n",
            "Action taken: [  0.3590923 -38.29839  ]\n",
            "Observation: [[ 1.6222136e-03 -2.2785258e+00  1.6141331e-03 -1.1410784e+00\n",
            "   0.0000000e+00 -1.0078856e+00  1.1459898e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 36\n",
            "Done: [False]\n",
            "Action taken: [ 0.6982691 16.93639  ]\n",
            "Observation: [[ 1.6012810e-03 -2.2913666e+00  1.5933039e-03 -1.1130118e+00\n",
            "   0.0000000e+00 -1.3274773e+00  1.1311739e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 37\n",
            "Done: [False]\n",
            "Action taken: [  0.10004698 -10.078791  ]\n",
            "Observation: [[ 1.5811382e-03 -2.2963235e+00  1.5732608e-03 -1.0865481e+00\n",
            "   0.0000000e+00 -1.0796171e+00  1.1169169e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 38\n",
            "Done: [False]\n",
            "Action taken: [ 0.05138843 24.712902  ]\n",
            "Observation: [[ 1.5617370e-03 -2.2861414e+00  1.5539555e-03 -9.8343801e-01\n",
            "   0.0000000e+00 -1.5434289e+00  1.1031844e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 39\n",
            "Done: [False]\n",
            "Action taken: [ 0.08013506 -9.784258  ]\n",
            "Observation: [[ 1.5430329e-03 -2.2244804e+00  1.5353438e-03 -5.2116776e-01\n",
            "   0.0000000e+00 -1.2935085e+00  1.0899450e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 40\n",
            "Done: [False]\n",
            "Action taken: [ 0.85945714 -1.3182862 ]\n",
            "Observation: [[ 1.5249852e-03 -2.1328459e+00  1.5173851e-03 -1.3769242e-01\n",
            "   0.0000000e+00 -1.2284176e+00  1.0771698e-03  0.0000000e+00]]\n",
            "Reward: [0.]\n",
            "iteration: 41\n",
            "Done: [False]\n",
            "Action taken: [ 0.85410506 12.912695  ]\n",
            "Observation: [[ 1.5075562e-03 -2.0242293e+00 -6.5573616e+00  1.8200399e-01\n",
            "   0.0000000e+00 -1.4436206e+00 -6.5567737e+00  0.0000000e+00]]\n",
            "Reward: [4.6589456]\n",
            "iteration: 42\n",
            "Done: [False]\n",
            "Action taken: [  0.3407247 -27.913404 ]\n",
            "Observation: [[-5.992712   -1.9328806  -4.636784    0.17990416  0.         -0.8537511\n",
            "  -4.6365714   0.        ]]\n",
            "Reward: [2.7635415]\n",
            "iteration: 43\n",
            "Done: [False]\n",
            "Action taken: [ 0.48002842 33.299244  ]\n",
            "Observation: [[-5.8542175  -1.8548477  -3.7859263   0.17787538  0.         -1.484683\n",
            "  -3.785808    0.        ]]\n",
            "Reward: [1.7930562]\n",
            "iteration: 44\n",
            "Done: [False]\n",
            "Action taken: [  0.75772536 -23.637781  ]\n",
            "Observation: [[-5.3685966  -1.7873394  -3.2787118   0.17591372  0.         -0.9836325\n",
            "  -3.2786334   0.        ]]\n",
            "Reward: [1.2775941]\n",
            "iteration: 45\n",
            "Done: [False]\n",
            "Action taken: [  0.66033006 -20.706667  ]\n",
            "Observation: [[-4.952422   -1.7283216  -2.9325707   0.17401555  0.         -0.5555515\n",
            "  -2.9325135   0.        ]]\n",
            "Reward: [0.9727384]\n",
            "iteration: 46\n",
            "Done: [False]\n",
            "Action taken: [ 0.5552527 58.528923 ]\n",
            "Observation: [[-4.61782    -1.6762712  -2.6770597   0.17217754  0.         -1.682437\n",
            "  -2.6770153   0.        ]]\n",
            "Reward: [0.77609366]\n",
            "iteration: 47\n",
            "Done: [False]\n",
            "Action taken: [  0.05660375 -40.66336   ]\n",
            "Observation: [[-4.3466907  -1.6300225  -2.4784765   0.17039657  0.         -0.85239094\n",
            "  -2.4784405   0.        ]]\n",
            "Reward: [0.6407326]\n",
            "iteration: 48\n",
            "Done: [False]\n",
            "Action taken: [  0.83044255 -12.6361265 ]\n",
            "Observation: [[-4.1231008  -1.5886662  -2.318403    0.16866975  0.         -0.590516\n",
            "  -2.318373    0.        ]]\n",
            "Reward: [0.542851]\n",
            "iteration: 49\n",
            "Done: [False]\n",
            "Action taken: [0.05897643 9.613934  ]\n",
            "Observation: [[-3.9354851  -1.5514809  -2.1858118   0.16699438  0.         -0.77278876\n",
            "  -2.185786    0.        ]]\n",
            "Reward: [0.469309]\n",
            "iteration: 50\n",
            "Done: [False]\n",
            "Action taken: [  0.5007581 -20.218449 ]\n",
            "Observation: [[-3.7756262  -1.5178858  -2.0736432   0.16536796  0.         -0.35846978\n",
            "  -2.073621    0.        ]]\n",
            "Reward: [0.41234434]\n",
            "iteration: 51\n",
            "Done: [False]\n",
            "Action taken: [ 0.09584906 42.801594  ]\n",
            "Observation: [[-3.637612   -1.4874077  -1.9771415   0.16378815  0.         -1.2061964\n",
            "  -1.9771218   0.        ]]\n",
            "Reward: [0.36711305]\n",
            "iteration: 52\n",
            "Done: [False]\n",
            "Action taken: [  0.91847754 -54.702038  ]\n",
            "Observation: [[-3.5171072  -1.4596565  -1.8929691   0.16225277  0.         -0.09075116\n",
            "  -1.8929514   0.        ]]\n",
            "Reward: [0.3304536]\n",
            "iteration: 53\n",
            "Done: [False]\n",
            "Action taken: [  0.19899096 -15.028609  ]\n",
            "Observation: [[-3.410864   -1.4343067  -1.818706    0.16075978  0.          0.2162192\n",
            "  -1.8186901   0.        ]]\n",
            "Reward: [0.30022427]\n",
            "iteration: 54\n",
            "Done: [False]\n",
            "Action taken: [ 0.9264303 14.446539 ]\n",
            "Observation: [[-3.3164053  -1.4110855  -1.752549    0.15930726  0.         -0.08265302\n",
            "  -1.7525345   0.        ]]\n",
            "Reward: [0.27492782]\n",
            "iteration: 55\n",
            "Done: [False]\n",
            "Action taken: [  0.6612774 -13.120953 ]\n",
            "Observation: [[-3.2318053  -1.3897609  -1.6931233   0.15789342  0.          0.19014776\n",
            "  -1.69311     0.        ]]\n",
            "Reward: [0.25348917]\n",
            "iteration: 56\n",
            "Done: [False]\n",
            "Action taken: [  0.20632106 -23.957212  ]\n",
            "Observation: [[-3.1555455  -1.3701348  -1.6393597   0.15651655  0.          0.68687737\n",
            "  -1.6393474   0.        ]]\n",
            "Reward: [0.2351183]\n",
            "iteration: 57\n",
            "Done: [False]\n",
            "Action taken: [ 0.74001575 20.40171   ]\n",
            "Observation: [[-3.0864103  -1.3520375  -1.5904126   0.15517509  0.          0.25224108\n",
            "  -1.5904012   0.        ]]\n",
            "Reward: [0.21922284]\n",
            "iteration: 58\n",
            "Done: [False]\n",
            "Action taken: [ 0.25063646 -8.79547   ]\n",
            "Observation: [[-3.023415   -1.3353214  -1.5456034   0.15386754  0.          0.4358002\n",
            "  -1.5455927   0.        ]]\n",
            "Reward: [0.20535074]\n",
            "iteration: 59\n",
            "Done: [False]\n",
            "Action taken: [  0.4487116 -49.94898  ]\n",
            "Observation: [[-2.9657524  -1.3198583  -1.5043799   0.15259251  0.          1.4708469\n",
            "  -1.5043699   0.        ]]\n",
            "Reward: [0.1931515]\n",
            "iteration: 60\n",
            "Done: [False]\n",
            "Action taken: [ 0.92130417 55.149216  ]\n",
            "Observation: [[-2.9127536  -1.305536   -1.4662882   0.15134865  0.          0.2940967\n",
            "  -1.4662788   0.        ]]\n",
            "Reward: [0.1823496]\n",
            "iteration: 61\n",
            "Done: [False]\n",
            "Action taken: [3.0233907e-02 3.7347672e+01]\n",
            "Observation: [[-2.8638601  -1.2922556  -1.4309508   0.15013471  0.         -0.5019172\n",
            "  -1.4309419   0.        ]]\n",
            "Reward: [0.1727257]\n",
            "iteration: 62\n",
            "Done: [False]\n",
            "Action taken: [ 0.5127765 35.97593  ]\n",
            "Observation: [[-2.8186011  -1.27993    -1.398051    0.14894953  0.         -1.252528\n",
            "  -1.3980426   0.        ]]\n",
            "Reward: [0.16410322]\n",
            "iteration: 63\n",
            "Done: [False]\n",
            "Action taken: [7.467630e-03 2.049722e+01]\n",
            "Observation: [[-2.7765768  -1.2684816  -1.3673209   0.14779198  0.         -1.644479\n",
            "  -1.3673129   0.        ]]\n",
            "Reward: [0.15633848]\n",
            "iteration: 64\n",
            "Done: [False]\n",
            "Action taken: [ 0.3406736 48.578114 ]\n",
            "Observation: [[-2.737445   -1.2578413  -1.338532    0.14666101  0.         -2.5223362\n",
            "  -1.3385243   0.        ]]\n",
            "Reward: [0.14931355]\n",
            "iteration: 65\n",
            "Done: [False]\n",
            "Action taken: [ 0.88884884 11.406866  ]\n",
            "Observation: [[-2.7009118  -1.2479476  -1.3114882   0.14555562  0.         -2.5926893\n",
            "  -1.3114809   0.        ]]\n",
            "Reward: [0.14293076]\n",
            "iteration: 66\n",
            "Done: [False]\n",
            "Action taken: [  0.13509543 -27.95238   ]\n",
            "Observation: [[-2.666722   -1.2387446  -1.28602     0.14447483  0.         -1.9765341\n",
            "  -1.286013    0.        ]]\n",
            "Reward: [0.13710856]\n",
            "iteration: 67\n",
            "Done: [False]\n",
            "Action taken: [  0.3464731 -29.829815 ]\n",
            "Observation: [[-2.6346538  -1.2301828  -1.2619802   0.14341778  0.         -1.3813677\n",
            "  -1.2619735   0.        ]]\n",
            "Reward: [0.13177843]\n",
            "iteration: 68\n",
            "Done: [False]\n",
            "Action taken: [  0.74946994 -53.61061   ]\n",
            "Observation: [[-2.6045132  -1.2222167  -1.2392399   0.14238359  0.         -0.3688892\n",
            "  -1.2392335   0.        ]]\n",
            "Reward: [0.1268824]\n",
            "iteration: 69\n",
            "Done: [False]\n",
            "Action taken: [ 0.03119264 -2.1429412 ]\n",
            "Observation: [[-2.5761294  -1.2148055  -1.2176863   0.14137146  0.         -0.32575852\n",
            "  -1.2176801   0.        ]]\n",
            "Reward: [0.12237103]\n",
            "iteration: 70\n",
            "Done: [False]\n",
            "Action taken: [  0.73449945 -42.097706  ]\n",
            "Observation: [[-2.549352   -1.2079121  -1.1972195   0.1403806   0.          0.47279352\n",
            "  -1.1972136   0.        ]]\n",
            "Reward: [0.11820205]\n",
            "iteration: 71\n",
            "Done: [False]\n",
            "Action taken: [0.8618101 6.5389543]\n",
            "Observation: [[-2.5240467  -1.2015024  -1.1777513   0.1394103   0.          0.34482813\n",
            "  -1.1777456   0.        ]]\n",
            "Reward: [0.11433902]\n",
            "iteration: 72\n",
            "Done: [False]\n",
            "Action taken: [ 0.6805953 -4.9536405]\n",
            "Observation: [[-2.5000951  -1.1955457  -1.1592028   0.13845985  0.          0.43683243\n",
            "  -1.1591973   0.        ]]\n",
            "Reward: [0.11075043]\n",
            "iteration: 73\n",
            "Done: [False]\n",
            "Action taken: [  0.6678308 -30.724606 ]\n",
            "Observation: [[-2.477391   -1.1900133  -1.141504    0.13752857  0.          1.018537\n",
            "  -1.1414987   0.        ]]\n",
            "Reward: [0.10740885]\n",
            "iteration: 74\n",
            "Done: [False]\n",
            "Action taken: [ 0.78770196 -0.68542796]\n",
            "Observation: [[-2.455839   -1.1848794  -1.124592    0.13661583  0.          1.0180842\n",
            "  -1.1245868   0.        ]]\n",
            "Reward: [0.10429037]\n",
            "iteration: 75\n",
            "Done: [False]\n",
            "Action taken: [ 0.22559123 13.623952  ]\n",
            "Observation: [[-2.4353535  -1.18012    -1.10841     0.13572103  0.          0.74657613\n",
            "  -1.108405    0.        ]]\n",
            "Reward: [0.10137396]\n",
            "iteration: 76\n",
            "Done: [False]\n",
            "Action taken: [0.10612041 8.124462  ]\n",
            "Observation: [[-2.4158573  -1.1757131  -1.092907    0.13484357  0.          0.5837167\n",
            "  -1.0929021   0.        ]]\n",
            "Reward: [0.09864117]\n",
            "iteration: 77\n",
            "Done: [False]\n",
            "Action taken: [ 0.6005148 44.95156  ]\n",
            "Observation: [[-2.397281   -1.1716381  -1.0780368   0.13398293  0.         -0.2912669\n",
            "  -1.078032    0.        ]]\n",
            "Reward: [0.09607567]\n",
            "iteration: 78\n",
            "Done: [False]\n",
            "Action taken: [ 0.50768405 52.67766   ]\n",
            "Observation: [[-2.3795607  -1.1678764  -1.0637575   0.13313855  0.         -1.3026628\n",
            "  -1.0637529   0.        ]]\n",
            "Reward: [0.09366297]\n",
            "iteration: 79\n",
            "Done: [False]\n",
            "Action taken: [  0.4495603 -57.753292 ]\n",
            "Observation: [[-2.3626394  -1.1644104  -1.0500311   0.13230994  0.         -0.17342803\n",
            "  -1.0500265   0.        ]]\n",
            "Reward: [0.0913902]\n",
            "iteration: 80\n",
            "Done: [False]\n",
            "Action taken: [ 0.71540385 33.31872   ]\n",
            "Observation: [[-2.3464644  -1.1612236  -1.0368227   0.13149662  0.         -0.8197654\n",
            "  -1.0368183   0.        ]]\n",
            "Reward: [0.08924587]\n",
            "iteration: 81\n",
            "Done: [False]\n",
            "Action taken: [  0.3913889 -28.316359 ]\n",
            "Observation: [[-2.330988  -1.1583014 -1.0241004  0.1306981  0.        -0.2605854\n",
            "  -1.0240961  0.       ]]\n",
            "Reward: [0.08721969]\n",
            "iteration: 82\n",
            "Done: [False]\n",
            "Action taken: [ 0.7028882 20.431341 ]\n",
            "Observation: [[-2.316166   -1.1556293  -1.0118353   0.12991397  0.         -0.6594615\n",
            "  -1.0118312   0.        ]]\n",
            "Reward: [0.08530243]\n",
            "iteration: 83\n",
            "Done: [False]\n",
            "Action taken: [ 0.0446762 -6.669329 ]\n",
            "Observation: [[-2.3019586  -1.1531943  -1.0000006   0.12914377  0.         -0.52306145\n",
            "  -0.9999965   0.        ]]\n",
            "Reward: [0.08348577]\n",
            "iteration: 84\n",
            "Done: [False]\n",
            "Action taken: [ 5.7550524e-03 -4.1565945e+01]\n",
            "Observation: [[-2.288329   -1.1509842  -0.98857164  0.12838712  0.          0.3042045\n",
            "  -0.98856765  0.        ]]\n",
            "Reward: [0.08176224]\n",
            "iteration: 85\n",
            "Done: [False]\n",
            "Action taken: [  0.32494295 -14.228297  ]\n",
            "Observation: [[-2.2752426  -1.1489874  -0.9775258   0.12764362  0.          0.5850216\n",
            "  -0.9775219   0.        ]]\n",
            "Reward: [0.08012503]\n",
            "iteration: 86\n",
            "Done: [False]\n",
            "Action taken: [  0.5666315 -15.881701 ]\n",
            "Observation: [[-2.2626686  -1.1471933  -0.9668421   0.12691286  0.          0.8954983\n",
            "  -0.9668383   0.        ]]\n",
            "Reward: [0.07856803]\n",
            "iteration: 87\n",
            "Done: [False]\n",
            "Action taken: [ 0.72231734 39.90718   ]\n",
            "Observation: [[-2.250578   -1.1455917  -0.9565013   0.12619454  0.          0.09083375\n",
            "  -0.95649755  0.        ]]\n",
            "Reward: [0.07708562]\n",
            "iteration: 88\n",
            "Done: [False]\n",
            "Action taken: [  0.4329661 -57.633133 ]\n",
            "Observation: [[-2.2389436  -1.1441733  -0.9464853   0.12548827  0.          1.2409357\n",
            "  -0.94648165  0.        ]]\n",
            "Reward: [-9.546293]\n",
            "iteration: 89\n",
            "Done: [False]\n",
            "Action taken: [0.66747737 5.939556  ]\n",
            "Observation: [[ 0.79486847  0.8188046   1.0444665   2.496789    0.         -0.62170136\n",
            "   1.0444647   0.        ]]\n",
            "Reward: [0.00017334]\n",
            "iteration: 90\n",
            "Done: [ True]\n",
            "No frames directory found, skipping video creation.\n"
          ]
        }
      ],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0') for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv())\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "R172XbXX5Am5",
        "outputId": "25aa3ba4-9080-4bc4-ef9c-d561c4caa346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: (array([1.163e+03, 4.450e+02, 1.000e+01, 1.000e+01, 1.000e+01, 3.550e+02,\n",
            "       1.000e+00, 0.000e+00], dtype=float32), {})\n",
            "Action taken: [  0.82033885 -21.23941   ]\n",
            "Observation: [1.163000e+03 4.450000e+02 1.000000e+01 0.000000e+00 1.000000e+01\n",
            " 4.080985e+02 1.000000e+00 0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 0\n",
            "Done: False\n",
            "Action taken: [ 0.15382095 28.321695  ]\n",
            "Observation: [ 1.1630000e+03  4.3493808e+02  1.0000000e+01 -4.0247712e+00\n",
            "  1.0000000e+01  3.3729428e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 1\n",
            "Done: False\n",
            "Action taken: [0.3621889 3.593    ]\n",
            "Observation: [ 1.1630000e+03  4.1649121e+02  1.0000000e+01 -7.3787475e+00\n",
            "  1.0000000e+01  3.2831180e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 2\n",
            "Done: False\n",
            "Action taken: [ 0.42223373 34.00359   ]\n",
            "Observation: [ 1.1630000e+03  3.9105688e+02  1.0000000e+01 -1.0173728e+01\n",
            "  1.0000000e+01  2.4330281e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 3\n",
            "Done: False\n",
            "Action taken: [ 0.84272474 47.882828  ]\n",
            "Observation: [ 1.1630000e+03  3.5979968e+02  1.0000000e+01 -1.2502877e+01\n",
            "  1.0000000e+01  1.2359574e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 4\n",
            "Done: False\n",
            "Action taken: [ 0.49130207 23.680483  ]\n",
            "Observation: [ 1.1630000e+03  3.2369009e+02  1.0000000e+01 -1.4443836e+01\n",
            "  1.0000000e+01  6.4394539e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 5\n",
            "Done: False\n",
            "Action taken: [  0.667912 -51.841423]\n",
            "Observation: [ 1.1630000e+03  2.8474176e+02  1.0000000e+01 -1.5579340e+01\n",
            "  1.0000000e+01  1.9399809e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 6\n",
            "Done: False\n",
            "Action taken: [ 0.2965679 -9.031181 ]\n",
            "Observation: [ 1.1630000e+03  2.6234671e+02  1.0000000e+01 -8.9580116e+00\n",
            "  1.0000000e+01  2.1657605e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 7\n",
            "Done: False\n",
            "Action taken: [ 0.22034763 16.575354  ]\n",
            "Observation: [ 1.1630000e+03  2.5374612e+02  1.0000000e+01 -3.4402387e+00\n",
            "  1.0000000e+01  1.7513766e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 8\n",
            "Done: False\n",
            "Action taken: [  0.8144069 -30.173744 ]\n",
            "Observation: [1.1630000e+03 2.5664090e+02 1.0000000e+01 1.1579058e+00 1.0000000e+01\n",
            " 2.5057202e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 9\n",
            "Done: False\n",
            "Action taken: [ 0.28144467 -3.028356  ]\n",
            "Observation: [1.1630000e+03 2.6911511e+02 1.0000000e+01 4.9896927e+00 1.0000000e+01\n",
            " 2.5814291e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 10\n",
            "Done: False\n",
            "Action taken: [ 0.46842152 42.5917    ]\n",
            "Observation: [1.1630000e+03 2.8957224e+02 1.0000000e+01 8.1828489e+00 1.0000000e+01\n",
            " 1.5166367e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 11\n",
            "Done: False\n",
            "Action taken: [9.2515792e-04 3.2916534e+00]\n",
            "Observation: [1.1630000e+03 3.1668176e+02 1.0000000e+01 1.0843812e+01 1.0000000e+01\n",
            " 1.4343452e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 12\n",
            "Done: False\n",
            "Action taken: [  0.09194581 -12.997368  ]\n",
            "Observation: [1.1630000e+03 3.4933496e+02 1.0000000e+01 1.3061281e+01 1.0000000e+01\n",
            " 1.7592795e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 13\n",
            "Done: False\n",
            "Action taken: [0.93943715 9.533784  ]\n",
            "Observation: [1.1630000e+03 3.8660791e+02 1.0000000e+01 1.4909172e+01 1.0000000e+01\n",
            " 1.5209349e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 14\n",
            "Done: False\n",
            "Action taken: [ 0.5334748 16.737316 ]\n",
            "Observation: [1.1630000e+03 4.2773062e+02 1.0000000e+01 1.6449081e+01 1.0000000e+01\n",
            " 1.1025020e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 15\n",
            "Done: False\n",
            "Action taken: [  0.79498434 -40.45139   ]\n",
            "Observation: [1.1630000e+03 4.7206146e+02 1.0000000e+01 1.7732340e+01 1.0000000e+01\n",
            " 2.1137868e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 16\n",
            "Done: False\n",
            "Action taken: [ 0.9912705 27.129353 ]\n",
            "Observation: [1.1630000e+03 5.1171790e+02 1.0000000e+01 1.5862570e+01 1.0000000e+01\n",
            " 1.4355528e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 17\n",
            "Done: False\n",
            "Action taken: [ 0.44432586 58.450912  ]\n",
            "Observation: [1.1630000e+03 5.3470300e+02 1.0000000e+01 9.1940365e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 18\n",
            "Done: False\n",
            "Action taken: [ 0.71978  45.284058]\n",
            "Observation: [1.1630000e+03 5.4379529e+02 1.0000000e+01 3.6369257e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 19\n",
            "Done: False\n",
            "Action taken: [  0.6998444 -39.84134  ]\n",
            "Observation: [ 1.1630000e+03  5.4131030e+02  1.0000000e+01 -9.9399996e-01\n",
            "  1.0000000e+01  9.9603348e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 20\n",
            "Done: False\n",
            "Action taken: [ 0.7490136 44.504677 ]\n",
            "Observation: [ 1.1630000e+03  5.2917755e+02  1.0000000e+01 -4.8531046e+00\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 21\n",
            "Done: False\n",
            "Action taken: [ 0.33910388 -2.4860702 ]\n",
            "Observation: [ 1.1630000e+03  5.0900497e+02  1.0000000e+01 -8.0690250e+00\n",
            "  1.0000000e+01  6.2151756e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 22\n",
            "Done: False\n",
            "Action taken: [ 0.23645842 57.854385  ]\n",
            "Observation: [ 1.1630000e+03  4.8213257e+02  1.0000000e+01 -1.0748959e+01\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 23\n",
            "Done: False\n",
            "Action taken: [ 0.20611498 -5.4504485 ]\n",
            "Observation: [ 1.16300000e+03  4.49676971e+02  1.00000000e+01 -1.29822369e+01\n",
            "  1.00000000e+01  1.36261215e+01  1.00000000e+00  0.00000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 24\n",
            "Done: False\n",
            "Action taken: [ 0.7879658 40.96347  ]\n",
            "Observation: [ 1.1630000e+03  4.1256873e+02  1.0000000e+01 -1.4843303e+01\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 25\n",
            "Done: False\n",
            "Action taken: [ 0.9031449 56.3267   ]\n",
            "Observation: [ 1.1630000e+03  3.7158325e+02  1.0000000e+01 -1.6394190e+01\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 26\n",
            "Done: False\n",
            "Action taken: [ 0.68240154 20.611088  ]\n",
            "Observation: [ 1.1630000e+03  3.2736676e+02  1.0000000e+01 -1.7686596e+01\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 27\n",
            "Done: False\n",
            "Action taken: [ 0.18642165 12.57699   ]\n",
            "Observation: [ 1.16300000e+03  2.87805634e+02  1.00000000e+01 -1.58244505e+01\n",
            "  1.00000000e+01  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 28\n",
            "Done: False\n",
            "Action taken: [ 0.87198037 35.483784  ]\n",
            "Observation: [ 1.1630000e+03  2.6489996e+02  1.0000000e+01 -9.1622705e+00\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 29\n",
            "Done: False\n",
            "Action taken: [2.3137052e-02 2.6450714e+01]\n",
            "Observation: [ 1.1630000e+03  2.5587381e+02  1.0000000e+01 -3.6104538e+00\n",
            "  1.0000000e+01  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 30\n",
            "Done: False\n",
            "Action taken: [ 0.12755832 22.062836  ]\n",
            "Observation: [1.1630000e+03 2.5841397e+02 1.0000000e+01 1.0160598e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 31\n",
            "Done: False\n",
            "Action taken: [ 0.66081667 47.12544   ]\n",
            "Observation: [1.1630000e+03 2.7059268e+02 1.0000000e+01 4.8714876e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 32\n",
            "Done: False\n",
            "Action taken: [  0.49913085 -23.192749  ]\n",
            "Observation: [1.1630000e+03 2.9080356e+02 1.0000000e+01 8.0843449e+00 1.0000000e+01\n",
            " 5.7981873e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 33\n",
            "Done: False\n",
            "Action taken: [ 0.39997754 19.171034  ]\n",
            "Observation: [1.1630000e+03 3.1770786e+02 1.0000000e+01 1.0761725e+01 1.0000000e+01\n",
            " 1.0054288e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 34\n",
            "Done: False\n",
            "Action taken: [ 0.94825417 57.977673  ]\n",
            "Observation: [1.1630000e+03 3.5019003e+02 1.0000000e+01 1.2992876e+01 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 35\n",
            "Done: False\n",
            "Action taken: [ 0.9500755 -9.509879 ]\n",
            "Observation: [1.1630000e+03 3.8732047e+02 1.0000000e+01 1.4852168e+01 1.0000000e+01\n",
            " 2.3774698e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 36\n",
            "Done: False\n",
            "Action taken: [ 0.39065003 33.613506  ]\n",
            "Observation: [1.1630000e+03 4.2832440e+02 1.0000000e+01 1.6401577e+01 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 37\n",
            "Done: False\n",
            "Action taken: [  0.90163285 -24.229162  ]\n",
            "Observation: [1.1630000e+03 4.7255630e+02 1.0000000e+01 1.7692753e+01 1.0000000e+01\n",
            " 6.0572906e+01 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 38\n",
            "Done: False\n",
            "Action taken: [  0.7557239 -20.921833 ]\n",
            "Observation: [1.1630000e+03 5.1213025e+02 1.0000000e+01 1.5829580e+01 1.0000000e+01\n",
            " 1.1287749e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 39\n",
            "Done: False\n",
            "Action taken: [ 0.21829076 46.844364  ]\n",
            "Observation: [1.1630000e+03 5.3504663e+02 1.0000000e+01 9.1665459e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 40\n",
            "Done: False\n",
            "Action taken: [ 0.7797512 19.331182 ]\n",
            "Observation: [1.1630000e+03 5.4408167e+02 1.0000000e+01 3.6140168e+00 1.0000000e+01\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 41\n",
            "Done: False\n",
            "Action taken: [ 0.01835257 -7.2529397 ]\n",
            "Observation: [ 1.1630000e+03  5.4154895e+02  1.0000000e+01 -1.0130907e+00\n",
            "  1.0000000e+01  1.8132349e+01  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 42\n",
            "Done: False\n",
            "Action taken: [ 0.41894215 36.55191   ]\n",
            "Observation: [ 1.163000e+03  5.293764e+02  1.000000e+01 -4.869014e+00  1.000000e+01\n",
            "  0.000000e+00  1.000000e+00  0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 43\n",
            "Done: False\n",
            "Action taken: [  0.9254982 -37.289696 ]\n",
            "Observation: [ 1.163000e+03  5.091707e+02  1.000000e+01 -8.082283e+00  1.000000e+01\n",
            "  9.322424e+01  1.000000e+00  0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 44\n",
            "Done: False\n",
            "Action taken: [  0.28232905 -47.91197   ]\n",
            "Observation: [ 1.1630000e+03  4.8227066e+02  1.0000000e+01 -1.0760007e+01\n",
            "  1.0000000e+01  2.1300417e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 45\n",
            "Done: False\n",
            "Action taken: [ 0.60573167 17.494179  ]\n",
            "Observation: [ 1.1630000e+03  4.4979205e+02  1.0000000e+01 -1.2991444e+01\n",
            "  1.0000000e+01  1.6926871e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 46\n",
            "Done: False\n",
            "Action taken: [  0.3637522 -52.87928  ]\n",
            "Observation: [ 1.1630000e+03  4.1266461e+02  1.0000000e+01 -1.4850974e+01\n",
            "  1.0000000e+01  3.0146692e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 47\n",
            "Done: False\n",
            "Action taken: [ 0.76802695 -6.4377465 ]\n",
            "Observation: [ 1.1630000e+03  3.7166315e+02  1.0000000e+01 -1.6400583e+01\n",
            "  1.0000000e+01  3.1756128e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 48\n",
            "Done: False\n",
            "Action taken: [0.48514858 7.1730423 ]\n",
            "Observation: [ 1.1630000e+03  3.2743335e+02  1.0000000e+01 -1.7691925e+01\n",
            "  1.0000000e+01  2.9962866e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 49\n",
            "Done: False\n",
            "Action taken: [ 0.29276833 43.660816  ]\n",
            "Observation: [ 1.1630000e+03  2.8786111e+02  1.0000000e+01 -1.5828890e+01\n",
            "  1.0000000e+01  1.9047664e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 50\n",
            "Done: False\n",
            "Action taken: [  0.1493574 -31.974192 ]\n",
            "Observation: [ 1.163000e+03  2.649462e+02  1.000000e+01 -9.165970e+00  1.000000e+01\n",
            "  2.704121e+02  1.000000e+00  0.000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 51\n",
            "Done: False\n",
            "Action taken: [ 0.12311135 28.010298  ]\n",
            "Observation: [ 1.1630000e+03  2.5591235e+02  1.0000000e+01 -3.6135373e+00\n",
            "  1.0000000e+01  2.0038637e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 52\n",
            "Done: False\n",
            "Action taken: [  0.38488576 -38.843266  ]\n",
            "Observation: [1.1630000e+03 2.5844608e+02 1.0000000e+01 1.0134903e+00 1.0000000e+01\n",
            " 2.9749454e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 53\n",
            "Done: False\n",
            "Action taken: [ 0.2959999 33.631638 ]\n",
            "Observation: [1.1630000e+03 2.7061945e+02 1.0000000e+01 4.8693466e+00 1.0000000e+01\n",
            " 2.1341544e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 54\n",
            "Done: False\n",
            "Action taken: [ 0.5855873 -9.440193 ]\n",
            "Observation: [1.1630000e+03 2.9082584e+02 1.0000000e+01 8.0825605e+00 1.0000000e+01\n",
            " 2.3701591e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 55\n",
            "Done: False\n",
            "Action taken: [ 0.24640101 44.12464   ]\n",
            "Observation: [1.16300000e+03 3.17726440e+02 1.00000000e+01 1.07602386e+01\n",
            " 1.00000000e+01 1.26704315e+02 1.00000000e+00 0.00000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 56\n",
            "Done: False\n",
            "Action taken: [  0.31795985 -52.08467   ]\n",
            "Observation: [1.1630000e+03 3.5020554e+02 1.0000000e+01 1.2991636e+01 1.0000000e+01\n",
            " 2.5691599e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 57\n",
            "Done: False\n",
            "Action taken: [  0.3112468 -59.713852 ]\n",
            "Observation: [1.1630000e+03 3.8733337e+02 1.0000000e+01 1.4851135e+01 1.0000000e+01\n",
            " 4.0620062e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 58\n",
            "Done: False\n",
            "Action taken: [ 0.9535598 46.277145 ]\n",
            "Observation: [1.1630000e+03 4.2833517e+02 1.0000000e+01 1.6400717e+01 1.0000000e+01\n",
            " 2.9050775e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 59\n",
            "Done: False\n",
            "Action taken: [ 0.81965566 -6.032918  ]\n",
            "Observation: [1.1630000e+03 4.7256525e+02 1.0000000e+01 1.7692036e+01 1.0000000e+01\n",
            " 3.0559006e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 60\n",
            "Done: False\n",
            "Action taken: [  0.1797183 -13.209887 ]\n",
            "Observation: [1.1630000e+03 5.1213770e+02 1.0000000e+01 1.5828983e+01 1.0000000e+01\n",
            " 3.3861478e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 61\n",
            "Done: False\n",
            "Action taken: [0.11202957 8.214447  ]\n",
            "Observation: [1.1630000e+03 5.3505286e+02 1.0000000e+01 9.1660480e+00 1.0000000e+01\n",
            " 3.1807864e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 62\n",
            "Done: False\n",
            "Action taken: [ 0.63873667 36.55384   ]\n",
            "Observation: [1.1630000e+03 5.4408685e+02 1.0000000e+01 3.6136017e+00 1.0000000e+01\n",
            " 2.2669405e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 63\n",
            "Done: False\n",
            "Action taken: [0.1624003 9.48734  ]\n",
            "Observation: [ 1.1630000e+03  5.4155322e+02  1.0000000e+01 -1.0134366e+00\n",
            "  1.0000000e+01  2.0297571e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 64\n",
            "Done: False\n",
            "Action taken: [ 0.410991 37.65895 ]\n",
            "Observation: [ 1.1630000e+03  5.2938000e+02  1.0000000e+01 -4.8693018e+00\n",
            "  1.0000000e+01  1.0882832e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 65\n",
            "Done: False\n",
            "Action taken: [  0.68655187 -49.998432  ]\n",
            "Observation: [ 1.1630000e+03  5.0917368e+02  1.0000000e+01 -8.0825233e+00\n",
            "  1.0000000e+01  2.3382440e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 66\n",
            "Done: False\n",
            "Action taken: [ 0.8068716 20.624205 ]\n",
            "Observation: [ 1.1630000e+03  4.8227316e+02  1.0000000e+01 -1.0760207e+01\n",
            "  1.0000000e+01  1.8226390e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 67\n",
            "Done: False\n",
            "Action taken: [  0.6483396 -59.59931  ]\n",
            "Observation: [ 1.1630000e+03  4.4979413e+02  1.0000000e+01 -1.2991611e+01\n",
            "  1.0000000e+01  3.3126218e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 68\n",
            "Done: False\n",
            "Action taken: [  0.34639758 -45.11011   ]\n",
            "Observation: [ 1.1630000e+03  4.1266635e+02  1.0000000e+01 -1.4851113e+01\n",
            "  1.0000000e+01  4.4403745e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 69\n",
            "Done: False\n",
            "Action taken: [  0.9489197 -24.984722 ]\n",
            "Observation: [ 1.1630000e+03  3.7166461e+02  1.0000000e+01 -1.6400700e+01\n",
            "  1.0000000e+01  5.0649927e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 70\n",
            "Done: False\n",
            "Action taken: [  0.62050515 -11.184557  ]\n",
            "Observation: [ 1.1630000e+03  3.2743457e+02  1.0000000e+01 -1.7692020e+01\n",
            "  1.0000000e+01  5.3446063e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 71\n",
            "Done: False\n",
            "Action taken: [ 0.48070472 19.453936  ]\n",
            "Observation: [ 1.1630000e+03  2.8786212e+02  1.0000000e+01 -1.5828970e+01\n",
            "  1.0000000e+01  4.8582581e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 72\n",
            "Done: False\n",
            "Action taken: [  0.1605665 -35.5026   ]\n",
            "Observation: [ 1.1630000e+03  2.6494705e+02  1.0000000e+01 -9.1660376e+00\n",
            "  1.0000000e+01  5.7458234e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 73\n",
            "Done: False\n",
            "Action taken: [  0.6106504 -38.531933 ]\n",
            "Observation: [ 1.1630000e+03  2.5591306e+02  1.0000000e+01 -3.6135931e+00\n",
            "  1.0000000e+01  6.7091217e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 74\n",
            "Done: False\n",
            "Action taken: [  0.7745719 -55.769585 ]\n",
            "Observation: [1.1630000e+03 2.5844666e+02 1.0000000e+01 1.0134438e+00 1.0000000e+01\n",
            " 7.1000000e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 75\n",
            "Done: False\n",
            "Action taken: [0.10883308 9.684933  ]\n",
            "Observation: [1.1630000e+03 2.7061993e+02 1.0000000e+01 4.8693080e+00 1.0000000e+01\n",
            " 6.8578766e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 76\n",
            "Done: False\n",
            "Action taken: [4.7339622e-02 4.8297314e+01]\n",
            "Observation: [1.1630000e+03 2.9082626e+02 1.0000000e+01 8.0825281e+00 1.0000000e+01\n",
            " 5.6504437e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 77\n",
            "Done: False\n",
            "Action taken: [  0.50878936 -43.56521   ]\n",
            "Observation: [1.1630000e+03 3.1772678e+02 1.0000000e+01 1.0760211e+01 1.0000000e+01\n",
            " 6.7395740e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 78\n",
            "Done: False\n",
            "Action taken: [1.9792123e-02 2.4024542e+01]\n",
            "Observation: [1.1630000e+03 3.5020581e+02 1.0000000e+01 1.2991614e+01 1.0000000e+01\n",
            " 6.1389606e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 79\n",
            "Done: False\n",
            "Action taken: [  0.39950228 -17.066475  ]\n",
            "Observation: [1.1630000e+03 3.8733362e+02 1.0000000e+01 1.4851116e+01 1.0000000e+01\n",
            " 6.5656226e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 80\n",
            "Done: False\n",
            "Action taken: [ 0.27853218 27.454176  ]\n",
            "Observation: [1.1630000e+03 4.2833536e+02 1.0000000e+01 1.6400702e+01 1.0000000e+01\n",
            " 5.8792682e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 81\n",
            "Done: False\n",
            "Action taken: [ 0.6728189 13.5290575]\n",
            "Observation: [1.1630000e+03 4.7256543e+02 1.0000000e+01 1.7692022e+01 1.0000000e+01\n",
            " 5.5410413e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 82\n",
            "Done: False\n",
            "Action taken: [ 0.34883508 -6.8126836 ]\n",
            "Observation: [1.1630000e+03 5.1213788e+02 1.0000000e+01 1.5828972e+01 1.0000000e+01\n",
            " 5.7113586e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 83\n",
            "Done: False\n",
            "Action taken: [  0.60446984 -47.722805  ]\n",
            "Observation: [1.1630000e+03 5.3505292e+02 1.0000000e+01 9.1660385e+00 1.0000000e+01\n",
            " 6.9044287e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 84\n",
            "Done: False\n",
            "Action taken: [  0.7897903 -26.74742  ]\n",
            "Observation: [1.1630000e+03 5.4408691e+02 1.0000000e+01 3.6135943e+00 1.0000000e+01\n",
            " 7.1000000e+02 1.0000000e+00 0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 85\n",
            "Done: False\n",
            "Action taken: [ 0.3623988 58.757965 ]\n",
            "Observation: [ 1.1630000e+03  5.4155334e+02  1.0000000e+01 -1.0134429e+00\n",
            "  1.0000000e+01  5.6310510e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 86\n",
            "Done: False\n",
            "Action taken: [  0.82111704 -35.50355   ]\n",
            "Observation: [ 1.1630000e+03  5.2938007e+02  1.0000000e+01 -4.8693070e+00\n",
            "  1.0000000e+01  6.5186395e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 87\n",
            "Done: False\n",
            "Action taken: [ 0.9162027 30.281992 ]\n",
            "Observation: [ 1.1630000e+03  5.0917374e+02  1.0000000e+01 -8.0825272e+00\n",
            "  1.0000000e+01  5.7615900e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 88\n",
            "Done: False\n",
            "Action taken: [  0.8161939 -57.817776 ]\n",
            "Observation: [ 1.1630000e+03  4.8227322e+02  1.0000000e+01 -1.0760211e+01\n",
            "  1.0000000e+01  7.1000000e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 89\n",
            "Done: False\n",
            "Action taken: [ 0.4493199 58.020363 ]\n",
            "Observation: [ 1.1630000e+03  4.4979419e+02  1.0000000e+01 -1.2991613e+01\n",
            "  1.0000000e+01  5.6494910e+02  1.0000000e+00  0.0000000e+00]\n",
            "Reward: 0.0\n",
            "iteration: 90\n",
            "Done: False\n",
            "Action taken: [ 3.8451482e-02 -5.7937763e+01]\n",
            "Observation: [1163.        412.66638   -10.        -14.851116   10.        709.7935\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 91\n",
            "Done: False\n",
            "Action taken: [  0.6320891 -48.090137 ]\n",
            "Observation: [1138.        375.5386    -10.        -14.851116   10.        710.\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 92\n",
            "Done: False\n",
            "Action taken: [  0.3412936 -10.6401   ]\n",
            "Observation: [1113.        338.4108    -10.        -14.851116   10.        710.\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 93\n",
            "Done: False\n",
            "Action taken: [ 0.9984306 43.339535 ]\n",
            "Observation: [1088.        301.28302   -10.        -14.851116   10.        601.6512\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 94\n",
            "Done: False\n",
            "Action taken: [ 0.10050417 20.697779  ]\n",
            "Observation: [1063.        264.1552    -10.        -14.851116   10.        549.90674\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 95\n",
            "Done: False\n",
            "Action taken: [  0.3323301 -11.398601 ]\n",
            "Observation: [1038.        227.02744   -10.        -14.851116   10.        578.4032\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 96\n",
            "Done: False\n",
            "Action taken: [  0.10454449 -21.043217  ]\n",
            "Observation: [1013.        189.89964   -10.        -14.851116   10.        631.0112\n",
            "    0.          0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 97\n",
            "Done: False\n",
            "Action taken: [  0.43304965 -21.307602  ]\n",
            "Observation: [988.       152.77185  -10.       -14.851116  10.       684.2803\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 98\n",
            "Done: False\n",
            "Action taken: [ 0.90655464 56.185627  ]\n",
            "Observation: [963.       115.644066 -10.       -14.851116  10.       543.8162\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 99\n",
            "Done: False\n",
            "Action taken: [  0.06221894 -23.799875  ]\n",
            "Observation: [938.        78.51627  -10.       -14.851116  10.       603.31586\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 100\n",
            "Done: False\n",
            "Action taken: [ 0.7808205 48.185608 ]\n",
            "Observation: [913.        41.388485 -10.       -14.851116  10.       482.85187\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 101\n",
            "Done: False\n",
            "Action taken: [ 0.5818658 23.988895 ]\n",
            "Observation: [888.        12.       -10.        14.851116  10.       422.87964\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 102\n",
            "Done: False\n",
            "Action taken: [  0.2095842 -17.976252 ]\n",
            "Observation: [863.        49.12779  -10.        14.851116  10.       467.82025\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 103\n",
            "Done: False\n",
            "Action taken: [  0.26223788 -52.427147  ]\n",
            "Observation: [838.        86.25558  -10.        14.851116  10.       598.8881\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 104\n",
            "Done: False\n",
            "Action taken: [ 0.60126275 16.181906  ]\n",
            "Observation: [813.       123.38337  -10.        14.851116  10.       558.43335\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 105\n",
            "Done: False\n",
            "Action taken: [  0.60995656 -59.100967  ]\n",
            "Observation: [788.       160.51115  -10.        14.851116  10.       706.1858\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 106\n",
            "Done: False\n",
            "Action taken: [ 0.08925573 26.547611  ]\n",
            "Observation: [763.       197.63895  -10.        14.851116  10.       639.8168\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 107\n",
            "Done: False\n",
            "Action taken: [ 0.73834246 49.17415   ]\n",
            "Observation: [738.       234.76674  -10.        14.851116  10.       516.88135\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 108\n",
            "Done: False\n",
            "Action taken: [  0.54891855 -31.911007  ]\n",
            "Observation: [713.       271.89453  -10.        14.851116  10.       596.6589\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 109\n",
            "Done: False\n",
            "Action taken: [  0.79075897 -35.03095   ]\n",
            "Observation: [688.       309.0223   -10.        14.851116  10.       684.23627\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 110\n",
            "Done: False\n",
            "Action taken: [0.10821152 7.988032  ]\n",
            "Observation: [663.       346.15012  -10.        14.851116  10.       664.2662\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 111\n",
            "Done: False\n",
            "Action taken: [ 0.71797186 10.551908  ]\n",
            "Observation: [638.       383.2779   -10.        14.851116  10.       637.8864\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 112\n",
            "Done: False\n",
            "Action taken: [0.7444535 3.7466776]\n",
            "Observation: [613.       420.4057   -10.        14.851116  10.       628.5197\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 113\n",
            "Done: False\n",
            "Action taken: [ 0.5513358 -7.58836  ]\n",
            "Observation: [588.       457.53348  -10.        14.851116  10.       647.4906\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 114\n",
            "Done: False\n",
            "Action taken: [ 0.45522404 -5.6414084 ]\n",
            "Observation: [563.       494.6613   -10.        14.851116  10.       661.5941\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 115\n",
            "Done: False\n",
            "Action taken: [1.9784646e-02 4.7217690e+01]\n",
            "Observation: [538.       531.78906  -10.        14.851116  10.       543.5499\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 116\n",
            "Done: False\n",
            "Action taken: [ 0.32370627 -9.161077  ]\n",
            "Observation: [513.       568.9169   -10.        14.851116  10.       566.45264\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 117\n",
            "Done: False\n",
            "Action taken: [  0.372447 -32.87187 ]\n",
            "Observation: [488.       606.0446   -10.        14.851116  10.       648.63226\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 118\n",
            "Done: False\n",
            "Action taken: [ 0.15670943 18.600187  ]\n",
            "Observation: [463.       643.1724   -10.        14.851116  10.       602.13184\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 119\n",
            "Done: False\n",
            "Action taken: [  0.93682253 -34.208252  ]\n",
            "Observation: [438.       680.30023  -10.        14.851116  10.       687.65247\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 120\n",
            "Done: False\n",
            "Action taken: [ 0.7708546 27.049334 ]\n",
            "Observation: [413.       717.42804  -10.        14.851116  10.       620.0291\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 121\n",
            "Done: False\n",
            "Action taken: [  0.19062108 -41.37027   ]\n",
            "Observation: [388.       754.5558   -10.        14.851116  10.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 122\n",
            "Done: False\n",
            "Action taken: [  0.09157596 -57.832893  ]\n",
            "Observation: [363.       788.       -10.       -14.851116  10.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 123\n",
            "Done: False\n",
            "Action taken: [3.6405257e-03 7.8046746e+00]\n",
            "Observation: [338.       750.8722   -10.       -14.851116  10.       690.48834\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 124\n",
            "Done: False\n",
            "Action taken: [  0.18969291 -16.24957   ]\n",
            "Observation: [313.       713.74445  -10.       -14.851116  10.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 125\n",
            "Done: False\n",
            "Action taken: [0.36695522 9.416447  ]\n",
            "Observation: [288.       676.61664  -10.       -14.851116  10.       686.45886\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 126\n",
            "Done: False\n",
            "Action taken: [  0.09314752 -11.052039  ]\n",
            "Observation: [263.       639.48883  -10.       -14.851116  10.       710.\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 127\n",
            "Done: False\n",
            "Action taken: [ 0.4524878 51.28845  ]\n",
            "Observation: [238.       602.361    -10.       -14.851116  10.       581.7789\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 128\n",
            "Done: False\n",
            "Action taken: [ 0.16825046 29.51913   ]\n",
            "Observation: [213.       565.2333   -10.       -14.851116  10.       507.98105\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 129\n",
            "Done: False\n",
            "Action taken: [  0.06919374 -12.884178  ]\n",
            "Observation: [188.       528.10547  -10.       -14.851116  10.       540.1915\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 130\n",
            "Done: False\n",
            "Action taken: [0.66100824 5.272209  ]\n",
            "Observation: [163.       490.9777   -10.       -14.851116  10.       527.011\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 131\n",
            "Done: False\n",
            "Action taken: [ 0.44009307 14.288148  ]\n",
            "Observation: [138.       453.84988  -10.       -14.851116  10.       491.29062\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 132\n",
            "Done: False\n",
            "Action taken: [ 0.8395792 43.47252  ]\n",
            "Observation: [113.       416.7221   -10.       -14.851116  10.       382.6093\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 133\n",
            "Done: False\n",
            "Action taken: [ 0.5936819 43.177338 ]\n",
            "Observation: [ 88.       379.5943   -10.       -14.851116  10.       274.66595\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 134\n",
            "Done: False\n",
            "Action taken: [ 0.08874768 10.878765  ]\n",
            "Observation: [ 63.       342.46652  -10.       -14.851116  10.       247.46906\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 135\n",
            "Done: False\n",
            "Action taken: [  0.30933097 -22.433487  ]\n",
            "Observation: [ 38.       305.3387   -10.       -14.851116  10.       303.55276\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 136\n",
            "Done: False\n",
            "Action taken: [ 0.60434854 14.410252  ]\n",
            "Observation: [ 13.       268.21094  -10.       -14.851116  10.       267.52713\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 137\n",
            "Done: False\n",
            "Action taken: [ 0.87645817 -4.146428  ]\n",
            "Observation: [-12.       231.08315  -10.       -14.851116  10.       277.89322\n",
            "   0.         0.      ]\n",
            "Reward: -36.43399061869649\n",
            "iteration: 138\n",
            "Done: False\n",
            "Action taken: [  0.41632274 -15.038978  ]\n",
            "Observation: [-12.       231.08315  -10.       -14.851116  10.       277.89322\n",
            "   0.         0.      ]\n",
            "Reward: 0.01\n",
            "iteration: 139\n",
            "Done: True\n",
            "Environment reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ],
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and train\n",
        "ent_coef=0.5\n",
        "eval_freq=2000\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef)\n",
        "# model = PPO.load(\"ppo_custom_pong\")\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_sjyFuJcGT-w",
        "outputId": "740a35f5-26f4-45c7-f1ef-e2ef82a38594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Eval num_timesteps=8000, episode_reward=-2.54 +/- 0.68\n",
            "Episode length: 68.00 +/- 11.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 68       |\n",
            "|    mean_reward     | -2.54    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -2.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.17    |\n",
            "| time/              |          |\n",
            "|    fps             | 2147     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-1.71 +/- 0.92\n",
            "Episode length: 83.80 +/- 23.01\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 83.8       |\n",
            "|    mean_reward          | -1.71      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 16000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17352124 |\n",
            "|    clip_fraction        | 0.422      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.17      |\n",
            "|    explained_variance   | 0.244      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.28      |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | 0.0395     |\n",
            "|    std                  | 1.38       |\n",
            "|    value_loss           | 0.929      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -1.71\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | -1.06    |\n",
            "| time/              |          |\n",
            "|    fps             | 1165     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-0.57 +/- 1.86\n",
            "Episode length: 120.80 +/- 23.59\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 121        |\n",
            "|    mean_reward          | -0.572     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 24000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16630663 |\n",
            "|    clip_fraction        | 0.435      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.82      |\n",
            "|    explained_variance   | 0.568      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -1.74      |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | 0.0424     |\n",
            "|    std                  | 1.9        |\n",
            "|    value_loss           | 0.747      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -0.57\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -0.991   |\n",
            "| time/              |          |\n",
            "|    fps             | 1042     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-1.54 +/- 0.84\n",
            "Episode length: 101.20 +/- 24.84\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 101        |\n",
            "|    mean_reward          | -1.54      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 32000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16927126 |\n",
            "|    clip_fraction        | 0.476      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.5       |\n",
            "|    explained_variance   | 0.64       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.09      |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | 0.0363     |\n",
            "|    std                  | 2.71       |\n",
            "|    value_loss           | 0.547      |\n",
            "----------------------------------------\n",
            "Evaluation at step 8000: mean reward -1.54\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -1.22    |\n",
            "| time/              |          |\n",
            "|    fps             | 981      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1.46 +/- 1.18\n",
            "Episode length: 94.20 +/- 20.31\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 94.2       |\n",
            "|    mean_reward          | -1.46      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 40000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14823309 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.17      |\n",
            "|    explained_variance   | 0.675      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.56      |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | 0.0285     |\n",
            "|    std                  | 3.74       |\n",
            "|    value_loss           | 0.614      |\n",
            "----------------------------------------\n",
            "Evaluation at step 10000: mean reward -1.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -1.67    |\n",
            "| time/              |          |\n",
            "|    fps             | 937      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-0.91 +/- 1.39\n",
            "Episode length: 111.40 +/- 57.57\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 111        |\n",
            "|    mean_reward          | -0.906     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 48000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16196406 |\n",
            "|    clip_fraction        | 0.418      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.81      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.67      |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 5.12       |\n",
            "|    value_loss           | 0.672      |\n",
            "----------------------------------------\n",
            "Evaluation at step 12000: mean reward -0.91\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -1.16    |\n",
            "| time/              |          |\n",
            "|    fps             | 926      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=0.32 +/- 2.31\n",
            "Episode length: 158.60 +/- 35.51\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 159        |\n",
            "|    mean_reward          | 0.324      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 56000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13600472 |\n",
            "|    clip_fraction        | 0.411      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -6.43      |\n",
            "|    explained_variance   | 0.627      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -2.79      |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | 0.0288     |\n",
            "|    std                  | 7.02       |\n",
            "|    value_loss           | 0.766      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 14000: mean reward 0.32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | -1.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 839      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-2.09 +/- 1.43\n",
            "Episode length: 92.00 +/- 25.58\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 92         |\n",
            "|    mean_reward          | -2.09      |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 64000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14800479 |\n",
            "|    clip_fraction        | 0.43       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.08      |\n",
            "|    explained_variance   | 0.714      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.49      |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | 0.0366     |\n",
            "|    std                  | 9.76       |\n",
            "|    value_loss           | 0.663      |\n",
            "----------------------------------------\n",
            "Evaluation at step 16000: mean reward -2.09\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | -0.0483  |\n",
            "| time/              |          |\n",
            "|    fps             | 834      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 78       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=1.36 +/- 5.39\n",
            "Episode length: 138.40 +/- 110.77\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 138        |\n",
            "|    mean_reward          | 1.36       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 72000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14038308 |\n",
            "|    clip_fraction        | 0.404      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -7.72      |\n",
            "|    explained_variance   | 0.689      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -3.63      |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | 0.0373     |\n",
            "|    std                  | 13.2       |\n",
            "|    value_loss           | 0.777      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 18000: mean reward 1.36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -0.39    |\n",
            "| time/              |          |\n",
            "|    fps             | 826      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-0.64 +/- 0.66\n",
            "Episode length: 110.20 +/- 35.67\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 110        |\n",
            "|    mean_reward          | -0.645     |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 80000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17711544 |\n",
            "|    clip_fraction        | 0.463      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.35      |\n",
            "|    explained_variance   | 0.595      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.05      |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | 0.0447     |\n",
            "|    std                  | 18.2       |\n",
            "|    value_loss           | 0.47       |\n",
            "----------------------------------------\n",
            "Evaluation at step 20000: mean reward -0.64\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 129      |\n",
            "|    ep_rew_mean     | 1.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=1.48 +/- 2.63\n",
            "Episode length: 168.40 +/- 81.02\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 168        |\n",
            "|    mean_reward          | 1.48       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 88000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16022202 |\n",
            "|    clip_fraction        | 0.45       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -8.99      |\n",
            "|    explained_variance   | 0.503      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.49      |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | 0.0404     |\n",
            "|    std                  | 25.4       |\n",
            "|    value_loss           | 0.546      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 22000: mean reward 1.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 137      |\n",
            "|    ep_rew_mean     | 4        |\n",
            "| time/              |          |\n",
            "|    fps             | 828      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=25.55 +/- 48.76\n",
            "Episode length: 236.80 +/- 128.23\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 237        |\n",
            "|    mean_reward          | 25.6       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 96000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17208788 |\n",
            "|    clip_fraction        | 0.462      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -9.68      |\n",
            "|    explained_variance   | 0.444      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.69      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | 0.0502     |\n",
            "|    std                  | 36.1       |\n",
            "|    value_loss           | 0.392      |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 24000: mean reward 25.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 134      |\n",
            "|    ep_rew_mean     | 6.9      |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=1.97 +/- 2.30\n",
            "Episode length: 161.60 +/- 67.48\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 162        |\n",
            "|    mean_reward          | 1.97       |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 104000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17116281 |\n",
            "|    clip_fraction        | 0.451      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -10.4      |\n",
            "|    explained_variance   | 0.562      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -4.87      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 51.4       |\n",
            "|    value_loss           | 0.395      |\n",
            "----------------------------------------\n",
            "Evaluation at step 26000: mean reward 1.97\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 123      |\n",
            "|    ep_rew_mean     | 10.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 819      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0')) for _ in range(4)])\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "# Sync normalization statistics from the training environment to the evaluation environment\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "eval_env.ret_rms = train_env.ret_rms\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mL_LYMHeejpF",
        "outputId": "a0c1b264-c71d-45dc-9f35-6c430a93be40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=8000, episode_reward=-84.46 +/- 115.27\n",
            "Episode length: 128.80 +/- 39.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 129      |\n",
            "|    mean_reward     | -84.5    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 2000: mean reward -84.46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -275     |\n",
            "| time/              |          |\n",
            "|    fps             | 2629     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-55.49 +/- 114.04\n",
            "Episode length: 131.00 +/- 70.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -55.5       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004481554 |\n",
            "|    clip_fraction        | 0.0276      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.1       |\n",
            "|    explained_variance   | 0.559       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.941      |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.00016     |\n",
            "|    std                  | 2.25e+06    |\n",
            "|    value_loss           | 1.42        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 4000: mean reward -55.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 121      |\n",
            "|    ep_rew_mean     | -252     |\n",
            "| time/              |          |\n",
            "|    fps             | 1159     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-54.59 +/- 49.95\n",
            "Episode length: 149.80 +/- 69.42\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 150          |\n",
            "|    mean_reward          | -54.6        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 24000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036968566 |\n",
            "|    clip_fraction        | 0.0201       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.1        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.763       |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00063      |\n",
            "|    std                  | 2.35e+06     |\n",
            "|    value_loss           | 1.69         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Evaluation at step 6000: mean reward -54.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | -262     |\n",
            "| time/              |          |\n",
            "|    fps             | 1023     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-59.87 +/- 43.11\n",
            "Episode length: 162.80 +/- 36.45\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 163          |\n",
            "|    mean_reward          | -59.9        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 32000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034573479 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.2        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.578       |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | 0.000596     |\n",
            "|    std                  | 2.45e+06     |\n",
            "|    value_loss           | 1.48         |\n",
            "------------------------------------------\n",
            "Evaluation at step 8000: mean reward -59.87\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 111      |\n",
            "|    ep_rew_mean     | -241     |\n",
            "| time/              |          |\n",
            "|    fps             | 941      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-164.55 +/- 161.91\n",
            "Episode length: 116.60 +/- 45.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 117         |\n",
            "|    mean_reward          | -165        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004476109 |\n",
            "|    clip_fraction        | 0.0281      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.3       |\n",
            "|    explained_variance   | 0.511       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.654      |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.000234    |\n",
            "|    std                  | 2.58e+06    |\n",
            "|    value_loss           | 1.46        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 10000: mean reward -164.55\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | -255     |\n",
            "| time/              |          |\n",
            "|    fps             | 889      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-150.29 +/- 81.91\n",
            "Episode length: 91.40 +/- 36.15\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 91.4         |\n",
            "|    mean_reward          | -150         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034466675 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.4        |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.01        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | 0.000546     |\n",
            "|    std                  | 2.68e+06     |\n",
            "|    value_loss           | 1.75         |\n",
            "------------------------------------------\n",
            "Evaluation at step 12000: mean reward -150.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -267     |\n",
            "| time/              |          |\n",
            "|    fps             | 870      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-122.84 +/- 159.98\n",
            "Episode length: 131.20 +/- 30.50\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 131         |\n",
            "|    mean_reward          | -123        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 56000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003678766 |\n",
            "|    clip_fraction        | 0.0178      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.5       |\n",
            "|    explained_variance   | 0.588       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -1.18       |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.000835    |\n",
            "|    std                  | 2.8e+06     |\n",
            "|    value_loss           | 2.03        |\n",
            "-----------------------------------------\n",
            "Evaluation at step 14000: mean reward -122.84\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 112      |\n",
            "|    ep_rew_mean     | -266     |\n",
            "| time/              |          |\n",
            "|    fps             | 867      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-128.49 +/- 138.29\n",
            "Episode length: 138.00 +/- 51.68\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -128         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 64000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025062133 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.6        |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.02        |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | 0.00116      |\n",
            "|    std                  | 2.91e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 16000: mean reward -128.49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -273     |\n",
            "| time/              |          |\n",
            "|    fps             | 850      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 77       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-75.76 +/- 35.28\n",
            "Episode length: 150.60 +/- 84.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 151          |\n",
            "|    mean_reward          | -75.8        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 72000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035605398 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.7        |\n",
            "|    explained_variance   | 0.479        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.703       |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | 0.000628     |\n",
            "|    std                  | 3.02e+06     |\n",
            "|    value_loss           | 1.78         |\n",
            "------------------------------------------\n",
            "Evaluation at step 18000: mean reward -75.76\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -245     |\n",
            "| time/              |          |\n",
            "|    fps             | 836      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-183.29 +/- 78.86\n",
            "Episode length: 98.40 +/- 33.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 98.4        |\n",
            "|    mean_reward          | -183        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 80000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003738511 |\n",
            "|    clip_fraction        | 0.0235      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -32.7       |\n",
            "|    explained_variance   | 0.544       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.86       |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.000683    |\n",
            "|    std                  | 3.16e+06    |\n",
            "|    value_loss           | 1.7         |\n",
            "-----------------------------------------\n",
            "Evaluation at step 20000: mean reward -183.29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 109      |\n",
            "|    ep_rew_mean     | -235     |\n",
            "| time/              |          |\n",
            "|    fps             | 840      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-188.48 +/- 100.36\n",
            "Episode length: 95.40 +/- 32.52\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 95.4         |\n",
            "|    mean_reward          | -188         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 88000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040115286 |\n",
            "|    clip_fraction        | 0.0256       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.8        |\n",
            "|    explained_variance   | 0.502        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.799       |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.000368     |\n",
            "|    std                  | 3.31e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 22000: mean reward -188.48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -277     |\n",
            "| time/              |          |\n",
            "|    fps             | 833      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-109.59 +/- 63.85\n",
            "Episode length: 115.40 +/- 55.33\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 115          |\n",
            "|    mean_reward          | -110         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 96000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031413306 |\n",
            "|    clip_fraction        | 0.0192       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -32.9        |\n",
            "|    explained_variance   | 0.577        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -1.2         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | 0.000992     |\n",
            "|    std                  | 3.45e+06     |\n",
            "|    value_loss           | 1.44         |\n",
            "------------------------------------------\n",
            "Evaluation at step 24000: mean reward -109.59\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 115      |\n",
            "|    ep_rew_mean     | -209     |\n",
            "| time/              |          |\n",
            "|    fps             | 825      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=-130.40 +/- 80.68\n",
            "Episode length: 138.00 +/- 56.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 138          |\n",
            "|    mean_reward          | -130         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 104000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035240077 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -33          |\n",
            "|    explained_variance   | 0.537        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.348       |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | 0.000891     |\n",
            "|    std                  | 3.61e+06     |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Evaluation at step 26000: mean reward -130.40\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -234     |\n",
            "| time/              |          |\n",
            "|    fps             | 820      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Training completed and logs are saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkoAAAHHCAYAAABA5XcCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACGH0lEQVR4nO3dd3hT5dsH8G/SNmm696Klg1k2gkDZGxFR9Ac4EBkORHyZgiDKEAFligMRUUBFARFFUIHKkiW77E1LgdKWtnTv5Hn/KAkNHTQl6UnS7+e6ckFOTk7upmly53nucz8yIYQAEREREZUglzoAIiIiInPFRImIiIioDEyUiIiIiMrARImIiIioDEyUiIiIiMrARImIiIioDEyUiIiIiMrARImIiIioDEyUiIiIiMrARIms1owZMyCTyaQOw6ytWrUKMpkMMTExkjz+0KFDERISIsljE5CZmYnXXnsNfn5+kMlkGDt2rNQhWTWZTIYZM2YYdB/+jqTHRMkA2g8VmUyGffv2lbhdCIGgoCDIZDI89dRTEkRYcSEhIbqfRSaTwdHREa1atcL3338vdWjVUufOnfV+H8Uv9evXlzq8RxIXF4cZM2YgKipK6lBM6sCBA5gxYwZSU1OlDqXC5syZg1WrVmHkyJH44YcfMHjw4Ec6nkajwbx58xAaGgp7e3s0adIEP//8c4Xvn5qaijfeeAPe3t5wdHREly5dcPz48RL7Pfj+pb28+eabjxS/OTL276iqJCcnY/78+ejYsSO8vb3h5uaGNm3aYN26daXun5eXh3fffRcBAQFQqVRo3bo1IiMjS933wIEDaN++PRwcHODn54fRo0cjMzPzkY5ZHluD70Gwt7fHTz/9hPbt2+tt37NnD27evAmlUilRZIZp1qwZJkyYAAC4ffs2VqxYgSFDhiAvLw+vv/66xNFVP4GBgZg7d26J7a6urhJEYzxxcXGYOXMmQkJC0KxZM73bvvnmG2g0GmkCM7IDBw5g5syZGDp0KNzc3KQOp0J27tyJNm3aYPr06UY53tSpU/Hxxx/j9ddfx+OPP45NmzbhpZdegkwmwwsvvFDufTUaDfr06YOTJ09i4sSJ8PLywtKlS9G5c2ccO3YMderU0du/+PuXVt26dY3yc5gTY/+OqsrBgwcxdepUPPnkk3j//fdha2uLX3/9FS+88ALOnTuHmTNn6u0/dOhQbNiwAWPHjkWdOnWwatUqPPnkk9i1a5feZ21UVBS6deuG8PBwLFq0CDdv3sSCBQtw+fJl/P3335U65kMJqrCVK1cKAOK5554TXl5eoqCgQO/2119/XbRo0UIEBweLPn36SBRlxZQWY2JionBychLh4eESRWWYgoICkZeXV+bt06dPF+byEler1SInJ6fM2zt16iQaNmxYhREV0b6mo6OjTfYYR44cEQDEypUrTfYY5mD+/Pkmfy6NLTQ01GjvVTdv3hR2dnZi1KhRum0ajUZ06NBBBAYGisLCwnLvv27dOgFA/PLLL7ptiYmJws3NTbz44ot6+1rCe2xpAIjp06cbdJ+K/o5ycnKEWq2uZGTGd+3aNRETE6O3TaPRiK5duwqlUikyMzN12w8dOiQAiPnz5+u25eTkiFq1aomIiAi9Y/Tu3Vv4+/uLtLQ03bZvvvlGABDbtm2r1DEfhlNvlfDiiy8iOTlZbwgvPz8fGzZswEsvvVTqfTQaDT799FM0bNgQ9vb28PX1xYgRI3D37l29/TZt2oQ+ffogICAASqUStWrVwqxZs6BWq/X269y5Mxo1aoRz586hS5cucHBwQI0aNTBv3rxK/1ze3t6oX78+rl69anDs48ePh6enJ4QQum3/93//B5lMhs8++0y3LSEhATKZDF999RWAoudt2rRpaNGiBVxdXeHo6IgOHTpg165dejHExMRAJpNhwYIF+PTTT1GrVi0olUqcO3cOALBv3z48/vjjsLe3R61atfD1119X+OfWPpfHjh1D27ZtoVKpEBoaimXLlpXYNy8vD9OnT0ft2rWhVCoRFBSESZMmIS8vT28/mUyGt99+G2vWrEHDhg2hVCqxdevWCsdUmg0bNkAmk2HPnj0lbvv6668hk8lw5swZAMCpU6cwdOhQhIWFwd7eHn5+fhg+fDiSk5Mf+jhl1VGEhIRg6NChuuspKSl455130LhxYzg5OcHFxQW9e/fGyZMndfvs3r0bjz/+OABg2LBhuimSVatWASi9RikrKwsTJkxAUFAQlEol6tWrhwULFui9trRxvv322/j999/RqFEjKJVKNGzYsMLPs6G/y/IeZ8aMGZg4cSIAIDQ0VPdzGlL7deHCBQwcOBDe3t5QqVSoV68epk6dqrfPiRMn0Lt3b7i4uMDJyQndunXDf//9V+JYqampGDt2rO45rF27Nj755BPd6N3u3bshk8kQHR2NP//8s1LxPmjTpk0oKCjAW2+9pdsmk8kwcuRI3Lx5EwcPHiz3/hs2bICvry+ee+453TZvb28MHDgQmzZtKvF7AYreP7KysgyO9WHPT0FBATw8PDBs2LAS901PT4e9vT3eeecdXQwVeQ8zVHm/I+1ta9euxfvvv48aNWrAwcEB6enpAIBDhw7hiSeegKurKxwcHNCpUyfs37+/xGOU9r5prNrO0NBQBAcH622TyWTo168f8vLycO3aNd32DRs2wMbGBm+88YZum729PV599VUcPHgQN27cAFD03EdGRuLll1+Gi4uLbt9XXnkFTk5OWL9+vcHHrBCD0qpqTvvt+8iRI6Jt27Zi8ODButt+//13IZfLxa1bt0r9tvPaa68JW1tb8frrr4tly5aJd999Vzg6OorHH39c5Ofn6/br16+fGDhwoJg/f7746quvxIABAwQA8c477+gdr1OnTiIgIEAEBQWJMWPGiKVLl4quXbsKAOKvv/566M9SWowFBQXCz89P+Pr6Ghz7xo0bBQBx+vRp3f2aNm0q5HK56N+/v27bL7/8IgCIM2fOCCGEuHPnjvD39xfjx48XX331lZg3b56oV6+esLOzEydOnNDdLzo6WgAQDRo0EGFhYeLjjz8WixcvFtevXxenTp0SKpVK1KxZU8ydO1fMmjVL+Pr6iiZNmlRoREn7XPr4+Ii3335bfPbZZ6J9+/YCgPj22291+6nVatGzZ0/h4OAgxo4dK77++mvx9ttvC1tbW/HMM8/oHROACA8PF97e3mLmzJniyy+/1Pt5Souhfv364s6dOyUu2m9e2dnZwsnJSbz11lsl7t+lSxe9EakFCxaIDh06iA8//FAsX75cjBkzRqhUKtGqVSuh0Wh0+5U2ooQyvvUGBweLIUOG6K4fOXJE1KpVS0yePFl8/fXX4sMPPxQ1atQQrq6u4tatW0IIIeLj48WHH34oAIg33nhD/PDDD+KHH34QV69eFUIIMWTIEBEcHKw7pvYbp0wmE6+99pr44osvRN++fQUAMXbs2BLPcdOmTYW/v7+YNWuW+PTTT0VYWJhwcHAQSUlJZT7XQhj+u3zY45w8eVK8+OKLAoBYvHix7ucs/q25PCdPnhQuLi7C09NTTJkyRXz99ddi0qRJonHjxrp9zpw5IxwdHXVxfPzxxyI0NFQolUrx33//6fbLysoSTZo0EZ6enuK9994Ty5YtE6+88oqQyWRizJgxut/LDz/8ILy8vESzZs1KxFva67C0S25uru5xX3vtNeHo6Kj3+hJCiCtXrggA4rPPPiv3Oahdu7bo3bt3ie0rVqwQAMSpU6d024KDg4VKpRI2NjYCgAgODhaffvpphZ7rijw/QggxfPhw4ebmVmLUevXq1brPAe1zVZH3MCEMG1Eq73e0a9cu3fths2bNxKJFi8TcuXNFVlaW2LFjh1AoFCIiIkIsXLhQLF68WDRp0kQoFApx6NAh3fENed9MTU2t0OshIyPjoT/Xe++9JwCIuLg43bbu3buXOpPxzz//CADijz/+EEIIsW/fPgFArFu3rsS+7du3F4899pjBx6wIJkoGKJ4offHFF8LZ2VlkZ2cLIYQYMGCA6NKlixCiZBKyd+9eAUCsWbNG73hbt24tsV17vOJGjBghHBwc9N6UOnXqJACI77//XrctLy9P+Pn5if/9738P/VmCg4NFz549dS/w06dPi8GDBwsAekPnFY09MTFRABBLly4VQhT9YcnlcjFgwAC9xGv06NHCw8ND92ZaWFhY4o3o7t27wtfXVwwfPly3TZsoubi4iMTERL39+/XrJ+zt7cX169d1286dO6d7E30Y7XO5cOFC3ba8vDzRrFkz4ePjo0sGf/jhByGXy8XevXv17r9s2TIBQOzfv1+3DYCQy+Xi7NmzD3384jGUdhkxYoRuvxdffFH4+PjoTWPcvn1byOVy8eGHH+q2lfY6+vnnnwUA8e+//+q2PUqilJubW2KoPzo6WiiVSr1Yypt6ezBR+v333wUA8dFHH+nt179/fyGTycSVK1f04lQoFHrbTp48KQCIzz//vMRjFWfo77Iij/MoU28dO3YUzs7Oeq9hIYRe0tGvXz+hUCh0SaYQQsTFxQlnZ2fRsWNH3bZZs2YJR0dHcenSJb1jTZ48WdjY2IjY2FjdtrKmsMp6LT54Kf477dOnjwgLCytxrKysLAFATJ48udznwNHRUe9vXuvPP/8UAMTWrVt12/r27Ss++eQT8fvvv4tvv/1WdOjQQQAQkyZNKvcxhKj487Nt2zYBQGzevFlvvyeffFLv56zoe5gQlZt6K+13pE2UwsLC9P7WNRqNqFOnjujVq5feayc7O1uEhoaKHj166LYZ8r5Z3vtT8Uvx94fSJCcnCx8fH9GhQwe97Q0bNhRdu3Ytsf/Zs2cFALFs2TIhxP0v2sXfw7QGDBgg/Pz8DD5mRbCYu5IGDhyIsWPHYsuWLXjiiSewZcsWvSmm4n755Re4urqiR48eSEpK0m1v0aIFnJycsGvXLt2UnUql0t2ekZGBvLw8dOjQAV9//TUuXLiApk2b6m53cnLCyy+/rLuuUCjQqlUrvSHN8mzfvh3e3t5624YNG4b58+cbHLt22u7ff//FyJEjsX//ftjY2GDixIn45ZdfcPnyZdSpUwd79+5F+/btdUO7NjY2sLGxAVA0xZeamgqNRoOWLVuWerbL//73P72Y1Wo1tm3bhn79+qFmzZq67eHh4ejVqxf++uuvCj0Xtra2GDFihO66QqHAiBEjMHLkSBw7dgxt2rTBL7/8gvDwcNSvX1/vuejatSsAYNeuXWjbtq1ue6dOndCgQYMKPT5QNLX1zTfflNgeGBio+//zzz+Pn3/+Gbt370a3bt0AFA0xazQaPP/887r9ir+OcnNzkZmZiTZt2gAAjh8/jg4dOlQ4rrIUP2lBrVYjNTUVTk5OqFevXqm/u4r466+/YGNjg9GjR+ttnzBhAjZs2IC///4bb7/9tm579+7dUatWLd31Jk2awMXF5aF/A4b+Liv7OBVx584d/PvvvxgzZozeaxiA7u9ErVZj+/bt6NevH8LCwnS3+/v746WXXsI333yD9PR0uLi44JdffkGHDh3g7u6u97N1794dH3/8Mf79918MGjSo3JgqemZQw4YNdf/Pyckp9UQWe3t73e3lMeT+f/zxh94+w4YNQ+/evbFo0SL83//9n97fzIMq+vx07doVXl5eWLdune4s5rt37yIyMlI37QYY/h5mTEOGDNH7W4+KisLly5fx/vvvl5hm79atG3744QdoNBoIIQx631y4cGGJMpHSBAQElHmbRqPBoEGDkJqais8//1zvtor+7rX/lrVv8dfIo74ei2OiVEne3t7o3r07fvrpJ2RnZ0OtVqN///6l7nv58mWkpaXBx8en1NsTExN1/z979izef/997Ny5UzffrJWWlqZ3PTAwsMRcsru7O06dOlWhn6F169b46KOPoFarcebMGXz00Ue4e/cuFApFpWLv0KGD7g9s7969aNmyJVq2bAkPDw/s3bsXvr6+OHnyZIk6rtWrV2PhwoW4cOECCgoKdNtDQ0NLPN6D2+7cuYOcnJwSZ8QAQL169SqcKAUEBMDR0VFvm/YMmpiYGLRp0waXL1/G+fPnSySXWsWfi7LiL4+joyO6d+9e7j7auoN169bpEqV169ahWbNmemf8pKSkYObMmVi7dm2JuB58HVWWRqPBkiVLsHTpUkRHR+vV0Xl6elbqmNevX0dAQACcnZ31toeHh+tuL+7BxAIo+ht42Ju6ob/Lyj5ORWiTrUaNGpW5z507d5CdnY169eqVuC08PBwajQY3btxAw4YNcfnyZZw6darCP1tpHvY6LI1KpSq1jig3N1d3u6nuL5PJMG7cOGzbtg27d+/W+wL5oIo+P7a2tvjf//6Hn376CXl5eVAqldi4cSMKCgr0vpQAhr2HGdODx798+TKAogSqLGlpacjLyzPofbNFixaPHOv//d//YevWrfj+++/1vvADFf/da/8ta9/ir5FHfT0Wx0TpEbz00kt4/fXXER8fj969e5d5SrBGo4GPjw/WrFlT6u3aP9jU1FR06tQJLi4u+PDDD1GrVi3Y29vj+PHjePfdd0ucRq39FvMg8UDRa1m8vLx0b4i9evVC/fr18dRTT2HJkiUYP368QbEDQPv27fHNN9/g2rVr2Lt3Lzp06ACZTIb27dtj7969CAgIgEaj0RvN+PHHHzF06FD069cPEydOhI+PD2xsbDB37twSReWAYS9uY9NoNGjcuDEWLVpU6u1BQUF6100Rq1KpRL9+/fDbb79h6dKlSEhIwP79+zFnzhy9/QYOHIgDBw5g4sSJaNasGZycnKDRaPDEE09U+nT8B08omDNnDj744AMMHz4cs2bNgoeHB+RyOcaOHVtlp/xX9m/A0N/lo/6tVSWNRoMePXpg0qRJpd5ekVPo4+PjK/RYrq6uute5v78/du3aBSGE3he427dvAyh/tEF7f+2+xVX0/trfWUpKSrn7GfL8vPDCC/j666/x999/o1+/fli/fj3q16+v90Fv6HuYMT34HqP9u5s/f36JVhxaTk5OpSYQ5UlJSUF+fn6F4imtncnMmTOxdOlSfPzxx6X2gfL398etW7dKbH/wd+/v76+3/cF9i79GKnrMimCi9AieffZZjBgxAv/991+ZTbQAoFatWvjnn3/Qrl27cj88d+/ejeTkZGzcuBEdO3bUbY+OjjZq3GXp06cPOnXqhDlz5mDEiBFwdHSscOwAdAlQZGQkjhw5gsmTJwMAOnbsiK+++ko3alP828mGDRsQFhaGjRs36r25VrRniPYMIe03qeIuXrxYoWMARb1+srKy9EaVLl26BAC6s7Jq1aqFkydPolu3bpJ2/H7++eexevVq7NixA+fPn4cQQu8b7t27d7Fjxw7MnDkT06ZN020v7Tkqjbu7e4mmifn5+SXenDZs2IAuXbrg22+/1duempoKLy8v3XVDnqvg4GD8888/yMjI0BtVunDhgu52YzDF77Kyx9FOpWnPWCyNt7c3HBwcSn1NX7hwAXK5XJco1KpVC5mZmZUaFdLSfiA9zMqVK3VnQjZr1gwrVqzA+fPn9aacDx06pLu9PM2aNcPevXuh0Wggl98/IfvQoUNwcHB4aIKnHZkra6RIy5Dnp2PHjvD398e6devQvn177Ny5s8SZiI/6HmZM2ulhFxeXcn8+Q983n3vuuVLPtn3QkCFDdGe0an355ZeYMWMGxo4di3fffbfU+zVr1gy7du3STR9rPfjaadSoEWxtbXH06FEMHDhQt19+fj6ioqL0tlX0mBXB9gCPwMnJCV999RVmzJiBvn37lrnfwIEDoVarMWvWrBK3FRYW6j6UtN9ai39Lzc/Px9KlS40beDneffddJCcn62plKho7UDQMXKNGDSxevBgFBQVo164dgKIE6urVq9iwYQPatGkDW9v7+XlpP/OhQ4ceeipx8fv36tULv//+O2JjY3Xbz58/j23btlX45y4sLNRrKZCfn4+vv/4a3t7eusRu4MCBuHXrVql1RDk5OZU6TbkyunfvDg8PD6xbtw7r1q1Dq1at9IbgS3tOAeDTTz+t0PFr1aqFf//9V2/b8uXLS4wo2djYlHiMX375pcS3OG3yWZGO1U8++STUajW++OILve2LFy+GTCZD7969K/QzPIwpfpeG/JzFeXt7o2PHjvjuu+/0XsPA/d+hjY0NevbsiU2bNumdwp+QkKBrfqv9MBg4cCAOHjxY6us/NTUVhYWFD40pMjKyQpdevXrp7vPMM8/Azs5O7/1KCIFly5ahRo0aejVft2/fLjFN1b9/fyQkJGDjxo26bUlJSfjll1/Qt29fXb1JSkpKiddiQUEBPv74YygUCnTp0qXcn82Q50cul6N///7YvHkzfvjhBxQWFpaYdnvU9zBjatGiBWrVqoUFCxaU2qn6zp07AAx/31y4cGGFXg8PjtKtW7cOo0ePxqBBg8ocvQWKfvdqtRrLly/XbcvLy8PKlSvRunVr3ZcAV1dXdO/eHT/++CMyMjJ0+/7www/IzMzEgAEDDD5mRXBE6RGVNxes1alTJ4wYMQJz585FVFQUevbsCTs7O1y+fBm//PILlixZgv79+6Nt27Zwd3fHkCFDMHr0aMhkMvzwww9VOrzfu3dvNGrUCIsWLcKoUaMqHLtWhw4dsHbtWjRu3Bju7u4AgMceewyOjo64dOlSifqkp556Chs3bsSzzz6LPn36IDo6GsuWLUODBg1K/UMvzcyZM7F161Z06NABb731FgoLC/H555+jYcOGFa7XCggIwCeffIKYmBjUrVsX69atQ1RUFJYvXw47OzsAwODBg7F+/Xq8+eab2LVrF9q1awe1Wo0LFy5g/fr12LZtG1q2bFmhxytNWloafvzxx1JvK15zYWdnh+eeew5r165FVlYWFixYoLevi4sLOnbsiHnz5qGgoAA1atTA9u3bKzwy+dprr+HNN9/E//73P/To0QMnT57Etm3b9EaJgKLf3Ycffohhw4ahbdu2OH36NNasWaNXbAwUJV5ubm5YtmwZnJ2d4ejoiNatW5dav9G3b1906dIFU6dORUxMDJo2bYrt27dj06ZNGDt2rF5B9aMwxe9Sm1BPnToVL7zwAuzs7NC3b98StW+l+eyzz9C+fXs89thjeOONNxAaGoqYmBj8+eefuqVfPvroI0RGRqJ9+/Z46623YGtri6+//hp5eXl6/dMmTpyIP/74A0899RSGDh2KFi1aICsrC6dPn8aGDRsQExNT4nf5oMqMRgUGBmLs2LGYP38+CgoK8Pjjj+P333/H3r17sWbNGr3pyylTpmD16tWIjo7Wjdj2798fbdq0wbBhw3Du3DldZ261Wq3XxfmPP/7ARx99hP79+yM0NBQpKSn46aefcObMGcyZMwd+fn7lxmno8/P888/j888/x/Tp09G4cWNdvZyWMd7DjEUul2PFihXo3bs3GjZsiGHDhqFGjRq4desWdu3aBRcXF2zevBmAYe+blalROnz4MF555RV4enqiW7duJco32rZtq3uvaN26NQYMGIApU6YgMTERtWvXxurVqxETE1NixHr27Nlo27YtOnXqhDfeeAM3b97EwoUL0bNnTzzxxBO6/Qw55kNV+Pw40msPUJ6yTrldvny5aNGihVCpVMLZ2Vk0btxYTJo0Sa+fxP79+0WbNm2ESqUSAQEBYtKkSbrTVHft2qXbr6xOzg+ebm1ojEIIsWrVqhKn/lYkdiGE+PLLLwUAMXLkSL3t3bt3FwDEjh079LZrNBoxZ84cERwcLJRKpWjevLnYsmVLiZ9D2x6geJfV4vbs2SNatGghFAqFCAsLE8uWLatwZ27tc3n06FEREREh7O3tRXBwsPjiiy9K7Jufny8++eQT0bBhQ6FUKoW7u7to0aKFmDlzpl6nWDzQZqEiMaCc024fFBkZKQAImUwmbty4UeL2mzdvimeffVa4ubkJV1dXMWDAABEXF1fi9OTS2gOo1Wrx7rvvCi8vL+Hg4CB69eolrly5Ump7gAkTJgh/f3+hUqlEu3btxMGDB0WnTp1Ep06d9OLZtGmTaNCggbC1tdV7bZX2es3IyBDjxo0TAQEBws7OTtSpU0fMnz+/RH+esp7jB+Msy6P+Lkt7nFmzZokaNWoIuVxucKuAM2fO6H5n9vb2ol69euKDDz7Q2+f48eOiV69ewsnJSTg4OIguXbqIAwcOlDhWRkaGmDJliqhdu7ZQKBTCy8tLtG3bVixYsECvb5uxO1yr1Wrd37NCoRANGzYUP/74Y4n9hgwZUurzk5KSIl599VXh6ekpHBwcRKdOnUq83x49elT07dtX1KhRQygUCuHk5CTat28v1q9fX+E4K/r8CFH0HhUUFFRq2wrt7RV5DxPC+O0BincxL+7EiRPiueeeE56enkKpVIrg4GAxcODAEu+/j/K++TDa95ayLg+2C8nJyRHvvPOO8PPzE0qlUjz++ON6LSGK27t3r2jbtq2wt7cX3t7eYtSoUSI9Pb3EfoYcszwyIcywGpGoCnXu3BlJSUnl1ogQEVUHM2bMwMyZM83yRAWpsEaJiIiIqAysUSIiMpG0tLSHNrZ7WE0NWR+1Wq0rrC6Lk5MTnJycqigiKg8TJSIiExkzZgxWr15d7j6c4qh+bty48dBmlNOnTy91cWqqeqxRIiIykXPnziEuLq7cfR6l3xFZptzcXOzbt6/cfcLCwkqcQUrSYKJEREREVAYWcxMRERGVgTVKBtJoNIiLi4Ozs7Oky1gQERFRxQkhkJGRgYCAAL1lch6GiZKB4uLiDGp9TkRERObjxo0bCAwMrPD+TJQMpF2o88aNG3oL7REREZH5Sk9PR1BQkN6C2xXBRMlA2uk2FxcXJkpEREQWxtCyGRZzExEREZWBiRIRERFRGZgoEREREZWBiRIRERFRGZgoEREREZWBiRIRERFRGZgoEREREZWBiRIRERFRGZgoEREREZWBiRIRERFRGZgoEREREZWBiRIRERFRGZgokcXJK1TjTkYesvIKpQ6FiIisnK3UAVD1otYIZOYWIj23AOm5BcjILUR6TtG/GbkFSNf+m1OIjDz929Pv3S+/UAMAsLORoXM9HzzTLADd6vtCpbCR+KcjIiJrw0SJKkwIgex8dbGkRpvYFE9mCpCR+2CCo02ECpFpxFGgArVA5LkERJ5LgKPCBr0a+eGZZjXQrpYnbG04WEpERI+OiRKV60hMCqZsPI2kzDxk5BZCrRFGOa7SVg4XlR2c7W3hYn//XxeVLZzt7eCstNXd7mxvB5d7/zrbF213UtriSmImNkXdwqaoONxKzcHG47ew8fgteDkp0KexP55pXgPNg9wgk8mMEjMREVU/MiGEcT75qon09HS4uroiLS0NLi4uUodjUhqNQJ/P9+H87XS97TZymX7iYl8soVHdT2xciiU2DyY8ClvjjfgIIXA89i42RcVhy6nbSMnK190W5KHCM01roF/zANT2cTbaYxIRkWWp7Oc3EyUDVadE6c9TtzHqp+NwVtri5zfawNtZCWd7W6jsbMx2lKZArcG+K0n4IyoO287GIztfrbutgb8LnmkWgKebBcDfVSVhlEREVNWYKFWR6pIoqTUCPRfvwdU7WRjXvS7GdK8jdUgGy84vxD/nE/FH1C3svngHhfemDWUyoFWIB55pVgNPNvaDm4NC4kiJiMjUmChVkeqSKP167CYm/HISbg522DupC5zt7aQO6ZHczcrHX2duY1NUHA5Hp+i229nI0Klu0Zlz3cN55hwRkbViolRFqkOiVKDWoOvC3biRkoPJvevjzU61pA7JqOJSc7D5ZBx+j4rTq79yVNigV0M/PN0sAO1re/HMOSIiK8JEqYpUh0RpzaHrmPrbGXg5KfHvpM5wUFjvyZGXEjLwR1QcNp28hRspObrtno4K9Gnij2ea1cBjNXnmHBGRpWOiVEWsPVHKLVCj8/zdiE/PxYy+DTC0XajUIVWJojPnUvFH1C1sOXUbyQ+cOfd00wD0a1YDdXx55hwRkSWq7Oe31c0t/Pnnn2jdujVUKhXc3d3Rr18/vdtjY2PRp08fODg4wMfHBxMnTkRhIZfC0PrpUCzi03MR4GqPF1vXlDqcKiOTydAi2B0zn2mE/97rhlXDHsdzzWvAUWGDGyk5+HLXVfRY/C96L9mLZXuuIi415+EHJSIii2dVcyq//vorXn/9dcyZMwddu3ZFYWEhzpw5o7tdrVajT58+8PPzw4EDB3D79m288sorsLOzw5w5cySM3Dxk5xdi6e4rAID/61YHStvqWdhsZyNH53o+6FzPBzn5avxzPgGbouKw51Iizt9Ox/nb6fj47wtoFeqBZ5oF4MlG/nB35JlzRETWyGqm3goLCxESEoKZM2fi1VdfLXWfv//+G0899RTi4uLg6+sLAFi2bBneffdd3LlzBwrFwz/srHnqbenuK5i39SKCPR3wz/hOsGMxs57U7Hz8dToem6Ju4VCJM+e88XSzGuge7mPVNV1ERJaq2k+9HT9+HLdu3YJcLkfz5s3h7++P3r17640oHTx4EI0bN9YlSQDQq1cvpKen4+zZs6UeNy8vD+np6XoXa5SeW4Cv91wDAIztXodJUincHBR4qXVNrBsRgQOTu+K9J+ujgb8LCtQC/5xPxOifT6DlR//g12M3pQ6ViIiMxGo+Da9dK/qQnzFjBt5//31s2bIF7u7u6Ny5M1JSir79x8fH6yVJAHTX4+PjSz3u3Llz4erqqrsEBQWZ8KeQzrd7o5GWU4DaPk54umkNqcMxewFuKrzRsRb+GtMBkeM64v+61kaQhwrZ+WqsOXRd6vCIiMhIzD5Rmjx5MmQyWbmXCxcuQKPRAACmTp2K//3vf2jRogVWrlwJmUyGX375pdKPP2XKFKSlpekuN27cMNaPZjbuZuXj233RAIDxPerCRs5T4Q1Rx9cZE3rWw1eDWgAAYpKzJY6IiIiMxeyLKSZMmIChQ4eWu09YWBhu374NAGjQoIFuu1KpRFhYGGJjYwEAfn5+OHz4sN59ExISdLeVRqlUQqlUVjZ8i7Ds36vIzCtEwwAXPNGw9OeBHi7EyxEAkJKVj7ScAriqLLubORERWUCi5O3tDW9v74fu16JFCyiVSly8eBHt27cHABQUFCAmJgbBwcEAgIiICMyePRuJiYnw8fEBAERGRsLFxUUvwapOEjNysfpADABgQs+6kHM0qdKclLbwdlbiTkYeridnoUmgm9QhERHRIzL7qbeKcnFxwZtvvonp06dj+/btuHjxIkaOHAkAGDBgAACgZ8+eaNCgAQYPHoyTJ09i27ZteP/99zFq1CirHzUqy9JdV5FboEHzmm7oUs9H6nAsXoinAwBOvxERWQuzH1EyxPz582Fra4vBgwcjJycHrVu3xs6dO+Hu7g4AsLGxwZYtWzBy5EhERETA0dERQ4YMwYcffihx5NK4lZqDnw4VTUtO7FmPy3QYQYinI47E3EVMUpbUoRARkRFYVaJkZ2eHBQsWYMGCBWXuExwcjL/++qsKozJfX+y8jHy1BhFhnmhb20vqcKyCtk6JiRIRkXWwmqk3MkxMUhbWHy3q9/NOr7oSR2M9QjzvJUrJTJSIiKwBE6VqasmOy1BrBLrU80aLYA+pw7EaIV6sUSIisiZMlKqhywkZ+D3qFgBgfI96EkdjXYI99VsEEBGRZWOiVA0t/ucShACeaOiHxoGuUodjVbQtAgDgOqffiIgsHhOlaubMrTT8dToeMhkwvidrk0wh9N6oUjQLuomILB4TpWpmUeQlAMAzTQNQ19dZ4misU/C9XkrXWadERGTxmChVI8eu38XOC4mwkcswpjtHk0yFLQKIiKwHE6VqZOH2iwCA/o8FIvTehzkZn7ZFQDRrlIiILB4TpWriwJUkHLiaDIWNHKO715E6HKumbRHAqTciIsvHRKkaEEJg4b3apBdbBaGGm0riiKxbCFsEEBFZDSZK1cDui3dw7PpdKG3lGNWlttThWD1HtgggIrIaTJSsnBACC+7VJg1pGwIfF3uJI6oe2CKAiMg6MFGyctvOxuNsXDocFTZ4s1MtqcOpNrQtAmKSWKdERGTJmChZMbVGYOH2otqkV9uHwsNRIXFE1Ye2RQCn3oiILBsTJSu2+WQcLidmwsXeFq92CJM6nGpF236BLQKIiCwbEyUrVaDW4NN/ikaTRnSqBVeVncQRVS/3p96YKBERWTImSlZq4/GbiEnOhqejAkPbhkgdTrWjbRFwN7sAadlsEUBEZKmYKFmhvEI1PttxBQAwsnMtOCptJY6o+nFU2sLnXouAGE6/ERFZLCZKVmjt4Ru4lZoDPxd7vNwmWOpwqi3tqBITJSIiy8VEycrk5Kvxxa6i0aS3u9aGvZ2NxBFVX9qlTNgigIjIcjFRsjLfH4zBnYw8BLqrMLBlkNThVGvBHFEiIrJ4TJSsSEZuAZbtuQoAGNOtDhS2/PVKSdsigIkSEZHl4iepFVm5PwZ3swsQ5u2IZ5vXkDqcak9Xo8QWAUREFouJkpVIzc7HN/9eAwCM614Xtjb81UpN20uJLQKIiCwXP02txPJ/ryEjrxD1/ZzRp7G/1OEQ2CKAiMgaMFGyAkmZeVi5PwYAMKFnPcjlMmkDIh22CCAismxMlKzA0l1XkVOgRtNAV3QP95E6HCqGLQKIiCwbEyULdzstBz8eug6gaDRJJuNokjkJ4ZlvREQWjYmShfti5xXkF2rQKsQDHep4SR0OPUA79RbNM9+IiCwSEyULdiMlG+uO3AAATOhZl6NJZkibKF3niBIRkUViomTBPv3nMgo1Ah3qeKF1mKfU4VAptDVKbBFARGSZmChZqCuJmfjtxE0ARbVJZJ4cFPdbBERzVImIyOIwUbJQn/5zCRoB9Gjgi2ZBblKHQ+XQFnRz+o2syZGYFLSZswPbzsZLHQqRSTFRskDn4tKx5dRtAMD4HnUljoYeJuReh24WdJM1WbU/BvHpudgUdUvqUIhMiomSBVoUeQkA8FQTf4T7u0gcDT3M/REl9lIi61Co1mDv5TsA2COMrB8TJQsTdSMV/5xPgFwGjONokkUIZYsAsjJRN1KRnlsIAIhNyYYQQuKIiEyHiZKFWbj9IgDguccCUcvbSeJoqCKCuYwJWZndF+/o/p+ZV4jkrHwJoyEyLSZKFuS/a8nYezkJtnIZxnSrI3U4VEHaFgGp2QVIzeYHClm+3ZcS9a7zRAWyZkyULIQQAou2F9UmPf94EII8HCSOiCrKQWELX5eiFgExrFMiC3cnIw9nbqUDAOr4FI1qs/6OrJlVJUqXLl3CM888Ay8vL7i4uKB9+/bYtWuX3j6xsbHo06cPHBwc4OPjg4kTJ6KwsFCiiCtu7+UkHI5JgcJWjv/rytEkSxPMDt1kJf69VDTt1qiGC1qGuAPgFwCyblaVKD311FMoLCzEzp07cezYMTRt2hRPPfUU4uOL+nyo1Wr06dMH+fn5OHDgAFavXo1Vq1Zh2rRpEkdePiEEFtyrTRrcJhh+rvYSR0SGYkE3WYvd9xKlznV9UNODXwDI+llNopSUlITLly9j8uTJaNKkCerUqYOPP/4Y2dnZOHPmDABg+/btOHfuHH788Uc0a9YMvXv3xqxZs/Dll18iP998a0cizyXg1M00OChsMLJzLanDoUoIvlenFMNEiSyYWiN0bQE61fPW9Qjj1BtZM6tJlDw9PVGvXj18//33yMrKQmFhIb7++mv4+PigRYsWAICDBw+icePG8PX11d2vV69eSE9Px9mzZ6UKvVwajdD1TRrWLgReTkqJI6LKCNWd+cYPFLJcJ2+mIjW7AC72tmge5MYpZaoWbKUOwFhkMhn++ecf9OvXD87OzpDL5fDx8cHWrVvh7l40jx4fH6+XJAHQXddOzz0oLy8PeXl5uuvp6ekm+glKt+X0bVyIz4CzvS3e6MDRJEulbTrJFgFkybRtATrU8YatjRw1PYst+pxTAFeVnZThEZmE2Y8oTZ48GTKZrNzLhQsXIITAqFGj4OPjg7179+Lw4cPo168f+vbti9u3b1f68efOnQtXV1fdJSgoyIg/XfkK1Rp8em806fUOYXB14JuQpQr2ZIsAsnx77tUndarrDQBwUtrqRrljOVpKVsrsR5QmTJiAoUOHlrtPWFgYdu7ciS1btuDu3btwcSla1mPp0qWIjIzE6tWrMXnyZPj5+eHw4cN6901ISAAA+Pn5lXrsKVOmYPz48brr6enpVZYs/XbiFq4lZcHdwQ7D2oVUyWOSaWhbBCSk5yEmORvNHBRSh0RkkOTMPJy6mQqgqD5JK8TTAUmZeYhJzkLjQFeJoiMyHbNPlLy9veHt7f3Q/bKzi77NyOX6g2RyuRwajQYAEBERgdmzZyMxMRE+Pj4AgMjISLi4uKBBgwalHlepVEKprPq6oPxCDZbsuAwAGNm5FpztOZpk6YI9HYsSpaQsNAtykzocIoPsvZwEIYBwfxf4utw/87ampwOOXr/LOiWyWmY/9VZRERERcHd3x5AhQ3Dy5ElcunQJEydORHR0NPr06QMA6NmzJxo0aIDBgwfj5MmT2LZtG95//32MGjVKkmSoPOuO3sDNuznwdlZicJsQqcMhIwjlUiZkwR6cdtMK8eSiz2TdrCZR8vLywtatW5GZmYmuXbuiZcuW2LdvHzZt2oSmTZsCAGxsbLBlyxbY2NggIiICL7/8Ml555RV8+OGHEkevL7dAjS92Fo0mvd2lNlQKG4kjImPQFXSzRQBZGI1G6BpNdq6nnygFs0UAWTmzn3ozRMuWLbFt27Zy9wkODsZff/1VRRFVzo//XUdCeh5quKnwQquqKx4n09L2nInmBwpZmNO30pCclQ8npS1aBLvr3aZrEZDCLwBknaxmRMlaZOUVYunuqwCA0d1qQ2nL0SRroR1RYi0HWRrttFu72p6ws9H/2NB+AUhIz0N2vvkvB0VkKCZKZmbVgRikZOUjxNMBzz0WKHU4ZERsEUCWavfFRABA53o+JW5zc1DAxb5ociI2haOlZH2YKJmRtJwCfL2naDRpXI+6Jb65kWXTtggAuOYbWY7U7HxE3UgFULKQW+v+aCkTJbI+/CQ2Iyv2XkN6biHq+jrhqSYBUodDJsAzhMjS7L2cBI0A6vo6IcBNVeo+XMqErBkTJTORnJmH7/ZFAwDG96gLG7lM4ojIFLSJEkeUyFJoly0pbdpNK9jj3qLP/AJAVsiqznqzZIkZeQh0d4CdrQy9GpbeJZwsHwu6yZJoNEJXyN25jGk34H79HZcxIWvERMlMhPu74K8xHZCcmQeZjKNJ1irUiy0CyHKcu52OpMw8OChs0DLEo8z9uOgzWTNOvZkRG7kMPsWWBiDro63lYNNJsgTa0aS2tbygsC3740I79RaXmoP8Qk2VxEZUVZgoEVUh7RRFWg5bBJD5u98WoPz1Nr2dlVDZ2UAjgJt3OVpK1oWJElEVYosAshRpOQU4HpsKoOy2AFoymYxLmZDVYqJEVMXYIoAswf4rSVBrBGp5OyLo3tRaebSJEuuUyNowUSKqYqFebBFA5q+8btyl4RcAslZMlIiqmK6gm9+8yUwJUawtwEPqk7TYdJKsFRMloiqmbRHA5nxkri7EZyAhPQ8qOxs8Xk5bgOJYo0TWiokSURXT9Zzh1BuZKW037ohanrC3s6nQfbSJ0o272VBrhMliI6pqTJSIqliwR1GilJZTgLtZbBFA5qeibQGK83dVQWEjR4FaIC41x1ShEVU5JkpEVUylsIHfvcairFMic5ORW4Bj1+8CeHhbgOJs5DIEehQtmhubwuk3sh5MlIgkwFOpyVztv5KMQo1AqJejrkC7okJ4ogJZISZKRBII1dUp8Zs3mZc9l4qm3QwZTdKq6cGCbrI+TJSIJMBFRMkcCSGw514hdycD6pO0QnRnvvF1TdaDiRKRBLQfKDzzjczJ5cRMxKXlQmkrR0SYp8H3D/Zi00myPkyUiCRwf0SJHyhkPrRnu7UJq3hbgOKCi029CcEWAWQdmCgRSYAtAsgcabtxV6Y+CQAC3R0glwE5BWrcycgzZmhEkmGiRCSB4i0ColnPQWYgK68QR6KL2gIY0j+pOIWtHDXci1oEcLSUrAUTJSKJhHix8JXMx4GrychXa1DTw0F3VmZlaEdLeaICWQsmSkQS0faciWaLADIDxdsCyGSySh9H2yMsliNKZCWYKBFJJMSLq62TeRBC6NZ3q+y0mxabTpK1YaJEJBHdBwpbBJDErt7Jws27OVDYyBFRy/C2AMXV9GTTSbIuTJSIJKKtUYpOyuKp1CQpbVuAVqEecFDYPtKxio8o8XVN1oCJEpFEtEWv6bmFSM0ukDgaqs60bQEeddoNuL+MSQZf12QlmCgRSYQtAsgc5OSrcSg6BYBxEiWVwga+LkoArFMi68BEiUhCbBFAUjt4LQn5hRrUcFOhlreTUY4ZfG/6LTaFdUpk+ZgoEUlI26+GLQJIKsUXwX2UtgDF3V/LkK9rsnxMlIgkFMwz30hiu7X1SZVctqQ02tf19RS+rsnyMVEikpD2DCFOvZEUopOycD05G3Y2MrSt7WW04wazRQBZESZKRBK6P/XGU6mp6u251xagZbAHnJSP1hagOO0ZnfwCQNaAiRKRhLSnUqfnFuIuT6WmKrbbiG0BitM2nUzKzEdmXqFRj01U1ZgoEUlIpbCBv2tRiwCeSk1VKbdAjYNXkwEAnev5GPXYrio7eDgqAHBUiSwfEyUiiQXrzhDiBwpVnUPRKcgr1MDPxR51fY3TFqA47Wgp65TI0jFRIpKYtk4phh8oVIW0y5Z0NmJbgOJCWNBNVsJiEqXZs2ejbdu2cHBwgJubW6n7xMbGok+fPnBwcICPjw8mTpyIwkL9+fHdu3fjscceg1KpRO3atbFq1SrTB09UDi6OS1LQ9k8ydn2SVjDP6CQrYTGJUn5+PgYMGICRI0eWertarUafPn2Qn5+PAwcOYPXq1Vi1ahWmTZum2yc6Ohp9+vRBly5dEBUVhbFjx+K1117Dtm3bqurHICohuNgiokRVITY5G9eSsmArN25bgOJ0U8p8XZOFM975oCY2c+ZMAChzBGj79u04d+4c/vnnH/j6+qJZs2aYNWsW3n33XcyYMQMKhQLLli1DaGgoFi5cCAAIDw/Hvn37sHjxYvTq1auqfhQiPQ+2CDDFNAhRcXsuFU27PRbsDhd7O5M8hm4ZE069kYWzmBGlhzl48CAaN24MX19f3bZevXohPT0dZ8+e1e3TvXt3vfv16tULBw8eLPO4eXl5SE9P17sQGVPx1dbZIoCqwm4TT7sB92uU4tJykVugNtnjEJma1SRK8fHxekkSAN31+Pj4cvdJT09HTk5OqcedO3cuXF1ddZegoCATRE/VWfEWAdGsUyITyytU48C9tgCdjLhsyYM8HBW6JpY3uDguWTBJE6XJkydDJpOVe7lw4YKUIWLKlClIS0vTXW7cuCFpPGSduJQJVZUj0XeRU6CGj7MSDfxdTPY4MpmMS5mQVZC0RmnChAkYOnRoufuEhYVV6Fh+fn44fPiw3raEhATdbdp/tduK7+Pi4gKVSlXqcZVKJZRKZYViIKqsEC8HHLyWzDPfyOS0bQE61TVNW4DiQjwdcTYunQXdZNEkTZS8vb3h7W2cod+IiAjMnj0biYmJ8PEp6jIbGRkJFxcXNGjQQLfPX3/9pXe/yMhIREREGCUGosrStQjgN28yMe2yJZ1MWJ+kVZMjSmQFLKZGKTY2FlFRUYiNjYVarUZUVBSioqKQmZkJAOjZsycaNGiAwYMH4+TJk9i2bRvef/99jBo1Sjci9Oabb+LatWuYNGkSLly4gKVLl2L9+vUYN26clD8aEUK82CKATO/m3WxcScyEXAZ0qG36REnXdJI1SmTBLKY9wLRp07B69Wrd9ebNmwMAdu3ahc6dO8PGxgZbtmzByJEjERERAUdHRwwZMgQffvih7j6hoaH4888/MW7cOCxZsgSBgYFYsWIFWwOQ5LQjSmwRQKa0595o0mM13eHqYJq2AMWx6SRZA4tJlFatWvXQLtrBwcElptYe1LlzZ5w4ccKIkRE9Om3Rq7ZFgHZBUSJj0rYFMOXZbsVpX9e37uagQK2BnY3FTGIQ6fBVS2QG7O3YIoBMK79QgwNXkgAAnev5VMlj+jrbQ2krR6FGIC619BYsROaOiRKRmWCLADKlo9dTkJWvhpeTAg0DTNcWoDi5XKZrqMoTFchSVWjqrXnz5hWumTh+/PgjBURUXYV4ObJFAJmMdhHcjnW8IZdXXQ1csKcjLidmIjY5C0DVTPkRGVOFEqV+/frp/p+bm4ulS5eiQYMGutPq//vvP5w9exZvvfWWSYIkqg60ZwhF85s3mcCeKmwLUFyIJ0eUyLJVKFGaPn267v+vvfYaRo8ejVmzZpXYh12riSpP2yKAU29kbLfTcnAhPgNyWdGIUlW6352br2uyTAbXKP3yyy945ZVXSmx/+eWX8euvvxolKKLq6MEWAUTGop12axrkBvcqPqPyfosAjiiRZTI4UVKpVNi/f3+J7fv374e9vb1RgiKqjoq3CEjJypc4GrImumm3KmoLUJzuJIWUbGg0/AJAlsfgPkpjx47FyJEjcfz4cbRq1QoAcOjQIXz33Xf44IMPjB4gUXVhb2eDAFd7xKXlIiY5G55OXGOQHl2BWoN9l6u2LUBxAW72sJXLkF+oQXx6LgLcSl9Xk8hcGZwoTZ48GWFhYViyZAl+/PFHAEB4eDhWrlyJgQMHGj1Aouok2NOxKFFKykKLYHepwyErcPz6XWTkFcLDUYEmNVyr/PFtbeQIdFchJjkb15OzmSiRxTEoUSosLMScOXMwfPhwJkVEJqBtEcDCVzIW7bRbhzpeVdoWoLhgT8d7iVIWImp5ShIDUWUZVKNka2uLefPmobCw0FTxEFVroV5sEUDGpV22pHMVtwUoLpgtAsiCGVzM3a1bN+zZs8cUsRBVe9ozhNh0kowhMT0X526nQyZBW4DitK/r2BS+rsnyGFyj1Lt3b0yePBmnT59GixYt4OjoqHf7008/bbTgiKqb0Hu9lGKSi1oEVLQjPlFptNNujWu4SnpygK7pZBJHlMjyGJwoabtvL1q0qMRtMpkMarX60aMiqqa062JpWwTwzDd6FLvvJUqdJWgLUJx26i02JZtfAMjiGDz1ptFoyrwwSSJ6NNoWAUDRqBJRZRWqNdirW7ak6tsCFBfo7gCZDMjMK0Qye4SRhTE4USIi09IuZcJpCnoUUTdSkZ5bCFeVHZoFuUkaS9EXgKK2ADyj07wUqDVSh2D2DJ56A4CsrCzs2bMHsbGxyM/X/3YwevRoowRGVF0FezriwNVkjijRIyneFsBGorYAxdX0cMCt1BxcT85Gi2APqcMhAB9tOYe1R27gm1dasm1DOQxOlE6cOIEnn3wS2dnZyMrKgoeHB5KSkuDg4AAfHx8mSkSPSNsigKdS06O43xZA2mk3rRAvBxy8lszXtRn58/RtZOYVYty6KPw9pkOVrwNoKQyeehs3bhz69u2Lu3fvQqVS4b///sP169fRokULLFiwwBQxElUrIWwRQI/oTkYeTt9KAwB0rOslcTRFanpoF8fl69ocpGTl43ZaLgAgPj0X7/56iotxl8HgRCkqKgoTJkyAXC6HjY0N8vLyEBQUhHnz5uG9994zRYxE1cr9GqUsvnFRpey9XDSa1DDABT7O5rFYubZFwHWOKJmFc3HpAAA3BzvY2ciw/VwCfjocK3FU5sngRMnOzg5yedHdfHx8EBtb9MS6urrixo0bxo2OqBqq6VF0hlBGXlGLACJDmUM37gdpm05yRMk8nI0rGnFsW8sTk3rVBwDM2nIOlxMypAzLLBmcKDVv3hxHjhwBAHTq1AnTpk3DmjVrMHbsWDRq1MjoARJVN/Z2NvB3YYsAqhy1RuDfeyNKneqaR30SANS8N6J0N7sAaTkFEkdD524XjSg1DHDFq+1D0aGOF3ILNBi9Ngq5BWz1U5zBidKcOXPg7+8PAJg9ezbc3d0xcuRI3LlzB8uXLzd6gETVEVsEUGWdupmK1OwCONvb4rGablKHo+OktIXXvQaqsZx+k9zZe1NvDQJcIJfLsHBAU3g4KnD+djrmbb0ocXTmxeBEqWXLlujSpQuAoqm3rVu3Ij09HceOHUPTpk2NHiBRdRRSbCkTIkNop9061PGCrY15tcrTLWXC17WkcvLVuHYnEwDQ0N8FAODjYo/5/ZsAAL7bH41dFxMli8/cGPxX9N133yE6OtoUsRDRPdoPlGie+UYG0i5b0kniZUtKU1NX0M3XtZQuxKdDIwAvJyV8XO4X+3cL98XQtiEAgIm/nMSdjDyJIjQvBidKc+fORe3atVGzZk0MHjwYK1aswJUrV0wRG1G1FaIrfOUUBVVcSlY+Tt1MBWBe9UlafF2bh+LTbg+a3Ls+6vk6IykzH+/8chIaDc+8NThRunz5MmJjYzF37lw4ODhgwYIFqFevHgIDA/Hyyy+bIkaiaoctAqgy9l6+AyGA+n7O8HM1j7YAxQWzRYBZuF/IXTJRsrezwWcvNofSVo49l+5g5YGYKo7O/FRqArtGjRoYNGgQFi9ejCVLlmDw4MFISEjA2rVrjR0fUbVUvEUAFxGlitLWJ3Uyo7YAxWlbBLBGSVraEaXSEiUAqOfnjPf7hAMAPvn7gq6VQHVlcKK0fft2vPfee2jbti08PT0xZcoUuLu7Y8OGDbhz544pYiSqdriIKBlKoxH49159UmcznHYD7tfeJWbkITu/UOJoqqdCtQYX7o0oNfAvPVECgJfbBKN7uC/y1RqM/vlEtf59GZwoPfHEE/j222/Rr18/3L59G8ePH8fixYvxzDPPwN3d3RQxElVLwbqCbk5T0MOdiUtDclY+nJS2aBlinu/Fbg4KuKrsAACxKXxdSyE6KQt5hRo4KGx0NWOlkclkmNe/CXyclbh6JwuztpyvwijNi8GJ0qJFi9CuXTvMmzcPDRs2xEsvvYTly5fj0qVLpoiPqNrS1ilxRIkqQjvt1q62J+zMrC1AcaxTkpZ22i3cv6h/Unk8HBVY/HwzyGTAz4djsfXM7aoI0ewY/Nc0duxYbNy4EUlJSdi6dSvatm2LrVu3olGjRggMDDRFjETVUui9b3tsEUAVseeS+XXjLg2XMpFWeYXcpWlX2wtvdAwDALz762ncTssxWWzmqlJfO4QQOH78OCIjI7Ft2zbs2rULGo0G3t7mWUBIZImC2ZyPKig1Ox8nYu8CMK/13Upzv+kkR5SkoC3MrmiiBAATetRDk0BXpOUUYOzaKKirWcsAgxOlvn37wtPTE61atcKaNWtQt25drF69GklJSThx4oQpYiSqlkK1U29J2WwRQOXaezkJGgHU9XVCgJtK6nDKVdOjKFHiMiZVTwiBc9oeSv6uFb6fwlaOJS80h4PCBoeiU7Bsz1VThWiWbA29Q/369TFixAh06NABrq4Vf6KJyDBBD7QI0K6TRfQgXVsAM+zG/SAuzyOd22m5uJtdAFu5DHV8nQy6b6iXI2Y+3RATN5zCoshLaFvLE81rmudJA8Zm8IjS/Pnz8dRTT8HV1RW5ubmmiImIoN8iIIZ1SlQGjUbo6pM61zPv+iQACL43ohSXmoP8Qo3E0VQv2kLu2j5OsLezMfj+/VsE4qkm/lBrBMasjUJGboGxQzRLBidKGo0Gs2bNQo0aNeDk5IRr164BAD744AN8++23Rg+QqDoL8WI9B5Xv3O10JGXmwUFhY7ZtAYrzdlZCZWcDjQBu3uXruiqdK2fpkoqQyWSY/Wxj1HBTITYlG9M3nTVmeGbL4ETpo48+wqpVqzBv3jwoFArd9kaNGmHFihVGDY6outN1MuaIEpVBO5rUtpYnlLaGjxJUNZlMxhYBEtEWcpfXaPJhXFV2WPJCM8hlwMYTt/D7iVvGCs9sGZwoff/991i+fDkGDRoEG5v7f5RNmzbFhQsXjBocUXUXyiUf6CH26JYtMf9pNy2e0SmN+60BHq2+uGWIB0Z3qwMAeP/3M1ZfmG9wonTr1i3Url27xHaNRoOCAtPNV86ePRtt27aFg4MD3NzcStx+8uRJvPjiiwgKCoJKpUJ4eDiWLFlSYr/du3fjscceg1KpRO3atbFq1SqTxUz0qPiBQuVJyynAMW1bAAso5NYK0fVSsu4PWHOSll2Am3eLeiBVduqtuLe71EbLYHdk5hVi9NoTKFBbb72ZwYlSgwYNsHfv3hLbN2zYgObNmxslqNLk5+djwIABGDlyZKm3Hzt2DD4+Pvjxxx9x9uxZTJ06FVOmTMEXX3yh2yc6Ohp9+vRBly5dEBUVhbFjx+K1117Dtm3bTBY30aPQtgiIYYsAKsX+K0lQawTCvB0RdK9I2hKw6WTVO3u7aNot0F2lW0bmUdjayPHpC83gbG+LqBup+GzH5Uc+prkyuD3AtGnTMGTIENy6dQsajQYbN27ExYsX8f3332PLli2miBEAMHPmTAAocwRo+PDhetfDwsJw8OBBbNy4EW+//TYAYNmyZQgNDcXChQsBAOHh4di3bx8WL16MXr16mSx2osrStgjIZIsAKoV22s1cF8EtC2uUqp62kNuQRpMPE+jugLnPNcbbP53AF7uuoF1tL7QJ8zTa8c2FwSNKzzzzDDZv3ox//vkHjo6OmDZtGs6fP4/NmzejR48epoix0tLS0uDh4aG7fvDgQXTv3l1vn169euHgwYNlHiMvLw/p6el6F6KqwhYBVBYhircFsJxpN+B+onTjbna16/Islco0mqyIp5oEYECLQAgBjFsXhdTsfKMe3xxUagmTDh06IDIyEomJicjOzsa+ffvQs2dPHD161NjxVdqBAwewbt06vPHGG7pt8fHx8PX11dvP19cX6enpyMkpff2auXPnwtXVVXcJCgoyadxED2KLACrNhfgMxKfnwt5OjlahHg+/gxnxd1VBYSNHgVogLrX6rR0mBUPXeDPEjKcbItTLEbfTcjH519NWVyZgcKKUmZlZIqmIiopC37590bp1a4OONXnyZMhksnIvlTmT7syZM3jmmWcwffp09OzZ0+D7FzdlyhSkpaXpLjdu3Hik4xEZKoQtAqgU2tGkiDDPSjUPlJKNXIZAj6KRUk6/mV5ugRqXEzMBAA1rGD9RclTaYskLzWBnI8PWs/FYd8S6PicrnCjduHEDERERupGV8ePHIzs7G6+88gpat24NR0dHHDhwwKAHnzBhAs6fP1/uJSwszKBjnjt3Dt26dcMbb7yB999/X+82Pz8/JCQk6G1LSEiAi4sLVKrS10dSKpVwcXHRuxBVJW2iFM3CVypm98VEAJbRjbs0ujPfUvi6NrVLCRlQawTcHezg52JvksdoEuiGd3rWAwDM3HwOV+4lZtagwsXcEydORG5uLpYsWYKNGzdiyZIl2Lt3L1q3bo2rV68iMDDQ4Af39vaGt7fx5tbPnj2Lrl27YsiQIZg9e3aJ2yMiIvDXX3/pbYuMjERERITRYiAyNu3aWDxDiLQycgtwNKaoLYAlrO9WGhZ0V537hdyukMlkJnuc1zuE4d/Ld7D/SjJG/3wCv41qaxFNUB+mwiNK//77L7766iu8/fbbWLt2LYQQGDRoEL744otKJUmGio2NRVRUFGJjY6FWqxEVFYWoqChkZhZlrWfOnEGXLl3Qs2dPjB8/HvHx8YiPj8edO3d0x3jzzTdx7do1TJo0CRcuXMDSpUuxfv16jBs3zuTxE1VWiLaXElsE0D0HriajUCMQ4umgS6QtjXbNN34BML2zj7h0SUXJ5TIsGtgM7g52OHc7HfO3XjTp41WVCidKCQkJCA0NBQD4+PjAwcEBvXv3NllgD5o2bRqaN2+O6dOnIzMzE82bN0fz5s11BeQbNmzAnTt38OOPP8Lf3193efzxx3XHCA0NxZ9//onIyEg0bdoUCxcuxIoVK9gagMxa8RYBSZnWd0YJGW73RctZBLcswV5sOllVTFnI/SBfF3vM798UALBiX7Suls6SGVTMLZfL9f5ffK03U1u1ahWEECUunTt3BgDMmDGj1NtjYmL0jtO5c2ecOHECeXl5uHr1KoYOHVplPwNRZRRvEcBv36a180IC+n91AIsjL+FGinl+gAshsOdefZKlTrsB+t25OVJqOmqNwPnb2tYAVVNj272BL16JCAYATFh/EkmZeVXyuKZS4URJCIG6devCw8MDHh4eulEd7XXthYiMT9siIJpnvplMVl4hJm04jaPX72LJjsvoMG8XXlh+EL8eu4ns/EKpw9O5kpiJuLRcKGzlFt3cr4abCnIZkFOgxp0My/4gNWcxyVnIzlfD3k6OMG+nKnvc954MR11fJyRl5mHiLyctOhmucDH3ypUrTRkHEZUjxNMR+68kc5rChL7Zew1JmXmo4aZCqJcj9l9Nwn/XUvDftRRM23QGfZr4Y0DLILQMdjdpQezDaKfd2oR5QqWw3EJZha0cNdxVuJGSg5jkbPiY6Gys6k5byF3fzwU28qp73drb2eCzF5vj6S/2Y9fFO1h1IAbD2oVW2eMbU4UTpSFDhpgyDiIqB1sEmNadjDws//caAGDKk/XxVJMA3ErNwcZjN7Hh+E1cT87G+qM3sf7oTYR4OqB/i0A891ggAtxKbytiSrsv3WsLYMHTblrBHo73EqUsi2uaaSmqqpC7NPX9XDD1yXBM/+Ms5v51AW3CPBFeRdN/xlSpztxEVLVCvNh00pSW7LiE7Hw1mga6ok9jfwBFU0P/160Odr/TGetHRGBAi0A4KGwQk5yNBdsvod0nOzH420PYFHULuQXqKokzK68QR6LvtQWwsGVLSqNtERDLkVKTqcpC7tK8EhGMbvV9kK/WYPTPJ5CTXzV/K8bERInIAoR63e85Y8lz/ebo6p1M/Hy4qJPwlCfDS0yryWQytAr1wPwBTXFkancsGNAUrUM9IASw93ISxqyNwuOz/8F7v53Gidi7Jv39HLyajHy1BkEeKoRZaFuA4nRd5zlSahJCCJyLSwNQdYXcD5LJZJjXvwm8nZW4nJiJ2X+dkySOR8FEicgCBLqzRYCpzN96EWqNQLf6Pg8tjnZU2qJ/i0CsGxGBfyd2wehudVDDTYWM3EL8dCgWzy49gB6L/8WyPVeRmJ5r9FjvT7v5SFonZSw12XTSpO5k5CEpMx9yWdE0mFQ8nZRYNLCoZcCP/8Vi29l4yWKpDCZKRBageIsAfvs2nmPXU7D1bDzkMuDd3vUNum9NTweM71EXeyd1wU+vtcazzWvA3k6OK4mZ+PjvC2gzdweGrTyMv07fRl7ho083CCF0hdyW3BaguOIjShwpNT5tfVItbyfJC/871PHGGx2LliR799dTiE8z/hcJU2GiRGQhQlmnZFRCCMz9q2jR7YEtg1DX17lSx5HLZWhb2wuLn2+Gw1O74+PnGqNFsDs0Ath18Q7eWnMcrefswPRNZ3DmVlqlE4JrSVm4eTcHChs52ta23LYAxdW81507I7cQqdkFEkdjfc5qp90kqk960Ds966FRDRekZhdg3LooqDWWkRxX+Kw3LbVajVWrVmHHjh1ITEyERqPRu33nzp1GC46I7gv2dMC+KxxRMpbt5xJw9Ppd2NvJMa5HXaMc08XeDi+0qokXWtXEtTuZ2HDsJjYev4X49FysPngdqw9eR30/Z/RvEYhnm9eAp5OywsfWjia1CvWAg8Lgt26zpFLYwNdFiYT0PMQkZ8HdseqaGFcHUhdyP0hhK8dnLzRHn8/24eC1ZCz/9xpGdq4ldVgPZfCI0pgxYzBmzBio1Wo0atQITZs21bsQkWnoRpRYz/HICtUafLK1aDTptfZh8DVBD58wbydMeqI+9k/uitXDW+GpJv5Q2MpxIT4DH/15Hq3n7MAb3x/F9rPxKFBrHnq83VbQjbs0wfem32LNtBO6JdO1BvB3lTiS+8K8nTDz6YYAgIXbL+LkjVRpA6oAg7+WrF27FuvXr8eTTz5piniIqAzaDxROvT26dUdv4NqdLHg4KjCiU5hJH8tGLkOnut7oVNcbadkF+ONUHDYcvYGTN9Ow/VwCtp9LgJeTAv2a1UD/loGlFt3m5KtxKDoFANDZCtoCFBfi6YDD0SmISWKiZEwZuQW6InlzGVHSGtAyEHsu3cGfp29j9NoT+HN0BzgpzXeU1OARJYVCgdq1a5siFiIqh7ZFQEwSC18fRVZeIRZHXgYAjO5aG872dlX22K4OdhjcJhib3m6P7eM64o2OYfByUiIpMx8r9kXjiU/3ou/n+/D9wRikZt8/u/G/a8nIL9SghpsKtX2qbhmKqhCsW/ONXwCM6fztDABAgKu92U1pymQyzHm2MWq4qXA9ORvTN52VOqRyGZwoTZgwAUuWLOEbNVEVC/IoahGQla9mi4BHoF2qJNjTAS+1DpYsjrq+znjvyXAcnNIV3w5piSca+sHORobTt9IwbdNZtJq9A6PWHMeuC4nYcSEBANCxrrdVtAUoTtt08jqn3ozK3Aq5H+TqYIfFzzeDXAb8evwmNkXdkjqkMhk81rVv3z7s2rULf//9Nxo2bAg7O/1vYxs3bjRacER0n9K2qEXArdSiJR+8nSteCExFii9VMrFXPShspT/x185Gjm7hvugW7ouUrHxsirqFX47exLnb6fjz9G38efq2bl9rm3YD7rcI4IiScZ3TLV1iPvVJD2oV6oG3u9bBZzsu4/3fzuCxmu4IuncmpDkxOFFyc3PDs88+a4pYiOghQr0cixKlpCw8HsK1sQxV2lIl5sTDUYFh7UIxrF0ozsalYcOxm9gUFYeUrHw4KmzQrraX1CEanbbpZFJmPjLzCs26VsWS3C/kNs8RJa3RXWtj/5UkHLt+F2PWnsD6ERGwtZH+C0xxBr8iV65caYo4iKgCQrzYIqCyHrZUiblpGOCKhgGumNI7HPuvJsHHWWmVSYSLvR08HBVIycrH9eQsNDTjERBLkV+oweXEoholcyvkfpCtjRyfPt8MTy7Zi+Oxqfhs5xWMN1K7DmMxr7SNiMql62TMM4QMZshSJeZEYStHl3o+Vp1ABHMpE6O6nJiBArWAi70tAt1VUofzUEEeDpj9XGMAwBc7L+PwvTM8zUWlvp5s2LAB69evR2xsLPLz9YtKjx8/bpTAiKgkLiJaOY+yVAmZXrCHA07EpjJRMhLdtFuAi9mPnGo93TQAey7ewa/Hb2Ls2hP4e0xHuDpU3Rmp5TF4ROmzzz7DsGHD4OvrixMnTqBVq1bw9PTEtWvX0Lt3b1PESET3hLBFgMGMtVQJmQ5bBBiXtpDb0kYhZz7TECGeDohLy8V7v582m/c4gxOlpUuXYvny5fj888+hUCgwadIkREZGYvTo0UhLSzNFjER0T5CHA+T3WgTcycyTOhyLYIqlSsi4tFNvHCk1jnMWUsj9ICelLZa80By2chkOR6cgId083uMMTpRiY2PRtm1bAIBKpUJGRlHB2ODBg/Hzzz8bNzoi0qO0tUGAW1HNAacpHq4qliqhR6dbxoSv6Uem0Yj7a7zVsKxECQCaBrnhi5eaY+uYDvBzNY+/V4MTJT8/P6SkFBVa1axZE//99x8AIDo62myGyYismbZOKZpLmTxUVS5VQpUXcm9EKS4tF7kFaomjsWw37mYjM68QCls5anlbZhf3Jxr5G7RgtKkZnCh17doVf/zxBwBg2LBhGDduHHr06IHnn3+e/ZWIqoC2Ton1HOWTcqkSMoyHo0LX+uAGO3Q/Em0hdz1fZ9iZWT8iS2XwWW/Lly+HRlO00vWoUaPg6emJAwcO4Omnn8aIESOMHiAR6WOLgIpZsTfaLJYqoYeTyWQI9nTA2bh0XE/ORh0W3Ffa/UJuy5t2M1cGJ0pyuRxy+f0s9YUXXsALL7xg1KCIqGycenu4Oxl5+PrfqwDMZ6kSKl+IpyPOxqWzoPsRmfsab5aoUu8ee/fuxcsvv4yIiAjculW0kN0PP/yAffv2GTU4IiopxOv+qdSsCyyduS9VQiXVZNNJo9AVcjNRMhqDE6Vff/0VvXr1gkqlwokTJ5CXV3T6XlpaGubMmWP0AIlIX5CHii0CymFpS5VQEW1B93XWKFVaUmYeEtLzIJMB9f2YKBmLwYnSRx99hGXLluGbb76Bnd394sh27dqxKzdRFSjeIoB1SiVZ6lIl1R2bTj46bX1SqKcjHK1wXUCpGJwoXbx4ER07diyx3dXVFampqcaIiYgeItSLS5mUhkuVWC5t08mbd3NQoNZIHI1l0p7xFs5pN6OqVB+lK1eulNi+b98+hIWxTwlRVdB1MmZBtw6XKrFsvs72UNrKodYIxKXmSB2ORdIWcrM+ybgMTpRef/11jBkzBocOHYJMJkNcXBzWrFmDd955ByNHjjRFjET0gBDdNAWn3rS4VIllk8tlxZYy4eu6Mu4XclvWGm/mzuBJzMmTJ0Oj0aBbt27Izs5Gx44doVQq8c477+D//u//TBEjET2ALQL0cakS61DTwxGXEjIRm5wFwFvqcCxKVl6h7v3A0tZ4M3cGJ0oymQxTp07FxIkTceXKFWRmZqJBgwZwcrLMVulEliikWI2SEKLan9nFpUqsQwhHlCrtQnwGhAB8nJXwdjaf5T+sQaXL4hUKBRo0aGDMWIiogrQtArLvtQjwca6+IyhcqsR6BHvxzLfKOsdGkyZT4URp+PDhFdrvu+++q3QwRFQx2hYBN+/mICYpu1onSlyqxHoEe7DpZGWd5dIlJlPhRGnVqlUIDg5G8+bN2Q2YyAyEejkWJUrJWWgV6iF1OJLgUiXWRXeSQko2NBoBubx6TykbgoXcplPhRGnkyJH4+eefER0djWHDhuHll1+Gh0f1fHMmMgfBng7Ye7l6twjgUiXWJcDNHrZyGfILNYhPz9U1VqXyFag1uBCfAYCF3KZQ4a9fX375JW7fvo1JkyZh8+bNCAoKwsCBA7Ft2zaOMBFJQPvtu7o2neRSJdbH1kaOQPei5IjTbxV37U4W8gs1cFLaoua96UsyHoPGqZVKJV588UVERkbi3LlzaNiwId566y2EhIQgMzPTVDESUSl03bmr6TImXKrEOnEpE8NpG02G+ztzutIEKj2hL5fLIZPJIISAWq02Zkylmj17Ntq2bQsHBwe4ubmVu29ycjICAwMhk8lKLKuye/duPPbYY1AqlahduzZWrVplspiJTCnYU79FQHXCpUqsF5tOGu5+ITfrk0zBoEQpLy8PP//8M3r06IG6devi9OnT+OKLLxAbG2vyPkr5+fkYMGBAhbp/v/rqq2jSpEmJ7dHR0ejTpw+6dOmCqKgojB07Fq+99hq2bdtmipCJTKqmh8P9FgEZeVKHU2WKL1UyoAWXKrE22i8AsSkcUaoo7WK4bA1gGhUu5n7rrbewdu1aBAUFYfjw4fj555/h5eVlytj0zJw5EwAeOgL01VdfITU1FdOmTcPff/+td9uyZcsQGhqKhQsXAgDCw8Oxb98+LF68GL169TJJ3ESmorCVo4a7CjdSchCTnA2fatKNmkuVWDdd08lqOqVsKCGEbuqNhdymUeFEadmyZahZsybCwsKwZ88e7Nmzp9T9Nm7caLTgDHXu3Dl8+OGHOHToEK5du1bi9oMHD6J79+5623r16oWxY8eWecy8vDzk5d3/tp6enm60eIkeVYinY1GilFQ9WgQ8uFSJn2v1SA6rE+3U23V2na+QW6k5SM8thJ2NjKOrJlLhROmVV14x6xdsXl4eXnzxRcyfPx81a9YsNVGKj4+Hr6+v3jZfX1+kp6cjJycHKlXJU1Hnzp2rG80iMjchno7Yezmp2pz5xqVKrF+guwNkMiArX43krHx4OXE5jvJo65Nq+zizj5iJGNRw0tgmT56MTz75pNx9zp8/j/r1H16sOWXKFISHh+Pll182Vni6444fP153PT09HUFBQUZ9DKLKul/4av2JEpcqqR7s7WwQ4KrCrdQcXE/OYqL0EOfYkdvkKr3WmzFMmDABQ4cOLXefsLCKfWvcuXMnTp8+jQ0bNgCA7iwgLy8vTJ06FTNnzoSfnx8SEhL07peQkAAXF5dSR5OAopYISiX/UMk8aVsERFeDeg4uVVJ91PRwwK3UouV5WgRb/5Tyo9COKLE+yXQkTZS8vb3h7e1tlGP9+uuvyMnJ0V0/cuQIhg8fjr1796JWrVoAgIiICPz1119694uMjERERIRRYiCqaiHFFhG15noOLlVSvYR4OeDgtWRcT7H+LwCPSrsYLkeUTEfSRMkQsbGxSElJQWxsLNRqNaKiogAAtWvXhpOTky4Z0kpKSgJQdGabtu/Sm2++iS+++AKTJk3C8OHDsXPnTqxfvx5//vlnVf4oREYT5K7fIsBaz3zjUiXVC5tOVszdrHzEpeUCAMKZKJmMxSRK06ZNw+rVq3XXmzdvDgDYtWsXOnfuXKFjhIaG4s8//8S4ceOwZMkSBAYGYsWKFWwNQBareIuA6KQsq0yUuFRJ9RPsoT3zjSNK5dEuhFvTwwEurNkzGYtJlFatWmVQQXnnzp1L7VbcuXNnnDhxwoiREUlL2yLgenI2WlvhUh5cqqT64YhSxbCQu2pwop/IwmkXx422wg+VY9fvcqmSakh7Nufd7AKk5RRIHI35YqPJqsFEicjCFS/otiZFS5WcB8ClSqobR6Wtri1ALKffyqRb460GEyVTYqJEZOG0Sz5YW4sALlVSvYVUox5hlZFboMbVO5kAuBiuqTFRIrJwD7YIsAZcqoRqFlvKhEq6EJ8BjQA8HRXwcWavP1NiokRk4R5sEWANuFQJhegKuq1rpNRYtIXcDQJceCaoiTFRIrJw2hYBABCdZPnfvrlUCQHFF8dlolQaXSE3z3gzOSZKRFbAmr59c6kSAu63CGCNUul0hdysTzI5JkpEVsBaWgRwqRLS0hZzJ2bkITu/UOJozItaI3Ahnmu8VRW+CxFZAW1Bd4yFT719tuMylyohAICbgwKuqqJp11iu+aYnOikTuQUaqOxsdAtjk+kwUSKyAqFe2lOpLfcD5eqdTPx0OBYAlyqhIto6pRgra33xqLTTbuH+zrCR8+/E1JgoEVmB4ks+WGqLAC5VQg/Svq5jUyx7pNTYip/xRqbHRInIChRvEZBogS0CuFQJleZ+00mOKBXHQu6qxUSJyAoobOUIdNdOU1jWt28uVUJlqenBppMPEkLg3G0WclclJkpEViLYQpd84FIlVJb7Xec5oqQVn56LlKx82MhlqOfHLxVVgYkSkZXQnv1iSdMUXKqEyqNN/uNSc5BfqJE4GvOgrU+q7e0EezsbiaOpHpgoEVkJXYM+C5p641IlVB5vJyUcFDbQCODmXcv5AmBKZ1nIXeWYKBFZCW2LAEtZxoRLldDDyGSyYnVKTJSA+yNKDZkoVRkmSkRWovgyJpbQIoBLlVBFhHApEz1nb99b442F3FWGiRKRlQi81yIgp8D8WwRcSsjAci5VQhXAxXHvS8spwI2UHACceqtKfHcishLFWwSY8/Tb6ZtpeP7rg8jKV6NlsDuXKqFyFW+mWt2dv9cWoIabCm4OComjqT6YKBFZkfvfvs3zQ+VoTApe+uY/3M0uQNNAV6wY0pJLlVC5OKJ0Hwu5pcFEiciKaFsERJvh2lj7ryRh8LeHkZFXiFahHvjxtdb8VkwPpU2UbtzNhlpj/rV3psRCbmkwUSKyIiFmOk3xz7kEDFt1BDkFanSs643Vw1rxLDeqEH9XFRQ2chSoBeJSc6QOR1Jn41jILQUmSkRWJMQMWwRsPhmHN388hvxCDXo19MU3r7SASsFGeVQxNnIZAj1UAKr39FteoRpXEjMBAA1rcI23qsREiciKmFuLgPVHb2DM2hMo1Aj0axaAL196DEpbJklkGN3rOsV8vgBUtcsJmSjUCLiq7BDADvZViokSkRUJdHeAjVxmFi0CVu2PxqQNp6ARwIutamLRwGawteFbDhmOBd33p90aBrjwBIgqxnctIiuisJWjhlvRNIWU029f7rqCGZvPAQBeax+KOc82glzON3eqnOB73bktaXkeY2Mht3SYKBFZmfsrrlf9h4oQAvO3XcD8bRcBAGO61cHUPuH8BkyPJPjeazo2pTqPKLE1gFSYKBFZmRBPbUF31X6oaDQCMzefw5e7ijpuv/dkfYzrUZdJEj0yc6u9q2oajdA1m2wYwELuqsZEicjK6NbGqsJpCrVGYPLGU1h1IAYAMKtfI7zRsVaVPT5ZtxpuKt3yPHfMfHkeU7ieko2sfDWUtnKE3Rtdo6rDRInIymibTlbVIqIFag3GrD2B9UdvQi4DFg5oisFtuMgtGY/CVo4a7kW1dzHVsKBbW8hd38+ZJ0RIgM84kZUpfoaQqacpcgvUGPnjMWw5dRt2NjJ8+dJj+F+LQJM+JlVPupFSM2umWhXO6eqTOO0mBSZKRFameIuAhHTTTVNk5xfi1dVH8M/5RCht5Vg+uCV6c4FbMpGa9858i62WI0os5JYSEyUiK1O8RYCpvn2n5xbglW8PY/+VZDgobLBy2OPoUt/HJI9FBFTzEaXbbA0gJSZKRFZI2yLAFAXdKVn5eOmb/3D0+l242Nvix9dao20tL6M/DlFxNatp08nEjFzcyciDTFZUo0RVj4kSkRUKvfehYuzC18T0XLyw/CDO3EqHp6MCP7/RBo/VdDfqYxCVpviIUnVqEaCddgvzcoSDwlbiaKonJkpEVijYBC0Cbt7NxsCvD+JSQiZ8XZRYNyKCPV2oymhrlDJyC5GaXSBxNFWHhdzSY6JEZIWM3SIgOikLA5cdRExyNoI8VPhlRFvU9nEyyrGJKkKlsIGvixJA9apT4tIl0mOiRGSFQryMN01xMT4DA5YdRFxaLsK8HbF+RISuXoSoKmlHSqvTUiYs5JaexSRKs2fPRtu2beHg4AA3N7cy91u1ahWaNGkCe3t7+Pj4YNSoUXq3nzp1Ch06dIC9vT2CgoIwb948E0dOVPUC3VWwkcuQW6B5pBYBp26m4vnlB5GUmYdwfxesHxEBf1eVESMlqjjt8jwxVbw8j1Qy8wp1i1s38GeiJBWLqQzLz8/HgAEDEBERgW+//bbUfRYtWoSFCxdi/vz5aN26NbKyshATE6O7PT09HT179kT37t2xbNkynD59GsOHD4ebmxveeOONKvpJiEzPzkaOQHcVridnIzopC36u9gYf40hMCoatPILMvEI0C3LD6mGt4OpgZ4JoiSom2FO6BZ+loF3fzc/FHp5OSomjqb4sJlGaOXMmgKIRo9LcvXsX77//PjZv3oxu3brptjdp0kT3/zVr1iA/Px/fffcdFAoFGjZsiKioKCxatIiJElmdYE9HXE/OxvXkLETU8jTovnsv38Hr3x9FboEGbcI8sGLI43BSWszbBVkpXdf5ajL1do6NJs2CxUy9PUxkZCQ0Gg1u3bqF8PBwBAYGYuDAgbhx44Zun4MHD6Jjx45QKBS6bb169cLFixdx9+7dUo+bl5eH9PR0vQuRJdC2CIg28Nv39rPxeHVVUZLUuZ43Vg1rxSSJzEJINRtR0q7xxvokaVlNonTt2jVoNBrMmTMHn376KTZs2ICUlBT06NED+fn5AID4+Hj4+vrq3U97PT4+vtTjzp07F66urrpLUFCQaX8QIiPRFnRfN6CeY1PULYxccxz5ag16N/LD8sEtYW9nY6oQiQyiPYkgKTMfmXmFEkdjeizkNg+SJkqTJ0+GTCYr93LhwoUKHUuj0aCgoACfffYZevXqhTZt2uDnn3/G5cuXsWvXrkrHOGXKFKSlpekuxUeoiMyZoUs+rD0ci7HroqDWCDzXvAY+f7E5FLZW812KrICLvR08HItmBKx9VKlArcGl+EwAQAN/9lCSkqTj6RMmTMDQoUPL3ScsLKxCx/L3L1qMs0GDBrpt3t7e8PLyQmxsLADAz88PCQkJevfTXvfz8yv1uEqlEkoli+jI8hRvEaDRCMjlsjL3/XZfNGZtOQcAGNS6JmY906jc/YmkEuzpgJSsfFxPzrbqhqeXEzKRr9bAWWmLIA+eaSolSRMlb29veHt7G+VY7dq1AwBcvHgRgYGBAICUlBQkJSUhODgYABAREYGpU6eioKAAdnZFZ+9ERkaiXr16cHfnMgxkXYq3CEjMyCv1zDchBL7cdQULtl8CALzRMQxTeteHTMYkicxTsIcDTsSmWv2ab9ppt/AAF/49SsxixtVjY2MRFRWF2NhYqNVqREVFISoqCpmZRUOTdevWxTPPPIMxY8bgwIEDOHPmDIYMGYL69eujS5cuAICXXnoJCoUCr776Ks6ePYt169ZhyZIlGD9+vJQ/GpFJaFsEAND1YilOCIFPtl7UJUnjutdlkkRmr7q0CGAht/mwmERp2rRpaN68OaZPn47MzEw0b94czZs3x9GjR3X7fP/992jdujX69OmDTp06wc7ODlu3btWNHrm6umL79u2Ijo5GixYtMGHCBEybNo2tAchqlXWWkEYjMOOPs1i25yoAYOqT4RjTvQ6TJDJ7IV7aBZ+tO1G6v3SJ9U4vWgqLOed31apVZfZQ0nJxccG3335bZkNKoKiv0t69e40cHZF5CvF0wB7otwhQawTe/fUUNhy7CZkM+KhfIwxqHSxdkEQGqOlxbxkTK556E0Lopt7YkVt6FpMoEZHhdAXd96be8gs1GLc+Cn+eug0buQwLBjTBs80DpQyRyCDaZUzi0nKRW6C2yvYVN1JykJFbCIWNnItPmwGLmXojIsPpeiklZyO3QI2RPx7Dn6duw85Ghi9feoxJElkcD0eFrgHqDSvt0H3udlF9Uh1fJ7boMAP8DRBZseK9lIavOoIdFxKhtJXjm1da4olGpbfEIDJnMpns/lImVjr9djaOjSbNCRMlIitWvEXAgavJcFTYYPXwVuhcz0fq0IgqzdBmqpZGt8Yb65PMAhMlIitWvEWAq8oOa15vgzZhhi2QS2RualaXEaUaPOPNHDBRIrJyQ9uGoGmQG9a+0QbNgtykDofokWkLuq9bYY1ScmYe4tNzAQDhHFEyCzzrjcjKDWsXimHtQqUOg8horLnppLYtQIing65onaTFESUiIrIo2mLum3dzUKDWSByNcZ1lo0mzw0SJiIgsiq+zPZS2cqg1AnGpOVKHY1S6Qm6e8WY2mCgREZFFkcvvtwiIsbKCbu0ab0yUzAcTJSIisjjapUysqU4pO78Q1+510WcPJfPBRImIiCxOiBW2CLgQnwEhAC8nJXyc7aUOh+5hokRERBYn2Mv6RpTYkds8MVEiIiKLE+xhfSNKLOQ2T0yUiIjI4miXMbmekg2NRkgcjXGcu1fIzREl88JEiYiILE6Amz1s5TLkF2p0nawtWaFagwvxGQC4xpu5YaJEREQWx7bYOobWMP12LSkLeYUaOCpsdKNlZB6YKBERkUWypqVMtP2Twv1dIJfLJI6GimOiREREFsmamk6ykNt8MVEiIiKLpB1Rik2xhhEltgYwV0yUiIjIImmbTsYkWfaIkhAC527fG1Hy52K45oaJEhERWaRgXXfuLAhhuS0C4tJykZpdAFu5DHX9nKQOhx7ARImIiCxSoLsDZDIgK1+N5Kx8qcOptLO3igq5a/s4QWlrI3E09CAmSkREZJHs7WwQ4KptEWC5dUq6aTfWJ5klJkpERGSxanpYfp3S/UJu1ieZIyZKRERksUK87tUppVhuoqRrDcCO3GaJiRIREVksS286mZqdj1upOQA49WaumCgREZHFCvaw7KaT2vqkIA8VXFV2EkdDpWGiREREFkvXdNJCR5Q47Wb+mCgREZHF0vZSuptdgLScAomjMRwLuc0fEyUiIrJYjkpbeDkpAQCxFjj9xhEl88dEiYiILJpuKRMLm37LLVDjyp1MAEDDGkyUzBUTJSIismiWeubbpYQMqDUC7g528HOxlzocKgMTJSIismj313yzrKm34vVJMplM4mioLEyUiIjIolluolS0xltD9k8ya0yUiIjIommn3iytRklXyM1EyawxUSIiIoumLeZOzMhDdn6hxNFUjFojcP52BgCOKJk7JkpERGTR3BwUuq7WsRay5ltMchZyCtSwt5Mj1MtJ6nCoHEyUiIjI4mnrlGKSLCNR0hZy1/dzgY2chdzmzGISpdmzZ6Nt27ZwcHCAm5tbqfscOXIE3bp1g5ubG9zd3dGrVy+cPHlSb59Tp06hQ4cOsLe3R1BQEObNm1cF0RMRkSnpljJJsYw6JRZyWw6LSZTy8/MxYMAAjBw5stTbMzMz8cQTT6BmzZo4dOgQ9u3bB2dnZ/Tq1QsFBUVt7dPT09GzZ08EBwfj2LFjmD9/PmbMmIHly5dX5Y9CRERGdr/ppGWMKLGQ23LYSh1ARc2cORMAsGrVqlJvv3DhAlJSUvDhhx8iKCgIADB9+nQ0adIE169fR+3atbFmzRrk5+fju+++g0KhQMOGDREVFYVFixbhjTfeqKofhYiIjKymh7ZFgPmPKAkhdIkS13gzfxYzovQw9erVg6enJ7799lvk5+cjJycH3377LcLDwxESEgIAOHjwIDp27AiFQqG7X69evXDx4kXcvXtXosiJiOhRhXhpu3Ob/4hSYkYekrPyIZcB9XydpQ6HHsJqEiVnZ2fs3r0bP/74I1QqFZycnLB161b8/fffsLUtGjiLj4+Hr6+v3v201+Pj40s9bl5eHtLT0/UuRERkXrTF3HGpOcgrVEscTfm09Um1vJ2gUthIHA09jKSJ0uTJkyGTycq9XLhwoULHysnJwauvvop27drhv//+w/79+9GoUSP06dMHOTk5lY5x7ty5cHV11V2003pERGQ+vJ2UcFDYQCOAm3cr/55fFVifZFkkrVGaMGEChg4dWu4+YWFhFTrWTz/9hJiYGBw8eBByuVy3zd3dHZs2bcILL7wAPz8/JCQk6N1Pe93Pz6/U406ZMgXjx4/XXU9PT2eyRERkZmQyGWp6OOBCfAZik7NRy9t8exPdX+ONiZIlkDRR8vb2hre3t1GOlZ2dDblcrrewoPa6RqMBAERERGDq1KkoKCiAnV1Rc7LIyEjUq1cP7u7upR5XqVRCqVQaJUYiIjKdEE9HXIjPMPulTM6ykNuiWEyNUmxsLKKiohAbGwu1Wo2oqChERUUhMzMTANCjRw/cvXsXo0aNwvnz53H27FkMGzYMtra26NKlCwDgpZdegkKhwKuvvoqzZ89i3bp1WLJkid6IERERWSZLWBw3PbdA1z28gT9HlCyBxbQHmDZtGlavXq273rx5cwDArl270LlzZ9SvXx+bN2/GzJkzERERAblcjubNm2Pr1q3w9/cHALi6umL79u0YNWoUWrRoAS8vL0ybNo2tAYiIrIC26aQ5twg4f280KcDVHu6OiofsTebAYhKlVatWldlDSatHjx7o0aNHufs0adIEe/fuNWJkRERkDkIsYETp3G0Wclsai5l6IyIiKk/Ne4nSjbvZUGuExNGU7qzujDfWJ1kKJkpERGQV/F1VUNjIUaAWGPztIaw/cgNp2QVSh6WHZ7xZHiZKRERkFWzkMvyvRQ0AwIGryZj06yk8PvsfvLb6KP44GYfs/EJJ48sv1OBKYgYAFnJbEoupUSIiInqYuc81wZudamHzyTj8cTIOlxIy8c/5BPxzPgEqOxt0b+CLp5sGoGNdLyhtq7Yr9qWEDBSoBVzsbRHorqrSx6bKY6JERERWJdjTEW93rYO3u9bBxfgM/HHyFjafvI3YlGxsPhmHzSfj4GJvi96N/NG3aQAiannCRi57+IEfUfFC7uI9/8i8MVEiIiKrVc/PGRP96uOdnvVw8mYa/oiKw5ZTcUjMyMO6ozew7ugNeDkp8VQTf/Rt6o/HarqbLIk5x0aTFomJEhERWT2ZTIZmQW5oFuSGqX3CcTg6BX+cjMPfZ24jKTMPqw7EYNWBGNRwU6Fv0wA83TQA4f7ORk2atIvhspDbssiEEOZ5DqWZSk9Ph6urK9LS0uDiwhc7EZElyy/UYN+VO9h88ja2n41HVr5ad1stb0c83bQGnm4WgFAvx0d6HI1GoMnM7cjMK8TWsR1Q34+fH1Wtsp/fTJQMxESJiMg65eSrsfNCIjafjMPOi4nIL9TobmtUwwVPNw3AU00CEOBmeCF2TFIWOi/YDYWtHGdn9oKdDU86r2qV/fzm1BsREREAlcIGfZr4o08Tf6TnFmD72QRsPhmHfVeScOZWOs7cSsecvy6gVYgH+jb1x5ON/eHpVLFF07WF3PV8nZkkWRgmSkRERA9wsbdD/xaB6N8iEMmZefjrTDw2n4zD4egUHI4puszYfA7tanvh6aYB6NnQFy72dmUej/VJlouJEhERUTk8nZQY3CYYg9sE43ZaDracvI0/Tsbh9K00/HvpDv69dAeK3+ToUs8bTzetga71faBS6Pdour90CRMlS8NEiYiIqIL8XVV4vWMYXu8YhuikLF1jyyuJmdh2NgHbzibAUWGDHg188XSzALSv7Q2FrbxYawAmSpaGxdwGYjE3EREVJ4TA+dsZ2HyqqJnlzbs5utvcHOzQrb4vfj1+EzIZcGZGLzgqOUYhBRZzExERSUAmk6FBgAsaBLhgUq96OB6bis0n47DlVFGPpl+P3wQAhHo6MkmyQPyNERERGYlMJkOLYHe0CHbHB081wH/XkrH5ZBwOXE3G4IhgqcOjSmCiREREZAI2chna1fZCu9peUodCj4DNHIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwESJiIiIqAy2UgdgaYQQAID09HSJIyEiIqKK0n5uaz/HK4qJkoEyMjIAAEFBQRJHQkRERIbKyMiAq6trhfeXCUNTq2pOo9EgLi4Ozs7OkMlkRj12eno6goKCcOPGDbi4uBj12NaGz1XF8bmqOD5XhuHzVXF8rirOVM+VEAIZGRkICAiAXF7xyiOOKBlILpcjMDDQpI/h4uLCP6QK4nNVcXyuKo7PlWH4fFUcn6uKM8VzZchIkhaLuYmIiIjKwESJiIiIqAxMlMyIUqnE9OnToVQqpQ7F7PG5qjg+VxXH58owfL4qjs9VxZnbc8VibiIiIqIycESJiIiIqAxMlIiIiIjKwESJiIiIqAxMlIiIiIjKwETJTHz55ZcICQmBvb09WrdujcOHD0sdklmaO3cuHn/8cTg7O8PHxwf9+vXDxYsXpQ7LInz88ceQyWQYO3as1KGYpVu3buHll1+Gp6cnVCoVGjdujKNHj0odltlRq9X44IMPEBoaCpVKhVq1amHWrFkGr59ljf7991/07dsXAQEBkMlk+P333/VuF0Jg2rRp8Pf3h0qlQvfu3XH58mVpgjUD5T1fBQUFePfdd9G4cWM4OjoiICAAr7zyCuLi4qo8TiZKZmDdunUYP348pk+fjuPHj6Np06bo1asXEhMTpQ7N7OzZswejRo3Cf//9h8jISBQUFKBnz57IysqSOjSzduTIEXz99ddo0qSJ1KGYpbt376Jdu3aws7PD33//jXPnzmHhwoVwd3eXOjSz88knn+Crr77CF198gfPnz+OTTz7BvHnz8Pnnn0sdmuSysrLQtGlTfPnll6XePm/ePHz22WdYtmwZDh06BEdHR/Tq1Qu5ublVHKl5KO/5ys7OxvHjx/HBBx/g+PHj2LhxIy5evIinn3666gMVJLlWrVqJUaNG6a6r1WoREBAg5s6dK2FUliExMVEAEHv27JE6FLOVkZEh6tSpIyIjI0WnTp3EmDFjpA7J7Lz77ruiffv2UodhEfr06SOGDx+ut+25554TgwYNkigi8wRA/Pbbb7rrGo1G+Pn5ifnz5+u2paamCqVSKX7++WcJIjQvDz5fpTl8+LAAIK5fv141Qd3DESWJ5efn49ixY+jevbtum1wuR/fu3XHw4EEJI7MMaWlpAAAPDw+JIzFfo0aNQp8+ffReY6Tvjz/+QMuWLTFgwAD4+PigefPm+Oabb6QOyyy1bdsWO3bswKVLlwAAJ0+exL59+9C7d2+JIzNv0dHRiI+P1/s7dHV1RevWrfleX0FpaWmQyWRwc3Or0sflorgSS0pKglqthq+vr952X19fXLhwQaKoLINGo8HYsWPRrl07NGrUSOpwzNLatWtx/PhxHDlyROpQzNq1a9fw1VdfYfz48Xjvvfdw5MgRjB49GgqFAkOGDJE6PLMyefJkpKeno379+rCxsYFarcbs2bMxaNAgqUMza/Hx8QBQ6nu99jYqW25uLt599128+OKLVb6oMBMlslijRo3CmTNnsG/fPqlDMUs3btzAmDFjEBkZCXt7e6nDMWsajQYtW7bEnDlzAADNmzfHmTNnsGzZMiZKD1i/fj3WrFmDn376CQ0bNkRUVBTGjh2LgIAAPldkEgUFBRg4cCCEEPjqq6+q/PE59SYxLy8v2NjYICEhQW97QkIC/Pz8JIrK/L399tvYsmULdu3ahcDAQKnDMUvHjh1DYmIiHnvsMdja2sLW1hZ79uzBZ599BltbW6jVaqlDNBv+/v5o0KCB3rbw8HDExsZKFJH5mjhxIiZPnowXXngBjRs3xuDBgzFu3DjMnTtX6tDMmvb9nO/1htEmSdevX0dkZGSVjyYBTJQkp1Ao0KJFC+zYsUO3TaPRYMeOHYiIiJAwMvMkhMDbb7+N3377DTt37kRoaKjUIZmtbt264fTp04iKitJdWrZsiUGDBiEqKgo2NjZSh2g22rVrV6LNxKVLlxAcHCxRROYrOzsbcrn+R4eNjQ00Go1EEVmG0NBQ+Pn56b3Xp6en49ChQ3yvL4M2Sbp8+TL++ecfeHp6ShIHp97MwPjx4zFkyBC0bNkSrVq1wqeffoqsrCwMGzZM6tDMzqhRo/DTTz9h06ZNcHZ21s3tu7q6QqVSSRydeXF2di5Ru+Xo6AhPT0/WdD1g3LhxaNu2LebMmYOBAwfi8OHDWL58OZYvXy51aGanb9++mD17NmrWrImGDRvixIkTWLRoEYYPHy51aJLLzMzElStXdNejo6MRFRUFDw8P1KxZE2PHjsVHH32EOnXqIDQ0FB988AECAgLQr18/6YKWUHnPl7+/P/r374/jx49jy5YtUKvVuvd7Dw8PKBSKqgu0Ss+xozJ9/vnnombNmkKhUIhWrVqJ//77T+qQzBKAUi8rV66UOjSLwPYAZdu8ebNo1KiRUCqVon79+mL58uVSh2SW0tPTxZgxY0TNmjWFvb29CAsLE1OnThV5eXlShya5Xbt2lfr+NGTIECFEUYuADz74QPj6+gqlUim6desmLl68KG3QEirv+YqOji7z/X7Xrl1VGqdMCLZTJSIiIioNa5SIiIiIysBEiYiIiKgMTJSIiIiIysBEiYiIiKgMTJSIiIiIysBEiYiIiKgMTJSIiIiIysBEiYjMWkxMDGQyGaKiokz+WKtWrYKbm5vJH4eILAcTJSKqtKFDh0Imk5W4PPHEE1KH9lAhISH49NNP9bY9//zzuHTpkjQB3dO5c2eMHTtW0hiI6D6u9UZEj+SJJ57AypUr9bYplUqJonk0KpWKawYSkR6OKBHRI1EqlfDz89O7uLu7AwBeeuklPP/883r7FxQUwMvLC99//z0AYOvWrWjfvj3c3Nzg6emJp556ClevXi3z8UqbHvv9998hk8l0169evYpnnnkGvr6+cHJywuOPP45//vlHd3vnzp1x/fp1jBs3TjcKVtaxv/rqK9SqVQsKhQL16tXDDz/8oHe7TCbDihUr8Oyzz8LBwQF16tTBH3/8Ue5ztnTpUtSpUwf29vbw9fVF//79ARSN0O3ZswdLlizRxRUTEwMAOHPmDHr37g0nJyf4+vpi8ODBSEpK0vuZ3n77bbz99ttwdXWFl5cXPvjgA3CVKqJHw0SJiExm0KBB2Lx5MzIzM3Xbtm3bhuzsbDz77LMAgKysLIwfPx5Hjx7Fjh07IJfL8eyzz0Kj0VT6cTMzM/Hkk09ix44dOHHiBJ544gn07dsXsbGxAICNGzciMDAQH374IW7fvo3bt2+XepzffvsNY8aMwYQJE3DmzBmMGDECw4YNw65du/T2mzlzJgYOHIhTp07hySefxKBBg5CSklLqMY8ePYrRo0fjww8/xMWLF7F161Z07NgRALBkyRJERETg9ddf18UVFBSE1NRUdO3aFc2bN8fRo0exdetWJCQkYODAgXrHXr16NWxtbXH48GEsWbIEixYtwooVKyr9PBIRgCpdgpeIrMqQIUOEjY2NcHR01LvMnj1bCCFEQUGB8PLyEt9//73uPi+++KJ4/vnnyzzmnTt3BABx+vRpIYTQrSJ+4sQJIYQQK1euFK6urnr3+e2338TD3s4aNmwoPv/8c9314OBgsXjxYr19Hjx227Ztxeuvv663z4ABA8STTz6puw5AvP/++7rrmZmZAoD4+++/S43j119/FS4uLiI9Pb3U2zt16iTGjBmjt23WrFmiZ8+eettu3LghAOhWn+/UqZMIDw8XGo1Gt8+7774rwsPDS30cIqoYjigR0SPp0qULoqKi9C5vvvkmAMDW1hYDBw7EmjVrABSNHm3atAmDBg3S3f/y5ct48cUXERYWBhcXF4SEhACAbvSnMjIzM/HOO+8gPDwcbm5ucHJywvnz5w0+5vnz59GuXTu9be3atcP58+f1tjVp0kT3f0dHR7i4uCAxMbHUY/bo0QPBwcEICwvD4MGDsWbNGmRnZ5cbx8mTJ7Fr1y44OTnpLvXr1wcAvWnKNm3a6E1BRkRE4PLly1Cr1RX7gYmoBBZzE9EjcXR0RO3atcu8fdCgQejUqRMSExMRGRkJlUqld1Zc3759ERwcjG+++QYBAQHQaDRo1KgR8vPzSz2eXC4vUXdTUFCgd/2dd95BZGQkFixYgNq1a0OlUqF///5lHvNR2dnZ6V2XyWRlTh06Ozvj+PHj2L17N7Zv345p06ZhxowZOHLkSJmtCTIzM9G3b1988sknJW7z9/d/5PiJqGxMlIjIpNq2bYugoCCsW7cOf//9NwYMGKBLLJKTk3Hx4kV888036NChAwBg37595R7P29sbGRkZyMrKgqOjIwCU6LG0f/9+DB06VFcHlZmZqSuK1lIoFA8daQkPD8f+/fsxZMgQvWM3aNDgoT93eWxtbdG9e3d0794d06dPh5ubG3bu3Innnnuu1Lgee+wx/PrrrwgJCYGtbdlv24cOHdK7/t9//6FOnTqwsbF5pHiJqjMmSkT0SPLy8hAfH6+3zdbWFl5eXrrrL730EpYtW4ZLly7pFUK7u7vD09MTy5cvh7+/P2JjYzF58uRyH69169ZwcHDAe++9h9GjR+PQoUNYtWqV3j516tTBxo0b0bdvX8hkMnzwwQclRnhCQkLw77//4oUXXoBSqdSLV2vixIkYOHAgmjdvju7du2Pz5s3YuHGj3hl0htqyZQuuXbuGjh07wt3dHX/99Rc0Gg3q1auni+vQoUOIiYmBk5MTPDw8MGrUKHzzzTd48cUXMWnSJHh4eODKlStYu3YtVqxYoUuEYmNjMX78eIwYMQLHjx/H559/joULF1Y6ViICi7mJqPKGDBkiAJS41KtXT2+/c+fOCQAiODhYr9hYCCEiIyNFeHi4UCqVokmTJmL37t0CgPjtt9+EECWLuYUoKt6uXbu2UKlU4qmnnhLLly/XK+aOjo4WXbp0ESqVSgQFBYkvvviiRJH0wYMHRZMmTYRSqdTdt7RC8aVLl4qwsDBhZ2cn6tatq1eYLoTQi1XL1dVVrFy5stTnbO/evaJTp07C3d1dqFQq0aRJE7Fu3Trd7RcvXhRt2rQRKpVKABDR0dFCCCEuXboknn32WeHm5iZUKpWoX7++GDt2rO757NSpk3jrrbfEm2++KVxcXIS7u7t47733SjzfRGQYmRBsskFEZOk6d+6MZs2aleg2TkSPhme9EREREZWBiRIRERFRGTj1RkRERFQGjigRERERlYGJEhEREVEZmCgRERERlYGJEhEREVEZmCgRERERlYGJEhEREVEZmCgRERERlYGJEhEREVEZmCgRERERleH/ASbOYhrQg0OmAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2wbu_aiXgH",
        "outputId": "71c7d9ee-a0bc-4a9a-81d3-3515602701b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -78.79866129999999 +/- 133.82650038538142\n"
          ]
        }
      ],
      "source": [
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FQZDHQX5lGpv",
        "outputId": "6038b180-a03b-4890-df18-05483d3ee61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.8291962   0.23519397  0.8407947   1.132925    0.         -0.04394993\n",
            "   1.1052301   0.        ]] [[  0.      -58.19413]]\n",
            "[[0.8291962  0.23519397 0.8407947  0.0107825  0.         0.5884097\n",
            "  1.1052301  0.        ]] [[ 1.      28.12511]]\n",
            "[[ 0.8291962   0.21027105  0.8407947  -0.19724701  0.          0.28279153\n",
            "   1.1052301   0.        ]] [[ 1.       30.814932]]\n",
            "[[ 0.8291962   0.16457899  0.8407947  -0.37060493  0.         -0.05205528\n",
            "   1.1052301   0.        ]] [[ 1.       30.720488]]\n",
            "[[ 0.8291962   0.10157932  0.8407947  -0.51506984  0.         -0.38587594\n",
            "   1.1052301   0.        ]] [[ 1.       29.779215]]\n",
            "[[ 0.8291962   0.02415661  0.8407947  -0.6354573   0.         -0.70946825\n",
            "   1.1052301   0.        ]] [[ 1.       28.724741]]\n",
            "[[ 0.8291962  -0.06528528  0.8407947  -0.7357801   0.         -1.0216023\n",
            "   1.1052301   0.        ]] [[ 1.       27.822733]]\n",
            "[[ 0.8291962  -0.16474308  0.8407947  -0.81938255  0.         -1.3239347\n",
            "   1.1052301   0.        ]] [[ 1.       27.127653]]\n",
            "[[ 0.8291962  -0.27254745  0.8407947  -0.8890512   0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       28.625246]]\n",
            "[[ 0.8291962  -0.38730747  0.8407947  -0.94710845  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       43.054733]]\n",
            "[[ 0.8291962  -0.50786376  0.8407947  -0.9954895   0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       51.418102]]\n",
            "[[ 0.8291962 -0.6332503  0.8407947 -1.0358069  0.        -1.5869763\n",
            "   1.1052301  0.       ]] [[ 1.      55.70237]]\n",
            "[[ 0.8291962  -0.76266193  0.8407947  -1.0694048   0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       57.781754]]\n",
            "[[ 0.8291962  -0.89542806  0.8407947  -1.0974032   0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       58.778847]]\n",
            "[[ 0.8291962 -1.0251613  0.8407947 -1.0720888  0.        -1.5869763\n",
            "   1.1052301  0.       ]] [[ 1.       59.157715]]\n",
            "[[ 0.8291962 -1.1083493  0.8407947 -0.6835808  0.        -1.5869763\n",
            "   1.1052301  0.       ]] [[ 1.      58.09391]]\n",
            "[[ 0.8291962  -1.1527498   0.8407947  -0.35982406  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.      55.00145]]\n",
            "[[ 0.8291962  -1.1648272   0.8407947  -0.09002682  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.      46.78988]]\n",
            "[[ 0.8291962  -1.1499689   0.8407947   0.13480423  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       29.680521]]\n",
            "[[ 0.8291962  -1.112664    0.8407947   0.32216343  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[0.5629069 5.404804 ]]\n",
            "[[ 0.8291962  -1.0566535   0.8407947   0.47829613  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[  0.       -18.401152]]\n",
            "[[ 0.8291962  -0.9850552   0.8407947   0.60840666  0.         -1.3870223\n",
            "   1.1052301   0.        ]] [[  0.       -25.110006]]\n",
            "[[ 0.8291962  -0.90046704  0.8407947   0.71683216  0.         -1.1141673\n",
            "   1.1052301   0.        ]] [[  0.     -26.0417]]\n",
            "[[ 0.8291962  -0.80505395  0.8407947   0.8071867   0.         -0.8311883\n",
            "   1.1052301   0.        ]] [[  0.      -25.69055]]\n",
            "[[ 0.8291962  -0.70062     0.8407947   0.8824822   0.         -0.55202496\n",
            "   1.1052301   0.        ]] [[  0.       -25.095037]]\n",
            "[[ 0.8291962  -0.58866894  0.8407947   0.9452284   0.         -0.27933273\n",
            "   1.1052301   0.        ]] [[  0.       -24.551815]]\n",
            "[[ 0.8291962  -0.4704533   0.8407947   0.99751693  0.         -0.01254327\n",
            "   1.1052301   0.        ]] [[  0.       -24.129309]]\n",
            "[[ 0.8291962  -0.34701738  0.8407947   1.0410906   0.          0.24965504\n",
            "   1.1052301   0.        ]] [[  0.       -23.826284]]\n",
            "[[ 0.8291962  -0.21923123  0.8407947   1.0774021   0.          0.50856054\n",
            "   1.1052301   0.        ]] [[  0.       -23.624168]]\n",
            "[[ 0.8291962  -0.08781976  0.8407947   1.1076617   0.          0.7652698\n",
            "   1.1052301   0.        ]] [[  0.       -23.502573]]\n",
            "[[0.8291962 0.0466128 0.8407947 1.1328781 0.        1.0206579 1.1052301\n",
            "  0.       ]] [[  0.       -23.443224]]\n",
            "[[0.8291962  0.18356279 0.8407947  1.1538916  0.         1.2754009\n",
            "  1.1052301  0.        ]] [[  0.      -23.43044]]\n",
            "[[0.8291962  0.32261077 0.8407947  1.1714029  0.         1.4990764\n",
            "  1.1052301  0.        ]] [[  0.       -24.931822]]\n",
            "[[0.8291962  0.46340713 0.8407947  1.1859957  0.         1.4990764\n",
            "  1.1052301  0.        ]] [[  0.       -35.740253]]\n",
            "[[0.8291962 0.6056603 0.8407947 1.1981564 0.        1.4990764 1.1052301\n",
            "  0.       ]] [[  0.       -44.077454]]\n",
            "[[0.8291962 0.7491274 0.8407947 1.2082902 0.        1.4990764 1.1052301\n",
            "  0.       ]] [[  0.       -50.100437]]\n",
            "[[0.8291962  0.89360636 0.8407947  1.216735   0.         1.4990764\n",
            "  1.1052301  0.        ]] [[  0.       -54.112144]]\n",
            "[[0.8291962 1.0213784 0.8407947 1.0772831 0.        1.4990764 1.1052301\n",
            "  0.       ]] [[  0.       -54.650093]]\n",
            "[[0.8291962  1.1029321  0.8407947  0.69150347 0.         1.4990764\n",
            "  1.1052301  0.        ]] [[  0.       -49.064377]]\n",
            "[[ 0.8291962   1.1459707  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -46.05594]]\n",
            "[[ 0.7643759   1.189009   -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -48.414978]]\n",
            "[[ 0.6995556   1.2320474  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -50.446632]]\n",
            "[[ 0.6347352   1.275086   -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -52.174385]]\n",
            "[[ 0.5699149   1.3181243  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -53.62648]]\n",
            "[[ 0.5050946   1.3611629  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.     -54.8339]]\n",
            "[[ 0.4402742   1.4042011  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -55.82836]]\n",
            "[[ 0.3754539   1.4472398  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -56.64074]]\n",
            "[[ 0.31063354  1.4902784  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -57.29981]]\n",
            "[[ 0.2458132   1.5333166  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -57.831497]]\n",
            "[[ 0.18099287  1.5763552  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.258522]]\n",
            "[[ 0.11617254  1.6193937  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.600327]]\n",
            "[[ 0.0513522   1.6624321  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.873253]]\n",
            "[[-0.01346814  1.7054706  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.090843]]\n",
            "[[-0.07828847  1.7485088  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.26416]]\n",
            "[[-0.14310881  1.7915474  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.40218]]\n",
            "[[-0.20792915  1.834586   -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.512123]]\n",
            "[[-0.27274948  1.8776243  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.599773]]\n",
            "[[-0.33756983  1.9206629  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.66973]]\n",
            "[[-0.40239015  1.9637011  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.725647]]\n",
            "[[-0.4672105   2.0067396  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.77043]]\n",
            "[[-0.5320308   2.0497782  -0.9886919   0.37002048  0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.806366]]\n",
            "[[-0.59685117  2.0796852  -0.9886919  -0.3484555   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -58.42575]]\n",
            "[[-0.6616715  2.0366466 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -58.18463]]\n",
            "[[-0.72649187  1.9936084  -0.9886919  -0.3484555   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -57.899963]]\n",
            "[[-0.79131216  1.9505697  -0.9886919  -0.3484555   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -57.563225]]\n",
            "[[-0.8561325  1.9075315 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -57.164207]]\n",
            "[[-0.92095286  1.8644929  -0.9886919  -0.3484555   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -56.690685]]\n",
            "[[-0.9857732  1.8214543 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -56.128067]]\n",
            "[[-1.0505935  1.778416  -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -55.459023]]\n",
            "[[-1.1154139  1.7353776 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -54.663033]]\n",
            "[[-1.1802342  1.692339  -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -53.716064]]\n",
            "[[-1.2450545  1.6493007 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -52.590214]]\n",
            "[[-1.3098749  1.6062621 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -51.25345]]\n",
            "[[-1.3746952  1.5632238 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -49.669754]]\n",
            "[[-1.4395156  1.5201852 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -47.799442]]\n",
            "[[-1.5043359  1.4771466 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -45.60025]]\n",
            "[[-1.5691562  1.4341084 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -43.029106]]\n",
            "[[-1.6339766  1.3910698 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -40.044876]]\n",
            "[[-1.6987969  1.3480313 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -36.612495]]\n",
            "[[-1.7636173  1.304993  -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -32.708015]]\n",
            "[[-1.8284376  1.2619544 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -28.324415]]\n",
            "[[-1.8932579  1.2189162 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.     -23.4778]]\n",
            "[[-1.9580783  1.1758776 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -18.211767]]\n",
            "[[-2.0228987  1.132839  -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -12.599707]]\n",
            "[[-2.087719   1.0898007 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[ 0.       -6.742614]]\n",
            "[[-2.1525393  1.0467621 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[ 0.15873815 -0.7622643 ]]\n",
            "[[-2.2173595  1.0037239 -0.9886919 -0.3484555  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[0.77667516 5.2089396 ]]\n",
            "reset\n",
            "[[ 0.8291962   0.23519397  0.8407947   1.132925    0.         -0.04394993\n",
            "   1.1052301   0.        ]] [[  0.      -58.19413]]\n",
            "[[0.8291962  0.23519397 0.8407947  0.0107825  0.         0.5884097\n",
            "  1.1052301  0.        ]] [[ 1.      28.12511]]\n",
            "[[8.2919621e-01 2.3390949e-01 8.4079468e-01 6.0840001e-05 0.0000000e+00\n",
            "  2.8279153e-01 1.1052301e+00 0.0000000e+00]] [[1.       8.778093]]\n",
            "[[ 0.8291962   0.23155451  0.8407947  -0.00887388  0.          0.18740547\n",
            "   1.1052301   0.        ]] [[0.43936312 3.0718086 ]]\n",
            "[[ 0.8291962   0.22830762  0.8407947  -0.01631947  0.          0.15402596\n",
            "   1.1052301   0.        ]] [[0.30739433 1.8123809 ]]\n",
            "[[ 0.8291962   0.22431731  0.8407947  -0.02252413  0.          0.13433197\n",
            "   1.1052301   0.        ]] [[0.275454  1.5148082]]\n",
            "[[ 0.8291962   0.21970749  0.8407947  -0.02769468  0.          0.11787149\n",
            "   1.1052301   0.        ]] [[0.26497325 1.4235477 ]]\n",
            "[[ 0.8291962   0.21458153  0.8407947  -0.03200348  0.          0.10240269\n",
            "   1.1052301   0.        ]] [[0.25945318 1.3799219 ]]\n",
            "[[ 0.8291962   0.20902546  0.8407947  -0.03559414  0.          0.08740795\n",
            "   1.1052301   0.        ]] [[0.25538638 1.3502076 ]]\n",
            "[[ 0.8291962   0.2031108   0.8407947  -0.03858636  0.          0.07273608\n",
            "   1.1052301   0.        ]] [[0.25197923 1.3267797 ]]\n",
            "[[ 0.8291962   0.19689748  0.8407947  -0.04107987  0.          0.05831876\n",
            "   1.1052301   0.        ]] [[0.24900562 1.3074726 ]]\n",
            "[[ 0.8291962   0.19043519  0.8407947  -0.0431578   0.          0.04411128\n",
            "   1.1052301   0.        ]] [[0.24637294 1.2914023 ]]\n",
            "[[ 0.8291962   0.1837653   0.8407947  -0.04488941  0.          0.03007836\n",
            "   1.1052301   0.        ]] [[0.24401942 1.2779802 ]]\n",
            "[[ 0.8291962   0.17692278  0.8407947  -0.04633242  0.          0.01619136\n",
            "   1.1052301   0.        ]] [[0.24189374 1.2667172 ]]\n",
            "[[ 0.8291962   0.169936    0.8407947  -0.04753492  0.          0.00242666\n",
            "   1.1052301   0.        ]] [[0.23996454 1.2573164 ]]\n",
            "[[ 0.8291962   0.16282925  0.8407947  -0.04853701  0.         -0.01123578\n",
            "   1.1052301   0.        ]] [[0.23819456 1.2494218 ]]\n",
            "[[ 0.8291962   0.15562241  0.8407947  -0.04937208  0.         -0.02481252\n",
            "   1.1052301   0.        ]] [[0.23655862 1.2427945 ]]\n",
            "[[ 0.8291962   0.1483322   0.8407947  -0.05006797  0.         -0.0383171\n",
            "   1.1052301   0.        ]] [[0.23503608 1.2372367 ]]\n",
            "[[ 0.8291962   0.14097255  0.8407947  -0.05064788  0.         -0.05176147\n",
            "   1.1052301   0.        ]] [[0.23360455 1.2325369 ]]\n",
            "[[ 0.8291962   0.133555    0.8407947  -0.05113115  0.         -0.06515463\n",
            "   1.1052301   0.        ]] [[0.23225367 1.228596  ]]\n",
            "[[ 0.8291962   0.12608917  0.8407947  -0.05153386  0.         -0.07850508\n",
            "   1.1052301   0.        ]] [[0.23096704 1.2252573 ]]\n",
            "[[ 0.8291962   0.11858316  0.8407947  -0.05186946  0.         -0.09181918\n",
            "   1.1052301   0.        ]] [[0.22973561 1.2224357 ]]\n",
            "[[ 0.8291962   0.11104366  0.8407947  -0.05214912  0.         -0.10510264\n",
            "   1.1052301   0.        ]] [[0.22854972 1.2200375 ]]\n",
            "[[ 0.8291962   0.10347626  0.8407947  -0.05238218  0.         -0.11835998\n",
            "   1.1052301   0.        ]] [[0.2274029 1.2180026]]\n",
            "[[ 0.8291962   0.09588556  0.8407947  -0.05257639  0.         -0.13159528\n",
            "   1.1052301   0.        ]] [[0.22628829 1.2162647 ]]\n",
            "[[ 0.8291962   0.0882755   0.8407947  -0.05273823  0.         -0.14481176\n",
            "   1.1052301   0.        ]] [[0.22519958 1.2147644 ]]\n",
            "[[ 0.8291962   0.08064918  0.8407947  -0.0528731   0.         -0.1580118\n",
            "   1.1052301   0.        ]] [[0.22413735 1.2135075 ]]\n",
            "[[ 0.8291962   0.07300942  0.8407947  -0.05298549  0.         -0.1711983\n",
            "   1.1052301   0.        ]] [[0.22309119 1.2123938 ]]\n",
            "[[ 0.8291962   0.06535849  0.8407947  -0.05307915  0.         -0.18437259\n",
            "   1.1052301   0.        ]] [[0.22206217 1.2114334 ]]\n",
            "[[ 0.8291962   0.05769821  0.8407947  -0.0531572   0.         -0.19753641\n",
            "   1.1052301   0.        ]] [[0.2210477 1.2106025]]\n",
            "[[ 0.8291962   0.05003021  0.8407947  -0.05322224  0.         -0.21069133\n",
            "   1.1052301   0.        ]] [[0.22004232 1.2098478 ]]\n",
            "[[ 0.8291962   0.04235565  0.8407947  -0.05327644  0.         -0.22383805\n",
            "   1.1052301   0.        ]] [[0.2190502 1.2092109]]\n",
            "[[ 0.8291962   0.03467567  0.8407947  -0.05332161  0.         -0.23697771\n",
            "   1.1052301   0.        ]] [[0.21806675 1.2086456 ]]\n",
            "[[ 0.8291962   0.0269911   0.8407947  -0.05335925  0.         -0.25011143\n",
            "   1.1052301   0.        ]] [[0.21709004 1.2081356 ]]\n",
            "[[ 0.8291962   0.01930292  0.8407947  -0.05339061  0.         -0.2632394\n",
            "   1.1052301   0.        ]] [[0.21611893 1.207669  ]]\n",
            "[[ 0.8291962   0.01161146  0.8407947  -0.05341675  0.         -0.2763624\n",
            "   1.1052301   0.        ]] [[0.21515587 1.2072692 ]]\n",
            "[[ 0.8291962   0.00391754  0.8407947  -0.05343854  0.         -0.2894811\n",
            "   1.1052301   0.        ]] [[0.21419391 1.2068696 ]]\n",
            "[[ 0.8291962  -0.00377869  0.8407947  -0.05345669  0.         -0.30259544\n",
            "   1.1052301   0.        ]] [[0.2132405 1.2065413]]\n",
            "[[ 0.8291962  -0.01147671  0.8407947  -0.05347181  0.         -0.31570607\n",
            "   1.1052301   0.        ]] [[0.21229026 1.2062342 ]]\n",
            "[[ 0.8291962  -0.01917605  0.8407947  -0.05348442  0.         -0.3288135\n",
            "   1.1052301   0.        ]] [[0.2113393 1.2059085]]\n",
            "[[ 0.8291962  -0.02687687  0.8407947  -0.05349492  0.         -0.34191737\n",
            "   1.1052301   0.        ]] [[0.21039751 1.2056612 ]]\n",
            "[[ 0.8291962  -0.03457867  0.8407947  -0.05350368  0.         -0.3550186\n",
            "   1.1052301   0.        ]] [[0.20945546 1.2054005 ]]\n",
            "[[ 0.8291962  -0.0422813   0.8407947  -0.05351098  0.         -0.36811686\n",
            "   1.1052301   0.        ]] [[0.20851693 1.2051629 ]]\n",
            "[[ 0.8291962  -0.04998474  0.8407947  -0.05351705  0.         -0.38121262\n",
            "   1.1052301   0.        ]] [[0.2075817 1.2049468]]\n",
            "[[ 0.8291962  -0.05768868  0.8407947  -0.05352212  0.         -0.394306\n",
            "   1.1052301   0.        ]] [[0.2066468 1.204723 ]]\n",
            "[[ 0.8291962  -0.06539327 -0.9886919  -0.05352634  0.         -0.407397\n",
            "  -0.904789    0.        ]] [[ 0.       -7.072678]]\n",
            "[[ 0.7643759  -0.07309769 -0.9886919  -0.05352634  0.         -0.33054268\n",
            "  -0.904789    0.        ]] [[ 0.14889175 -0.6764334 ]]\n",
            "[[ 0.6995556  -0.08080228 -0.9886919  -0.05352634  0.         -0.3231922\n",
            "  -0.904789    0.        ]] [[0.2558803  0.36497718]]\n",
            "[[ 0.6347352  -0.08850671 -0.9886919  -0.05352634  0.         -0.3271582\n",
            "  -0.904789    0.        ]] [[0.26882583 0.51701504]]\n",
            "[[ 0.5699149  -0.09621131 -0.9886919  -0.05352634  0.         -0.3327763\n",
            "  -0.904789    0.        ]] [[0.26734513 0.53290373]]\n",
            "[[ 0.5050946  -0.10391574 -0.9886919  -0.05352634  0.         -0.338567\n",
            "  -0.904789    0.        ]] [[0.26375934 0.52927727]]\n",
            "[[ 0.4402742  -0.11162032 -0.9886919  -0.05352634  0.         -0.3443184\n",
            "  -0.904789    0.        ]] [[0.2598802 0.5232871]]\n",
            "[[ 0.3754539  -0.11932475 -0.9886919  -0.05352634  0.         -0.35000467\n",
            "  -0.904789    0.        ]] [[0.2559527 0.517248 ]]\n",
            "[[ 0.31063354 -0.12702934 -0.9886919  -0.05352634  0.         -0.35562515\n",
            "  -0.904789    0.        ]] [[0.25201973 0.51155704]]\n",
            "[[ 0.2458132  -0.13473393 -0.9886919  -0.05352634  0.         -0.36118397\n",
            "  -0.904789    0.        ]] [[0.2480774 0.5061682]]\n",
            "[[ 0.18099287 -0.14243837 -0.9886919  -0.05352634  0.         -0.3666843\n",
            "  -0.904789    0.        ]] [[0.24413186 0.5011273 ]]\n",
            "[[ 0.11617254 -0.15014295 -0.9886919  -0.05352634  0.         -0.37212968\n",
            "  -0.904789    0.        ]] [[0.2401948 0.49653  ]]\n",
            "[[ 0.0513522  -0.15784739 -0.9886919  -0.05352634  0.         -0.3775252\n",
            "  -0.904789    0.        ]] [[0.23625603 0.4922641 ]]\n",
            "[[-0.01346814 -0.16555198 -0.9886919  -0.05352634  0.         -0.38287428\n",
            "  -0.904789    0.        ]] [[0.23233154 0.48846346]]\n",
            "[[-0.07828847 -0.1732564  -0.9886919  -0.05352634  0.         -0.38818213\n",
            "  -0.904789    0.        ]] [[0.22841164 0.4850157 ]]\n",
            "[[-0.14310881 -0.180961   -0.9886919  -0.05352634  0.         -0.39345244\n",
            "  -0.904789    0.        ]] [[0.2245138  0.48206538]]\n",
            "[[-0.20792915 -0.18866542 -0.9886919  -0.05352634  0.         -0.39869076\n",
            "  -0.904789    0.        ]] [[0.22062638 0.4794777 ]]\n",
            "[[-0.27274948 -0.19637002 -0.9886919  -0.05352634  0.         -0.40390098\n",
            "  -0.904789    0.        ]] [[0.2167668 0.477395 ]]\n",
            "[[-0.33756983 -0.20407444 -0.9886919  -0.05352634  0.         -0.40908852\n",
            "  -0.904789    0.        ]] [[0.2129267 0.4757113]]\n",
            "[[-0.40239015 -0.21177903 -0.9886919  -0.05352634  0.         -0.41425774\n",
            "  -0.904789    0.        ]] [[0.20912042 0.47453648]]\n",
            "[[-0.4672105  -0.21948346 -0.9886919  -0.05352634  0.         -0.41941422\n",
            "  -0.904789    0.        ]] [[0.20533895 0.47375864]]\n",
            "[[-0.5320308  -0.22718805 -0.9886919  -0.05352634  0.         -0.42456236\n",
            "  -0.904789    0.        ]] [[0.20159638 0.47348315]]\n",
            "[[-0.59685117 -0.23489249 -0.9886919  -0.05352634  0.         -0.42970732\n",
            "  -0.904789    0.        ]] [[0.19788927 0.47365195]]\n",
            "[[-0.6616715  -0.24259707 -0.9886919  -0.05352634  0.         -0.43485424\n",
            "  -0.904789    0.        ]] [[0.19422337 0.47429258]]\n",
            "[[-0.72649187 -0.2503015  -0.9886919  -0.05352634  0.         -0.4400081\n",
            "  -0.904789    0.        ]] [[0.19059667 0.47535878]]\n",
            "[[-0.79131216 -0.2580061  -0.9886919  -0.05352634  0.         -0.44517347\n",
            "  -0.904789    0.        ]] [[0.18702021 0.47693163]]\n",
            "[[-0.8561325  -0.26571053 -0.9886919  -0.05352634  0.         -0.4503561\n",
            "  -0.904789    0.        ]] [[0.18348297 0.47888124]]\n",
            "[[-0.92095286 -0.27341512 -0.9886919  -0.05352634  0.         -0.4555598\n",
            "  -0.904789    0.        ]] [[0.18000215 0.48135006]]\n",
            "[[-0.9857732  -0.28111956 -0.9886919  -0.05352634  0.         -0.4607903\n",
            "  -0.904789    0.        ]] [[0.17656448 0.48418963]]\n",
            "[[-1.0505935  -0.28882414 -0.9886919  -0.05352634  0.         -0.46605173\n",
            "  -0.904789    0.        ]] [[0.17318282 0.48750335]]\n",
            "[[-1.1154139  -0.29652855 -0.9886919  -0.05352634  0.         -0.4713491\n",
            "  -0.904789    0.        ]] [[0.16984762 0.49118203]]\n",
            "[[-1.1802342  -0.30423316 -0.9886919  -0.05352634  0.         -0.47668645\n",
            "  -0.904789    0.        ]] [[0.1665709  0.49532455]]\n",
            "[[-1.2450545  -0.31193757 -0.9886919  -0.05352634  0.         -0.48206884\n",
            "  -0.904789    0.        ]] [[0.16333842 0.49978   ]]\n",
            "[[-1.3098749  -0.3196422  -0.9886919  -0.05352634  0.         -0.48749962\n",
            "  -0.904789    0.        ]] [[0.16016637 0.5046905 ]]\n",
            "[[-1.3746952  -0.3273466  -0.9886919  -0.05352634  0.         -0.49298382\n",
            "  -0.904789    0.        ]] [[0.15703748 0.5098787 ]]\n",
            "[[-1.4395156  -0.33505118 -0.9886919  -0.05352634  0.         -0.49852434\n",
            "  -0.904789    0.        ]] [[0.15396777 0.51548797]]\n",
            "[[-1.5043359  -0.34275562 -0.9886919  -0.05352634  0.         -0.50412583\n",
            "  -0.904789    0.        ]] [[0.15094142 0.5213572 ]]\n",
            "[[-1.5691562  -0.3504602  -0.9886919  -0.05352634  0.         -0.5097911\n",
            "  -0.904789    0.        ]] [[0.14797038 0.5275926 ]]\n",
            "[[-1.6339766  -0.35816464 -0.9886919  -0.05352634  0.         -0.51552415\n",
            "  -0.904789    0.        ]] [[0.14504066 0.53405243]]\n",
            "[[-1.6987969  -0.36586922 -0.9886919  -0.05352634  0.         -0.5213274\n",
            "  -0.904789    0.        ]] [[0.14216505 0.54085135]]\n",
            "[[-1.7636173  -0.37357366 -0.9886919  -0.05352634  0.         -0.52720445\n",
            "  -0.904789    0.        ]] [[0.13932848 0.54783887]]\n",
            "[[-1.8284376  -0.38127825 -0.9886919  -0.05352634  0.         -0.53315747\n",
            "  -0.904789    0.        ]] [[0.13654226 0.55511564]]\n",
            "[[-1.8932579  -0.38898268 -0.9886919  -0.05352634  0.         -0.5391896\n",
            "  -0.904789    0.        ]] [[0.13379245 0.56254256]]\n",
            "[[-1.9580783  -0.39668727 -0.9886919  -0.05352634  0.         -0.5453024\n",
            "  -0.904789    0.        ]] [[0.13109039 0.57022053]]\n",
            "[[-2.0228987  -0.4043917  -0.9886919  -0.05352634  0.         -0.5514986\n",
            "  -0.904789    0.        ]] [[0.12842324 0.57802033]]\n",
            "[[-2.087719   -0.4120963  -0.9886919  -0.05352634  0.         -0.5577796\n",
            "  -0.904789    0.        ]] [[0.12579958 0.5860165 ]]\n",
            "[[-2.142168   -0.41980073  0.62198466  0.7637885   0.         -0.5641475\n",
            "  -0.904789    0.        ]] [[  0.       -53.310028]]\n",
            "[[-2.0928528  -0.329587    0.62198466  0.7637885   0.          0.01513974\n",
            "  -0.904789    0.        ]] [[  0.       -41.124138]]\n",
            "[[-2.0435379  -0.23937315  0.62198466  0.7637885   0.          0.46201032\n",
            "  -0.904789    0.        ]] [[  0.      -26.33338]]\n",
            "[[-1.9942228  -0.14915945  0.62198466  0.7637885   0.          0.748159\n",
            "  -0.904789    0.        ]] [[  0.      -18.16591]]\n",
            "[[-1.9449077  -0.05894559  0.62198466  0.7637885   0.          0.9455565\n",
            "  -0.904789    0.        ]] [[  0.      -15.28641]]\n",
            "[[-1.8955927   0.03126811  0.62198466  0.7637885   0.          1.1116647\n",
            "  -0.904789    0.        ]] [[  0.       -14.407489]]\n",
            "[[-1.8462776   0.12148198  0.62198466  0.7637885   0.          1.268222\n",
            "  -0.904789    0.        ]] [[  0.       -14.158705]]\n",
            "[[-1.7969625   0.21169569  0.62198466  0.7637885   0.          1.4220757\n",
            "  -0.904789    0.        ]] [[  0.       -14.100868]]\n",
            "[[-1.7476474   0.30190954  0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -18.696203]]\n",
            "[[-1.6983323   0.39212325  0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -27.308119]]\n",
            "[[-1.6490173   0.48233712  0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -34.834946]]\n",
            "[[-1.5997022   0.57255083  0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -41.115562]]\n",
            "[[-1.5503871   0.66276467  0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -46.136467]]\n",
            "[[-1.501072    0.7529784   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -50.000504]]\n",
            "[[-1.4517571   0.8431921   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -52.879982]]\n",
            "[[-1.4024419   0.9334061   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -54.97117]]\n",
            "[[-1.3531269   1.0236198   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.     -56.4609]]\n",
            "[[-1.3038118   1.1138335   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -57.508312]]\n",
            "[[-1.2544967   1.2040472   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.239048]]\n",
            "[[-1.2051816   1.2942612   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.747147]]\n",
            "[[-1.1558666   1.384475    0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.100475]]\n",
            "[[-1.1065514   1.4746886   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.346863]]\n",
            "[[-1.0572364   1.5649023   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.519478]]\n",
            "[[-1.0079213   1.6551163   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.641144]]\n",
            "[[-0.9586063   1.7453301   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.727512]]\n",
            "[[-0.9092912   1.8355438   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.78929]]\n",
            "[[-0.8599762   1.9257578   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.833847]]\n",
            "[[-0.81066114  2.0159714   0.62198466  0.7637885   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.866257]]\n",
            "[[-0.76134604  2.0796852   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -50.809258]]\n",
            "[[-0.712031    1.9894714   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -46.73616]]\n",
            "[[-0.6627158   1.8992574   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -41.175358]]\n",
            "[[-0.61340076  1.8090438   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -33.911263]]\n",
            "[[-0.5640857   1.7188301   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -24.917532]]\n",
            "[[-0.5147707   1.6286163   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -14.436913]]\n",
            "[[-0.46545562  1.5384023   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[ 0.       -2.993232]]\n",
            "[[-0.4161406   1.4481887   0.62198466 -0.7422235   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[1.       8.679007]]\n",
            "[[-0.36682537  1.3579749   0.62198466 -0.7422235   0.          1.404767\n",
            "  -0.904789    0.        ]] [[ 1.        13.4736805]]\n",
            "[[-0.31751034  1.2677612   0.62198466 -0.7422235   0.          1.2583568\n",
            "  -0.904789    0.        ]] [[ 1.       14.577791]]\n",
            "[[-0.26819527  1.1775472   0.62198466 -0.7422235   0.          1.0999491\n",
            "  -0.904789    0.        ]] [[ 1.       14.787649]]\n",
            "[[-0.21888024  1.0873336   0.62198466 -0.7422235   0.          0.93926084\n",
            "  -0.904789    0.        ]] [[ 1.       14.767397]]\n",
            "[[-0.16956519  0.9971198   0.62198466 -0.7422235   0.          0.77879256\n",
            "  -0.904789    0.        ]] [[ 1.       14.692986]]\n",
            "[[-0.12025014  0.9069061   0.62198466 -0.7422235   0.          0.6191329\n",
            "  -0.904789    0.        ]] [[ 1.       14.611633]]\n",
            "[[-0.07093494  0.81669205  0.62198466 -0.7422235   0.          0.46035728\n",
            "  -0.904789    0.        ]] [[ 1.        14.5365305]]\n",
            "[[-0.02161989  0.7264784   0.62198466 -0.7422235   0.          0.30239785\n",
            "  -0.904789    0.        ]] [[ 1.       14.471601]]\n",
            "[[ 0.02769515  0.6362647   0.62198466 -0.7422235   0.          0.14514384\n",
            "  -0.904789    0.        ]] [[ 1.      14.41849]]\n",
            "[[ 0.0770102   0.5460508   0.62198466 -0.7422235   0.         -0.0115329\n",
            "  -0.904789    0.        ]] [[ 1.       14.377808]]\n",
            "[[ 0.12632525  0.4558371   0.62198466 -0.7422235   0.         -0.16776766\n",
            "  -0.904789    0.        ]] [[ 1.      14.34943]]\n",
            "[[ 0.17564029  0.36562324  0.62198466 -0.7422235   0.         -0.32369414\n",
            "  -0.904789    0.        ]] [[ 1.       14.332992]]\n",
            "[[ 0.22495534  0.27540955  0.62198466 -0.7422235   0.         -0.4794419\n",
            "  -0.904789    0.        ]] [[ 1.       14.327566]]\n",
            "[[ 0.27427053  0.18519568  0.62198466 -0.7422235   0.         -0.6351307\n",
            "  -0.904789    0.        ]] [[ 1.       14.332083]]\n",
            "[[ 0.3235856   0.09498198  0.62198466 -0.7422235   0.         -0.79086864\n",
            "  -0.904789    0.        ]] [[ 1.     14.3451]]\n",
            "[[ 0.37290064  0.00476811  0.62198466 -0.7422235   0.         -0.94674796\n",
            "  -0.904789    0.        ]] [[ 1.       14.365259]]\n",
            "[[ 0.42221567 -0.08544559  0.62198466 -0.7422235   0.         -1.1028464\n",
            "  -0.904789    0.        ]] [[ 1.       14.390995]]\n",
            "[[ 0.47153074 -0.17565945  0.62198466 -0.7422235   0.         -1.2592244\n",
            "  -0.904789    0.        ]] [[ 1.       14.420951]]\n",
            "[[ 0.5208458  -0.26587316  0.62198466 -0.7422235   0.         -1.415928\n",
            "  -0.904789    0.        ]] [[ 1.       14.453722]]\n",
            "[[ 0.5701608  -0.35608703  0.62198466 -0.7422235   0.         -1.5729877\n",
            "  -0.904789    0.        ]] [[ 1.      14.48818]]\n",
            "[[ 0.61947584 -0.4463009   0.62198466 -0.7422235   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       24.667574]]\n",
            "[[ 0.66879094 -0.5365146   0.62198466 -0.7422235   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       34.256588]]\n",
            "[[ 0.718106   -0.6267285   0.62198466 -0.7422235   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       41.896034]]\n",
            "[[ 0.7674213  -0.71694213  0.62198466 -0.7422235   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.     47.6218]]\n",
            "[[ 0.81673634 -0.8071559   0.62198466 -0.7422235   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       51.696655]]\n",
            "[[ 0.8291962  -0.89736974 -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       53.413258]]\n",
            "[[ 0.7711615  -0.98908746 -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       55.468987]]\n",
            "[[ 0.71312636 -1.0808053  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       56.884766]]\n",
            "[[ 0.6550916  -1.172523   -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       57.841118]]\n",
            "[[ 0.5970568  -1.2642409  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.479153]]\n",
            "[[ 0.53902173 -1.3559586  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.902073]]\n",
            "[[ 0.48098695 -1.4476764  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.181915]]\n",
            "[[ 0.42295218 -1.5393941  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.367443]]\n",
            "[[ 0.36491725 -1.631112   -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.491005]]\n",
            "[[ 0.30688232 -1.7228297  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.573853]]\n",
            "[[ 0.24884754 -1.8145475  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.629845]]\n",
            "[[ 0.19081262 -1.9062653  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      59.66804]]\n",
            "[[ 0.13277769 -1.9979831  -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      59.69436]]\n",
            "[[ 0.07474291 -2.089701   -0.8929357  -0.75477725  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.712677]]\n",
            "[[ 0.01670798 -2.0932746  -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       51.423103]]\n",
            "[[-0.04132695 -2.0015569  -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.     47.2013]]\n",
            "[[-0.09936173 -1.909839   -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      41.09101]]\n",
            "[[-0.15739666 -1.8181212  -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      32.62486]]\n",
            "[[-0.21543159 -1.7264035  -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       21.603647]]\n",
            "[[-0.27346638 -1.6346856  -0.8929357   0.7763422   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[0.90867764 8.400511  ]]\n",
            "[[-0.3315013 -1.5429679 -0.8929357  0.7763422  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 0.        -5.8835526]]\n",
            "[[-0.38953608 -1.4512501  -0.8929357   0.7763422   0.         -1.5230434\n",
            "  -0.904789    0.        ]] [[  0.        -14.5280075]]\n",
            "[[-0.447571  -1.3595324 -0.8929357  0.7763422  0.        -1.3651764\n",
            "  -0.904789   0.       ]] [[  0.       -15.175485]]\n",
            "[[-0.50560594 -1.2678145  -0.8929357   0.7763422   0.         -1.2002739\n",
            "  -0.904789    0.        ]] [[  0.       -15.194045]]\n",
            "[[-0.5636407 -1.1760968 -0.8929357  0.7763422  0.        -1.0351695\n",
            "  -0.904789   0.       ]] [[  0.   -15.15]]\n",
            "[[-0.62167567 -1.084379   -0.8929357   0.7763422   0.         -0.8705438\n",
            "  -0.904789    0.        ]] [[  0.       -15.105215]]\n",
            "[[-0.67971057 -0.9926612  -0.8929357   0.7763422   0.         -0.7064048\n",
            "  -0.904789    0.        ]] [[  0.       -15.067238]]\n",
            "[[-0.73774534 -0.90094346 -0.8929357   0.7763422   0.         -0.5426784\n",
            "  -0.904789    0.        ]] [[  0.       -15.038115]]\n",
            "[[-0.7957803  -0.8092257  -0.8929357   0.7763422   0.         -0.37926844\n",
            "  -0.904789    0.        ]] [[  0.       -15.019208]]\n",
            "[[-0.8538152  -0.71750784 -0.8929357   0.7763422   0.         -0.21606404\n",
            "  -0.904789    0.        ]] [[  0.       -15.011398]]\n",
            "[[-0.9118501  -0.62579006 -0.8929357   0.7763422   0.         -0.05294441\n",
            "  -0.904789    0.        ]] [[  0.       -15.015095]]\n",
            "[[-0.96988493 -0.53407234 -0.8929357   0.7763422   0.          0.11021528\n",
            "  -0.904789    0.        ]] [[  0.       -15.030278]]\n",
            "[[-1.0279198  -0.44235456 -0.8929357   0.7763422   0.          0.2735401\n",
            "  -0.904789    0.        ]] [[  0.       -15.056365]]\n",
            "[[-1.0859547  -0.35063678 -0.8929357   0.7763422   0.          0.4371483\n",
            "  -0.904789    0.        ]] [[  0.       -15.092312]]\n",
            "[[-1.1439896  -0.25891903 -0.8929357   0.7763422   0.          0.6011471\n",
            "  -0.904789    0.        ]] [[  0.       -15.136592]]\n",
            "[[-1.2020245  -0.16720127 -0.8929357   0.7763422   0.          0.76562715\n",
            "  -0.904789    0.        ]] [[  0.       -15.187416]]\n",
            "[[-1.2600592  -0.07548334 -0.8929357   0.7763422   0.          0.93065953\n",
            "  -0.904789    0.        ]] [[  0.       -15.242787]]\n",
            "[[-1.3180943   0.01623441 -0.8929357   0.7763422   0.          1.0962933\n",
            "  -0.904789    0.        ]] [[  0.        -15.3005705]]\n",
            "[[-1.376129    0.10795217 -0.8929357   0.7763422   0.          1.2625551\n",
            "  -0.904789    0.        ]] [[  0.       -15.358749]]\n",
            "[[-1.4341639   0.19966993 -0.8929357   0.7763422   0.          1.4294493\n",
            "  -0.904789    0.        ]] [[  0.       -15.415443]]\n",
            "[[-1.4921987   0.29138768 -0.8929357   0.7763422   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -22.775776]]\n",
            "[[-1.5502336   0.38310546 -0.8929357   0.7763422   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -33.577637]]\n",
            "[[-1.6082685  0.4748232 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -41.90029]]\n",
            "[[-1.6663034   0.56654096 -0.8929357   0.7763422   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -47.91342]]\n",
            "[[-1.7243383  0.6582589 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -52.056526]]\n",
            "[[-1.7823732  0.7499765 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -54.819786]]\n",
            "[[-1.8404081  0.8416944 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -56.62578]]\n",
            "[[-1.8984429  0.933412  -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -57.79335]]\n",
            "[[-1.9564778  1.0251299 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -58.54516]]\n",
            "[[-2.0145128  1.1168479 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -59.029648]]\n",
            "[[-2.0725474  1.2085655 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -59.343178]]\n",
            "[[-2.1305823  1.3002834 -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.     -59.5474]]\n",
            "[[-2.1886172  1.392001  -0.8929357  0.7763422  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.68152]]\n",
            "reset\n",
            "[[ 0.8291962   0.23519397  0.8407947   1.132925    0.         -0.04394993\n",
            "   1.1052301   0.        ]] [[  0.      -58.19413]]\n",
            "[[0.8291962  0.23519397 0.8407947  0.0107825  0.         0.5884097\n",
            "  1.1052301  0.        ]] [[ 1.      28.12511]]\n",
            "[[ 0.8291962   0.16945171  0.8407947  -0.53796226  0.          0.28279153\n",
            "   1.1052301   0.        ]] [[ 1.      52.68879]]\n",
            "[[ 0.8291962   0.04892411  0.8407947  -0.99524957  0.         -0.28974506\n",
            "   1.1052301   0.        ]] [[ 1.      56.61498]]\n",
            "[[ 0.8291962 -0.1172578  0.8407947 -1.3763223  0.        -0.904945\n",
            "   1.1052301  0.       ]] [[ 1.       57.838696]]\n",
            "[[ 0.8291962  -0.32148495  0.8407947  -1.6938828   0.         -1.5334424\n",
            "   1.1052301   0.        ]] [[ 1.       58.330746]]\n",
            "[[ 0.8291962 -0.5468754  0.8407947 -1.8705302  0.        -1.5869763\n",
            "   1.1052301  0.       ]] [[ 1.       59.389984]]\n",
            "[[ 0.8291962 -0.6689585  0.8407947 -1.0082334  0.        -1.5869763\n",
            "   1.1052301  0.       ]] [[ 1.       55.948784]]\n",
            "[[ 0.8291962  -0.704952    0.8407947  -0.28965273  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.     20.1065]]\n",
            "[[ 0.8291962  -0.6692045   0.8407947   0.30916455  0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[  0.       -41.172585]]\n",
            "[[ 0.8291962  -0.5736725   0.8407947   0.80817896  0.         -1.1395792\n",
            "   1.1052301   0.        ]] [[  0.      -52.44599]]\n",
            "[[ 0.8291962 -0.4283203  0.8407947  1.2240243  0.        -0.569681\n",
            "   1.1052301  0.       ]] [[  0.       -55.150646]]\n",
            "[[ 0.8291962  -0.2414511   0.8407947   1.5705621   0.          0.02960707\n",
            "   1.1052301   0.        ]] [[  0.      -56.01208]]\n",
            "[[ 0.8291962  -0.01998446  0.8407947   1.8593435   0.          0.6382557\n",
            "   1.1052301   0.        ]] [[  0.       -56.260742]]\n",
            "[[0.8291962  0.23031336 0.8407947  2.0999947  0.         1.2496065\n",
            "  1.1052301  0.        ]] [[  0.       -56.298477]]\n",
            "[[0.8291962  0.46863416 0.8407947  2.0000238  0.         1.4990764\n",
            "  1.1052301  0.        ]] [[  0.      -56.29003]]\n",
            "[[0.8291962 0.6014926 0.8407947 1.1197389 0.        1.4990764 1.1052301\n",
            "  0.       ]] [[  0.       -40.873287]]\n",
            "[[0.8291962  0.6464657  0.8407947  0.38616815 0.         1.4990764\n",
            "  1.1052301  0.        ]] [[0.70002645 4.7825265 ]]\n",
            "[[ 0.8291962   0.61820084  0.8407947  -0.22514087  0.          1.4471077\n",
            "   1.1052301   0.        ]] [[ 1.      49.94024]]\n",
            "[[ 0.8291962   0.52890456  0.8407947  -0.734565    0.          0.904438\n",
            "   1.1052301   0.        ]] [[ 1.       56.321365]]\n",
            "[[ 0.8291962   0.38874868  0.8407947  -1.1590852   0.          0.29242843\n",
            "   1.1052301   0.        ]] [[ 1.       58.036583]]\n",
            "[[ 0.8291962   0.20620984  0.8407947  -1.512852    0.         -0.3382193\n",
            "   1.1052301   0.        ]] [[ 1.      58.64755]]\n",
            "[[ 0.8291962  -0.01164804  0.8407947  -1.8076576   0.         -0.97550607\n",
            "   1.1052301   0.        ]] [[ 1.       58.897198]]\n",
            "[[ 0.8291962  -0.25893873  0.8407947  -2.053329    0.         -1.5869763\n",
            "   1.1052301   0.        ]] [[ 1.       59.057755]]\n",
            "[[ 0.8291962 -0.4947536 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.      58.86252]]\n",
            "[[ 0.7643759 -0.7305683 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.406933]]\n",
            "[[ 0.6995556  -0.96638316 -0.9886919  -1.9575416   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.615704]]\n",
            "[[ 0.6347352 -1.2021979 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.      59.69816]]\n",
            "[[ 0.5699149 -1.4380128 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.732265]]\n",
            "[[ 0.5050946 -1.6738275 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.747173]]\n",
            "[[ 0.4402742 -1.9096423 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.      59.75409]]\n",
            "[[ 0.3754539 -2.0932746 -0.9886919  1.9791067  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[  0.       -45.689995]]\n",
            "[[ 0.31063354 -1.8574598  -0.9886919   1.9791067   0.         -1.0904913\n",
            "  -0.904789    0.        ]] [[  0.       -42.845524]]\n",
            "[[ 0.2458132 -1.621645  -0.9886919  1.9791067  0.        -0.6249155\n",
            "  -0.904789   0.       ]] [[  0.       -40.568176]]\n",
            "[[ 0.18099287 -1.3858302  -0.9886919   1.9791067   0.         -0.18408607\n",
            "  -0.904789    0.        ]] [[  0.       -38.995197]]\n",
            "[[ 0.11617254 -1.1500152  -0.9886919   1.9791067   0.          0.23965059\n",
            "  -0.904789    0.        ]] [[  0.       -38.041355]]\n",
            "[[ 0.0513522  -0.9142005  -0.9886919   1.9791067   0.          0.65302265\n",
            "  -0.904789    0.        ]] [[  0.       -37.544647]]\n",
            "[[-0.01346814 -0.67838573 -0.9886919   1.9791067   0.          1.060997\n",
            "  -0.904789    0.        ]] [[  0.       -37.359085]]\n",
            "[[-0.07828847 -0.44257084 -0.9886919   1.9791067   0.          1.4669551\n",
            "  -0.904789    0.        ]] [[  0.      -37.37356]]\n",
            "[[-0.14310881 -0.20675598 -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -49.248894]]\n",
            "[[-0.20792915  0.02905871 -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -55.60607]]\n",
            "[[-0.27274948  0.26487356 -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -58.254402]]\n",
            "[[-0.33756983  0.50068843 -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.289917]]\n",
            "[[-0.40239015  0.7365031  -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.691113]]\n",
            "[[-0.4672105  0.972318  -0.9886919  1.9791067  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.84965]]\n",
            "[[-0.5320308  1.2081329 -0.9886919  1.9791067  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.91448]]\n",
            "[[-0.59685117  1.4439477  -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -59.942127]]\n",
            "[[-0.6616715  1.6797626 -0.9886919  1.9791067  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.95448]]\n",
            "[[-0.72649187  1.9155772  -0.9886919   1.9791067   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -59.96028]]\n",
            "[[-0.79131216  2.0796852  -0.9886919  -1.9575416   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[ 1.       45.176105]]\n",
            "[[-0.8561325  1.8438703 -0.9886919 -1.9575416  0.         1.0081756\n",
            "  -0.904789   0.       ]] [[ 1.       41.365273]]\n",
            "[[-0.92095286  1.6080555  -0.9886919  -1.9575416   0.          0.55868465\n",
            "  -0.904789    0.        ]] [[ 1.     38.2218]]\n",
            "[[-0.9857732   1.3722407  -0.9886919  -1.9575416   0.          0.14335191\n",
            "  -0.904789    0.        ]] [[ 1.       36.108078]]\n",
            "[[-1.0505935   1.1364261  -0.9886919  -1.9575416   0.         -0.24901219\n",
            "  -0.904789    0.        ]] [[ 1.       34.881058]]\n",
            "[[-1.1154139  0.9006112 -0.9886919 -1.9575416  0.        -0.6280431\n",
            "  -0.904789   0.       ]] [[ 1.      34.23517]]\n",
            "[[-1.1802342   0.66479635 -0.9886919  -1.9575416   0.         -1.0000556\n",
            "  -0.904789    0.        ]] [[ 1.       33.938473]]\n",
            "[[-1.2450545   0.42898148 -0.9886919  -1.9575416   0.         -1.368844\n",
            "  -0.904789    0.        ]] [[ 1.       33.858063]]\n",
            "[[-1.3098749   0.19316661 -0.9886919  -1.9575416   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       40.916813]]\n",
            "[[-1.3746952  -0.04264808 -0.9886919  -1.9575416   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       51.872055]]\n",
            "[[-1.4395156  -0.27846295 -0.9886919  -1.9575416   0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      56.67533]]\n",
            "[[-1.5043359 -0.5142778 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.      58.55918]]\n",
            "[[-1.5691562 -0.7500925 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.277534]]\n",
            "[[-1.6339766 -0.9859074 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.556625]]\n",
            "[[-1.6987969 -1.2217221 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.669586]]\n",
            "[[-1.7636173 -1.457537  -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.717705]]\n",
            "[[-1.8284376 -1.6933519 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.      59.73937]]\n",
            "[[-1.8932579 -1.9291667 -0.9886919 -1.9575416  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.749687]]\n",
            "[[-1.9580783 -2.0932746 -0.9886919  1.9791067  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[  0.      -52.69756]]\n",
            "[[-2.0228987 -1.8574598 -0.9886919  1.9791067  0.        -1.0143445\n",
            "  -0.904789   0.       ]] [[  0.       -48.972286]]\n",
            "[[-2.087719   -1.621645   -0.9886919   1.9791067   0.         -0.48219278\n",
            "  -0.904789    0.        ]] [[  0.       -44.933456]]\n",
            "[[-2.1525393  -1.3858302  -0.9886919   1.9791067   0.          0.00607125\n",
            "  -0.904789    0.        ]] [[  0.       -41.748833]]\n",
            "[[-2.2173595  -1.1500152  -0.9886919   1.9791067   0.          0.45973012\n",
            "  -0.904789    0.        ]] [[  0.      -39.90382]]\n",
            "reset\n",
            "[[ 0.8291962   0.23519397  0.8407947   1.132925    0.         -0.04394993\n",
            "   1.1052301   0.        ]] [[  0.      -58.19413]]\n",
            "[[0.8291962  0.23519397 0.8407947  0.0107825  0.         0.5884097\n",
            "  1.1052301  0.        ]] [[ 1.      28.12511]]\n",
            "[[ 0.8291962   0.23248681  0.8407947  -0.01181337  0.          0.28279153\n",
            "   1.1052301   0.        ]] [[ 1.       10.180673]]\n",
            "[[ 0.8291962   0.22752383  0.8407947  -0.03064326  0.          0.17216441\n",
            "   1.1052301   0.        ]] [[0.6088522 4.7240124]]\n",
            "[[ 0.8291962   0.22068098 -0.9886919  -0.04633484  0.          0.1208315\n",
            "  -0.904789    0.        ]] [[ 0.       -4.664353]]\n",
            "[[ 0.7643759   0.21383795 -0.9886919  -0.04633484  0.          0.17151617\n",
            "  -0.904789    0.        ]] [[ 0.2122818 -0.4032051]]\n",
            "[[ 0.6995556   0.2069951  -0.9886919  -0.04633484  0.          0.17589761\n",
            "  -0.904789    0.        ]] [[0.28470576 0.3063329 ]]\n",
            "[[ 0.6347352   0.20015208 -0.9886919  -0.04633484  0.          0.17256884\n",
            "  -0.904789    0.        ]] [[0.2933727 0.4126448]]\n",
            "[[ 0.5699149   0.19330923 -0.9886919  -0.04633484  0.          0.16808487\n",
            "  -0.904789    0.        ]] [[0.291805   0.42238885]]\n",
            "[[ 0.5050946   0.18646622 -0.9886919  -0.04633484  0.          0.16349505\n",
            "  -0.904789    0.        ]] [[0.28867763 0.41773325]]\n",
            "[[ 0.4402742   0.17962335 -0.9886919  -0.04633484  0.          0.15895575\n",
            "  -0.904789    0.        ]] [[0.2853118  0.41119605]]\n",
            "[[ 0.3754539   0.17278033 -0.9886919  -0.04633484  0.          0.15448757\n",
            "  -0.904789    0.        ]] [[0.28190967 0.4046958 ]]\n",
            "[[ 0.31063354  0.16593748 -0.9886919  -0.04633484  0.          0.15008995\n",
            "  -0.904789    0.        ]] [[0.27848977 0.39840573]]\n",
            "[[ 0.2458132   0.15909447 -0.9886919  -0.04633484  0.          0.14576077\n",
            "  -0.904789    0.        ]] [[0.27506733 0.39247018]]\n",
            "[[ 0.18099287  0.1522516  -0.9886919  -0.04633484  0.          0.14149606\n",
            "  -0.904789    0.        ]] [[0.2716343  0.38680714]]\n",
            "[[ 0.11617254  0.14540859 -0.9886919  -0.04633484  0.          0.13729289\n",
            "  -0.904789    0.        ]] [[0.2682025  0.38152546]]\n",
            "[[ 0.0513522   0.13856573 -0.9886919  -0.04633484  0.          0.13314705\n",
            "  -0.904789    0.        ]] [[0.26476467 0.3765511 ]]\n",
            "[[-0.01346814  0.13172272 -0.9886919  -0.04633484  0.          0.1290553\n",
            "  -0.904789    0.        ]] [[0.26133344 0.37199587]]\n",
            "[[-0.07828847  0.12487986 -0.9886919  -0.04633484  0.          0.12501304\n",
            "  -0.904789    0.        ]] [[0.2579005  0.36777455]]\n",
            "[[-0.14310881  0.11803684 -0.9886919  -0.04633484  0.          0.12101667\n",
            "  -0.904789    0.        ]] [[0.25447845 0.36399633]]\n",
            "[[-0.20792915  0.11119398 -0.9886919  -0.04633484  0.          0.11706129\n",
            "  -0.904789    0.        ]] [[0.2510583  0.36056727]]\n",
            "[[-0.27274948  0.10435096 -0.9886919  -0.04633484  0.          0.11314332\n",
            "  -0.904789    0.        ]] [[0.247655  0.3576184]]\n",
            "[[-0.33756983  0.09750811 -0.9886919  -0.04633484  0.          0.10925731\n",
            "  -0.904789    0.        ]] [[0.24425709 0.35502928]]\n",
            "[[-0.40239015  0.09066509 -0.9886919  -0.04633484  0.          0.10539943\n",
            "  -0.904789    0.        ]] [[0.24087918 0.35292786]]\n",
            "[[-0.4672105   0.08382224 -0.9886919  -0.04633484  0.          0.10156436\n",
            "  -0.904789    0.        ]] [[0.2375119  0.35121423]]\n",
            "[[-0.5320308   0.07697938 -0.9886919  -0.04633484  0.          0.09774786\n",
            "  -0.904789    0.        ]] [[0.23416483 0.34996647]]\n",
            "[[-0.59685117  0.07013636 -0.9886919  -0.04633484  0.          0.09394503\n",
            "  -0.904789    0.        ]] [[0.2308416  0.34920806]]\n",
            "[[-0.6616715   0.0632935  -0.9886919  -0.04633484  0.          0.09015042\n",
            "  -0.904789    0.        ]] [[0.2275329 0.3488391]]\n",
            "[[-0.72649187  0.05645048 -0.9886919  -0.04633484  0.          0.08635978\n",
            "  -0.904789    0.        ]] [[0.2242518  0.34897214]]\n",
            "[[-0.79131216  0.04960763 -0.9886919  -0.04633484  0.          0.08256769\n",
            "  -0.904789    0.        ]] [[0.22098872 0.34950513]]\n",
            "[[-0.8561325   0.04276461 -0.9886919  -0.04633484  0.          0.07876989\n",
            "  -0.904789    0.        ]] [[0.21775663 0.35055238]]\n",
            "[[-0.92095286  0.03592176 -0.9886919  -0.04633484  0.          0.07496056\n",
            "  -0.904789    0.        ]] [[0.21454105 0.35196495]]\n",
            "[[-0.9857732   0.02907874 -0.9886919  -0.04633484  0.          0.07113597\n",
            "  -0.904789    0.        ]] [[0.2113595  0.35390133]]\n",
            "[[-1.0505935   0.02223588 -0.9886919  -0.04633484  0.          0.06729042\n",
            "  -0.904789    0.        ]] [[0.20819762 0.3562156 ]]\n",
            "[[-1.1154139   0.01539286 -0.9886919  -0.04633484  0.          0.06341967\n",
            "  -0.904789    0.        ]] [[0.20506546 0.35899514]]\n",
            "[[-1.1802342   0.00855001 -0.9886919  -0.04633484  0.          0.05951868\n",
            "  -0.904789    0.        ]] [[0.20195359 0.36214203]]\n",
            "[[-1.2450545   0.00170699 -0.9886919  -0.04633484  0.          0.05558346\n",
            "  -0.904789    0.        ]] [[0.19887286 0.36575174]]\n",
            "[[-1.3098749  -0.00513587 -0.9886919  -0.04633484  0.          0.05160911\n",
            "  -0.904789    0.        ]] [[0.19581196 0.36970812]]\n",
            "[[-1.3746952  -0.01197889 -0.9886919  -0.04633484  0.          0.04759165\n",
            "  -0.904789    0.        ]] [[0.19277969 0.3740874 ]]\n",
            "[[-1.4395156  -0.01882174 -0.9886919  -0.04633484  0.          0.04352671\n",
            "  -0.904789    0.        ]] [[0.18976673 0.37879175]]\n",
            "[[-1.5043359  -0.02566476 -0.9886919  -0.04633484  0.          0.03941056\n",
            "  -0.904789    0.        ]] [[0.18678105 0.3838886 ]]\n",
            "[[-1.5691562  -0.03250762 -0.9886919  -0.04633484  0.          0.0352391\n",
            "  -0.904789    0.        ]] [[0.18381384 0.3892845 ]]\n",
            "[[-1.6339766  -0.03935064 -0.9886919  -0.04633484  0.          0.03100901\n",
            "  -0.904789    0.        ]] [[0.1808728  0.39504188]]\n",
            "[[-1.6987969  -0.04619349 -0.9886919  -0.04633484  0.          0.02671631\n",
            "  -0.904789    0.        ]] [[0.17794652 0.4010411 ]]\n",
            "[[-1.7636173  -0.05303651 -0.9886919  -0.04633484  0.          0.02235848\n",
            "  -0.904789    0.        ]] [[0.1750474  0.40738815]]\n",
            "[[-1.8284376  -0.05987937 -0.9886919  -0.04633484  0.          0.01793168\n",
            "  -0.904789    0.        ]] [[0.17216173 0.41393882]]\n",
            "[[-1.8932579  -0.06672239 -0.9886919  -0.04633484  0.          0.01343364\n",
            "  -0.904789    0.        ]] [[0.1693005  0.42078435]]\n",
            "[[-1.9580783  -0.07356524 -0.9886919  -0.04633484  0.          0.0088612\n",
            "  -0.904789    0.        ]] [[0.16645268 0.427804  ]]\n",
            "[[-2.0228987  -0.08040826 -0.9886919  -0.04633484  0.          0.00421247\n",
            "  -0.904789    0.        ]] [[0.16362944 0.4350884 ]]\n",
            "[[-2.087719e+00 -8.725112e-02 -9.886919e-01 -4.633484e-02  0.000000e+00\n",
            "  -5.153045e-04 -9.047890e-01  0.000000e+00]] [[0.16082051 0.44252247]]\n",
            "[[-2.142168   -0.09409413  0.7555475  -0.49051866  0.         -0.00532386\n",
            "  -0.904789    0.        ]] [[ 1.       41.530815]]\n",
            "[[-2.0833883  -0.1541523   0.7555475  -0.49051866  0.         -0.4566137\n",
            "  -0.904789    0.        ]] [[ 1.       22.837688]]\n",
            "[[-2.0246089  -0.21421064  0.7555475  -0.49051866  0.         -0.7047767\n",
            "  -0.904789    0.        ]] [[ 1.       12.624237]]\n",
            "[[-1.9658293  -0.27426898  0.7555475  -0.49051866  0.         -0.8419565\n",
            "  -0.904789    0.        ]] [[ 0.9573101 10.249257 ]]\n",
            "[[-1.9070497  -0.3343273   0.7555475  -0.49051866  0.         -0.95332885\n",
            "  -0.904789    0.        ]] [[0.9086493 9.848437 ]]\n",
            "[[-1.8482702  -0.39438564  0.7555475  -0.49051866  0.         -1.0603458\n",
            "  -0.904789    0.        ]] [[0.8967002 9.788008 ]]\n",
            "[[-1.7894906  -0.45444396  0.7555475  -0.49051866  0.         -1.166706\n",
            "  -0.904789    0.        ]] [[0.8908996 9.783893 ]]\n",
            "[[-1.730711   -0.5145023   0.7555475  -0.49051866  0.         -1.2730215\n",
            "  -0.904789    0.        ]] [[0.8864545 9.7915   ]]\n",
            "[[-1.6719314  -0.57456064  0.7555475  -0.49051866  0.         -1.3794197\n",
            "  -0.904789    0.        ]] [[0.88262564 9.8039055 ]]\n",
            "[[-1.6131519  -0.634619    0.7555475  -0.49051866  0.         -1.4859527\n",
            "  -0.904789    0.        ]] [[0.87927413 9.819769  ]]\n",
            "[[-1.5543723  -0.6946771   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 0.92506534 10.289925  ]]\n",
            "[[-1.4955927  -0.75473547  0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       18.567106]]\n",
            "[[-1.4368131  -0.8147939   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       26.212408]]\n",
            "[[-1.3780335  -0.8748521   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       33.003784]]\n",
            "[[-1.319254   -0.9349105   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       38.822853]]\n",
            "[[-1.2604744  -0.99496883  0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      43.64945]]\n",
            "[[-1.2016948  -1.0550271   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       47.539585]]\n",
            "[[-1.1429152  -1.1150854   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.     50.5978]]\n",
            "[[-1.0841358  -1.1751437   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       52.951736]]\n",
            "[[-1.0253562  -1.2352021   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       54.732494]]\n",
            "[[-0.9665766  -1.2952604   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       56.061577]]\n",
            "[[-0.907797   -1.3553188   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       57.043793]]\n",
            "[[-0.84901744 -1.415377    0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      57.76485]]\n",
            "[[-0.7902378  -1.4754354   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.292137]]\n",
            "[[-0.7314583  -1.5354936   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      58.67708]]\n",
            "[[-0.67267877 -1.595552    0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.958138]]\n",
            "[[-0.6138991  -1.6556103   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.163666]]\n",
            "[[-0.5551196  -1.7156686   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.314346]]\n",
            "[[-0.49634007 -1.7757269   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.425194]]\n",
            "[[-0.4375604  -1.8357853   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.507072]]\n",
            "[[-0.3787809  -1.8958435   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.567833]]\n",
            "[[-0.3200012  -1.9559019   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      59.61315]]\n",
            "[[-0.2612217  -2.0159602   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.647125]]\n",
            "[[-0.2024422  -2.0760186   0.7555475  -0.49051866  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      59.67274]]\n",
            "[[-0.14366254 -2.0932746   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       56.334267]]\n",
            "[[-0.08488303 -2.0332162   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       55.310368]]\n",
            "[[-0.02610351 -1.9731579   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       53.985886]]\n",
            "[[ 0.03267616 -1.9130996   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      52.28784]]\n",
            "[[ 0.09145567 -1.8530413   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       50.136307]]\n",
            "[[ 0.15023534 -1.7929829   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       47.449333]]\n",
            "[[ 0.20901485 -1.7329247   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      44.15032]]\n",
            "[[ 0.26779437 -1.6728663   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       40.177864]]\n",
            "[[ 0.32657403 -1.612808    0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      35.49801]]\n",
            "[[ 0.38535354 -1.5527498   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       30.117502]]\n",
            "[[ 0.44413304 -1.4926914   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       24.094877]]\n",
            "[[ 0.5029126  -1.432633    0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       17.544836]]\n",
            "[[ 0.5616924  -1.3725748   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       10.631808]]\n",
            "[[ 0.6204719  -1.3125165   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[0.30481973 3.5521786 ]]\n",
            "[[ 0.67925143 -1.2524581   0.7555475   0.51208365  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 0.        -3.4907677]]\n",
            "[[ 0.7380309  -1.1923997   0.7555475   0.51208365  0.         -1.5490443\n",
            "  -0.904789    0.        ]] [[ 0.        -7.6348543]]\n",
            "[[ 0.79681045 -1.1323415   0.7555475   0.51208365  0.         -1.466081\n",
            "  -0.904789    0.        ]] [[ 0.      -8.51234]]\n",
            "[[ 0.8291962  -1.0722833  -1.0928118  -0.19331591  0.         -1.3735826\n",
            "  -0.904789    0.        ]] [[ 1.       47.647026]]\n",
            "[[ 0.7569978  -1.0967351  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       41.801136]]\n",
            "[[ 0.68479943 -1.1211871  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       43.267876]]\n",
            "[[ 0.61260074 -1.1456392  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      44.64991]]\n",
            "[[ 0.54040235 -1.1700912  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       45.946205]]\n",
            "[[ 0.46820393 -1.194543   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       47.156616]]\n",
            "[[ 0.39600554 -1.2189951  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       48.281902]]\n",
            "[[ 0.32380697 -1.2434471  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       49.323593]]\n",
            "[[ 0.25160858 -1.267899   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       50.283985]]\n",
            "[[ 0.17941019 -1.2923511  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       51.166042]]\n",
            "[[ 0.10721163 -1.316803   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       51.973236]]\n",
            "[[ 0.03501324 -1.341255   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       52.709484]]\n",
            "[[-0.03718532 -1.365707   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       53.379013]]\n",
            "[[-0.10938372 -1.390159   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      53.98623]]\n",
            "[[-0.18158211 -1.414611   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       54.535637]]\n",
            "[[-0.25378066 -1.439063   -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       55.031723]]\n",
            "[[-0.32597908 -1.4635149  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       55.478897]]\n",
            "[[-0.39817762 -1.4879669  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      55.88142]]\n",
            "[[-0.470376   -1.5124189  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      56.24335]]\n",
            "[[-0.5425744  -1.5368708  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       56.568516]]\n",
            "[[-0.614773   -1.5613228  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       56.860497]]\n",
            "[[-0.68697137 -1.5857748  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      57.12258]]\n",
            "[[-0.75916976 -1.6102269  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      57.35783]]\n",
            "[[-0.8313683  -1.6346788  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       57.568993]]\n",
            "[[-0.9035667  -1.6591308  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       57.758606]]\n",
            "[[-0.9757652  -1.6835828  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       57.928936]]\n",
            "[[-1.0479636  -1.7080348  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      58.08203]]\n",
            "[[-1.1201621  -1.7324867  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.219738]]\n",
            "[[-1.1923606  -1.7569387  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.343704]]\n",
            "[[-1.264559   -1.7813907  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.     58.4554]]\n",
            "[[-1.3367575  -1.8058426  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      58.55613]]\n",
            "[[-1.4089559  -1.8302946  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.647076]]\n",
            "[[-1.4811544  -1.8547467  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.      58.72928]]\n",
            "[[-1.553353   -1.8791987  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.803658]]\n",
            "[[-1.6255513  -1.9036506  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.871044]]\n",
            "[[-1.6977499  -1.9281026  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.932167]]\n",
            "[[-1.7699482  -1.9525546  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       58.987675]]\n",
            "[[-1.8421468  -1.9770066  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.038147]]\n",
            "[[-1.9143451  -2.0014586  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.     59.0841]]\n",
            "[[-1.9865437  -2.0259106  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.125984]]\n",
            "[[-2.058742   -2.0503626  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.164215]]\n",
            "[[-2.1309404  -2.0748146  -1.0928118  -0.19331591  0.         -1.5869763\n",
            "  -0.904789    0.        ]] [[ 1.       59.199158]]\n",
            "[[-2.142168  -2.0932746  1.9200026 -1.5776936  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[ 1.       59.737507]]\n",
            "[[-2.000873  -2.0932746  1.9200026  1.5992585  0.        -1.5869763\n",
            "  -0.904789   0.       ]] [[  0.       -23.885738]]\n",
            "[[-1.8595781 -1.9029675  1.9200026  1.5992585  0.        -1.3274246\n",
            "  -0.904789   0.       ]] [[  0.      -25.73141]]\n",
            "[[-1.7182832 -1.7126602  1.9200026  1.5992585  0.        -1.0478173\n",
            "  -0.904789   0.       ]] [[  0.       -26.648697]]\n",
            "[[-1.5769882 -1.522353   1.9200026  1.5992585  0.        -0.7582424\n",
            "  -0.904789   0.       ]] [[  0.      -27.12972]]\n",
            "[[-1.4356934  -1.3320459   1.9200026   1.5992585   0.         -0.46344045\n",
            "  -0.904789    0.        ]] [[  0.       -27.409693]]\n",
            "[[-1.2943984  -1.1417388   1.9200026   1.5992585   0.         -0.16559625\n",
            "  -0.904789    0.        ]] [[  0.      -27.60299]]\n",
            "[[-1.1531035  -0.9514316   1.9200026   1.5992585   0.          0.13434842\n",
            "  -0.904789    0.        ]] [[  0.       -27.763039]]\n",
            "[[-1.0118085  -0.76112443  1.9200026   1.5992585   0.          0.4360322\n",
            "  -0.904789    0.        ]] [[  0.       -27.911942]]\n",
            "[[-0.87051356 -0.5708173   1.9200026   1.5992585   0.          0.739334\n",
            "  -0.904789    0.        ]] [[  0.      -28.05612]]\n",
            "[[-0.7292187  -0.38051006  1.9200026   1.5992585   0.          1.0442027\n",
            "  -0.904789    0.        ]] [[  0.       -28.194769]]\n",
            "[[-0.5879238  -0.19020297  1.9200026   1.5992585   0.          1.3505776\n",
            "  -0.904789    0.        ]] [[  0.       -28.324434]]\n",
            "[[-4.4662893e-01  1.0429725e-04  1.9200026e+00  1.5992585e+00\n",
            "   0.0000000e+00  1.4990764e+00 -9.0478897e-01  0.0000000e+00]] [[  0.      -34.30968]]\n",
            "[[-0.30533388  0.19041139  1.9200026   1.5992585   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -43.65512]]\n",
            "[[-0.164039   0.3807185  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -50.246376]]\n",
            "[[-0.02274412  0.5710257   1.9200026   1.5992585   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.       -54.459435]]\n",
            "[[ 0.11855092  0.7613328   1.9200026   1.5992585   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.     -56.9261]]\n",
            "[[ 0.2598458   0.95163995  1.9200026   1.5992585   0.          1.4990764\n",
            "  -0.904789    0.        ]] [[  0.      -58.28927]]\n",
            "[[ 0.4011407  1.1419474  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -59.024895]]\n",
            "[[ 0.5424356  1.3322544  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -59.422897]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ImageSequenceClip.py:82: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  size = imread(sequence[0]).shape\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.6837306  1.5225616  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.64228]]\n",
            "[[ 0.8250257  1.7128687  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.      -59.76647]]\n",
            "[[ 0.9663204  1.9031757  1.9200026  1.5992585  0.         1.4990764\n",
            "  -0.904789   0.       ]] [[  0.       -59.838936]]\n",
            "reset\n",
            "Moviepy - Building video ./game_video.mp4.\n",
            "Moviepy - Writing video ./game_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./game_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "eval_env.norm_reward = False  # Disable reward normalization for evaluation\n",
        "\n",
        "# Load the normalization statistics\n",
        "train_env = DummyVecEnv([lambda: Monitor(gym.make('CustomPongEnv-v0'))])\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
        "train_env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", train_env)\n",
        "\n",
        "# Sync the observation normalization statistics\n",
        "eval_env.obs_rms = train_env.obs_rms\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    print(obs, action)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the VecNormalize wrapper\n",
        "env = DummyVecEnv([lambda: gym.make('CustomPongEnv-v0')])\n",
        "env = VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/vecnormalize.pkl\", env)\n",
        "\n",
        "# Extract normalization parameters\n",
        "mean = env.obs_rms.mean\n",
        "std = env.obs_rms.var ** 0.5\n",
        "\n",
        "# Save the mean and std to a file\n",
        "import json\n",
        "normalization_params = {'mean': mean.tolist(), 'std': std.tolist()}\n",
        "with open('normalization_params.json', 'w') as f:\n",
        "    json.dump(normalization_params, f)"
      ],
      "metadata": {
        "id": "TCTA0XkE21Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.policy)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Print the state dictionary keys to understand the structure\n",
        "for name, param in policy.state_dict().items():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "id": "tOQDfwZkEce8",
        "outputId": "968c31f8-2825-498c-c14c-13a4f457192f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorCriticPolicy(\n",
            "  (features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (pi_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (vf_features_extractor): FlattenExtractor(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (mlp_extractor): MlpExtractor(\n",
            "    (policy_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (value_net): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "log_std torch.Size([2])\n",
            "mlp_extractor.policy_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.policy_net.0.bias torch.Size([64])\n",
            "mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.policy_net.2.bias torch.Size([64])\n",
            "mlp_extractor.value_net.0.weight torch.Size([64, 8])\n",
            "mlp_extractor.value_net.0.bias torch.Size([64])\n",
            "mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
            "mlp_extractor.value_net.2.bias torch.Size([64])\n",
            "action_net.weight torch.Size([2, 64])\n",
            "action_net.bias torch.Size([2])\n",
            "value_net.weight torch.Size([1, 64])\n",
            "value_net.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Define the Keras model using the Functional API\n",
        "input_layer = tf.keras.layers.Input(batch_shape=(None, 8), name='input_layer')  # Explicit input shape\n",
        "\n",
        "# Policy network\n",
        "pi_dense1 = tf.keras.layers.Dense(64, activation='tanh', name='dense1_policy')(input_layer)\n",
        "pi_dense2 = tf.keras.layers.Dense(64, activation='tanh', name='dense2_policy')(pi_dense1)\n",
        "action_output = tf.keras.layers.Dense(2, name='action_output')(pi_dense2)\n",
        "\n",
        "# Value network\n",
        "vf_dense1 = tf.keras.layers.Dense(64, activation='tanh', name='dense1_value')(input_layer)\n",
        "vf_dense2 = tf.keras.layers.Dense(64, activation='tanh', name='dense2_value')(vf_dense1)\n",
        "value_output = tf.keras.layers.Dense(1, name='value_output')(vf_dense2)\n",
        "\n",
        "# Create the model\n",
        "tf_model = tf.keras.models.Model(inputs=input_layer, outputs=[action_output, value_output])\n",
        "\n",
        "# Need to run once to initialize model\n",
        "dummy_input = np.random.randn(1, 8).astype(np.float32)  # Assuming input size is 8\n",
        "tf_model(dummy_input)\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Extract the weights and biases\n",
        "weights = {}\n",
        "for name, param in policy.state_dict().items():\n",
        "    weights[name] = param.cpu().numpy()\n",
        "\n",
        "# Map the weights to the TensorFlow model\n",
        "# Note: Transpose the weights because PyTorch uses [out_features, in_features] and TensorFlow uses [in_features, out_features]\n",
        "\n",
        "# Policy network\n",
        "tf_model.get_layer('dense1_policy').set_weights([weights['mlp_extractor.policy_net.0.weight'].T, weights['mlp_extractor.policy_net.0.bias']])\n",
        "tf_model.get_layer('dense2_policy').set_weights([weights['mlp_extractor.policy_net.2.weight'].T, weights['mlp_extractor.policy_net.2.bias']])\n",
        "\n",
        "# Value network\n",
        "tf_model.get_layer('dense1_value').set_weights([weights['mlp_extractor.value_net.0.weight'].T, weights['mlp_extractor.value_net.0.bias']])\n",
        "tf_model.get_layer('dense2_value').set_weights([weights['mlp_extractor.value_net.2.weight'].T, weights['mlp_extractor.value_net.2.bias']])\n",
        "\n",
        "# Action and value outputs\n",
        "tf_model.get_layer('action_output').set_weights([weights['action_net.weight'].T, weights['action_net.bias']])\n",
        "tf_model.get_layer('value_output').set_weights([weights['value_net.weight'].T, weights['value_net.bias']])\n",
        "\n",
        "# Save the model in TensorFlow.js format\n",
        "tf_model.save(\"tf_standard_pong_model.keras\")\n",
        "tfjs.converters.save_keras_model(tf_model, \"tfjs_standard_pong_model\")\n"
      ],
      "metadata": {
        "id": "i-hPY2KxRxsE",
        "outputId": "737aa9ce-e338-42f7-d71a-f7aefeee4bb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Load the trained PyTorch model\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Extract the policy network\n",
        "policy = model.policy\n",
        "\n",
        "# Extract the weights and biases\n",
        "weights = {}\n",
        "for name, param in policy.state_dict().items():\n",
        "    weights[name] = param.cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
        "\n",
        "# Save weights to a JSON file\n",
        "with open(\"weights.json\", \"w\") as f:\n",
        "    json.dump(weights, f)\n"
      ],
      "metadata": {
        "id": "YpzzZP8iYZg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"tf_custom_pong_model.keras\"\n",
        "tf_model.save(saved_model_path)\n"
      ],
      "metadata": {
        "id": "4Yc_bcrpJwOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare pytorch and tensorflow models\n",
        "\n",
        "# Generate some test inputs\n",
        "test_inputs = np.random.randn(10, 8).astype(np.float32)  # Assuming input size is 8\n",
        "\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "pytorch_model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "# Check the device of the model\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "pytorch_model.policy.to(device)\n",
        "\n",
        "\n",
        "# Convert test inputs to PyTorch tensor and move to the same device as the model\n",
        "test_inputs_torch = th.tensor(test_inputs).to(device)\n",
        "\n",
        "# Get the outputs from the PyTorch model\n",
        "pytorch_outputs = []\n",
        "for input_tensor in test_inputs_torch:\n",
        "    with th.no_grad():\n",
        "        # Ensure input_tensor is on the same device\n",
        "        input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Extract the policy's action and value\n",
        "        obs_features = pytorch_model.policy.extract_features(input_tensor)\n",
        "        pi_latent, vf_latent = pytorch_model.policy.mlp_extractor(obs_features)\n",
        "        action_mean = pytorch_model.policy.action_net(pi_latent)\n",
        "        value = pytorch_model.policy.value_net(vf_latent)\n",
        "\n",
        "        # Collect the outputs\n",
        "        pytorch_outputs.append((action_mean.cpu().numpy(), value.cpu().numpy()))\n",
        "\n",
        "# Register the custom model when loading\n",
        "tf_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "# Get the outputs from the TensorFlow model\n",
        "tf_outputs = []\n",
        "for input_array in test_inputs:\n",
        "    input_tensor = tf.convert_to_tensor(input_array.reshape(1, -1), dtype=tf.float32)\n",
        "    action, value = tf_model(input_tensor)\n",
        "    tf_outputs.append((action.numpy(), value.numpy()))\n",
        "\n",
        "\n",
        "# Compare the outputs\n",
        "for i, (pytorch_output, tf_output) in enumerate(zip(pytorch_outputs, tf_outputs)):\n",
        "    pytorch_action, pytorch_value = pytorch_output\n",
        "    tf_action, tf_value = tf_output\n",
        "\n",
        "    print(f\"Test Input {i + 1}:\")\n",
        "    print(f\"PyTorch Action: {pytorch_action}, TensorFlow Action: {tf_action}\")\n",
        "    print(f\"PyTorch Value: {pytorch_value}, TensorFlow Value: {tf_value}\")\n",
        "    print(f\"Action Difference: {np.abs(pytorch_action - tf_action).sum()}\")\n",
        "    print(f\"Value Difference: {np.abs(pytorch_value - tf_value).sum()}\\n\")\n"
      ],
      "metadata": {
        "id": "UQwtivEXKBg2",
        "outputId": "b57ad718-22be-42c7-da36-ba0710ee4bd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Input 1:\n",
            "PyTorch Action: [[1.2848994 9.803228 ]], TensorFlow Action: [[1.2848994 9.80323  ]]\n",
            "PyTorch Value: [[-7.7905803]], TensorFlow Value: [[-7.7905803]]\n",
            "Action Difference: 1.9073486328125e-06\n",
            "Value Difference: 0.0\n",
            "\n",
            "Test Input 2:\n",
            "PyTorch Action: [[ -5.7213635 -56.177452 ]], TensorFlow Action: [[ -5.7213635 -56.177452 ]]\n",
            "PyTorch Value: [[-4.6384115]], TensorFlow Value: [[-4.6384134]]\n",
            "Action Difference: 0.0\n",
            "Value Difference: 1.9073486328125e-06\n",
            "\n",
            "Test Input 3:\n",
            "PyTorch Action: [[ 6.021303 56.608696]], TensorFlow Action: [[ 6.0213037 56.608704 ]]\n",
            "PyTorch Value: [[-4.490585]], TensorFlow Value: [[-4.490585]]\n",
            "Action Difference: 8.106231689453125e-06\n",
            "Value Difference: 0.0\n",
            "\n",
            "Test Input 4:\n",
            "PyTorch Action: [[ 6.327306 59.44471 ]], TensorFlow Action: [[ 6.3273077 59.444717 ]]\n",
            "PyTorch Value: [[-5.0233054]], TensorFlow Value: [[-5.0233073]]\n",
            "Action Difference: 9.5367431640625e-06\n",
            "Value Difference: 1.9073486328125e-06\n",
            "\n",
            "Test Input 5:\n",
            "PyTorch Action: [[ 6.233198 58.53507 ]], TensorFlow Action: [[ 6.233197 58.53506 ]]\n",
            "PyTorch Value: [[-5.260474]], TensorFlow Value: [[-5.2604737]]\n",
            "Action Difference: 8.58306884765625e-06\n",
            "Value Difference: 4.76837158203125e-07\n",
            "\n",
            "Test Input 6:\n",
            "PyTorch Action: [[ 6.334947 59.526264]], TensorFlow Action: [[ 6.334947 59.526268]]\n",
            "PyTorch Value: [[-0.10718327]], TensorFlow Value: [[-0.10717996]]\n",
            "Action Difference: 3.814697265625e-06\n",
            "Value Difference: 3.3080577850341797e-06\n",
            "\n",
            "Test Input 7:\n",
            "PyTorch Action: [[ 6.3549957 59.725563 ]], TensorFlow Action: [[ 6.3549957 59.725555 ]]\n",
            "PyTorch Value: [[2.1975527]], TensorFlow Value: [[2.1975522]]\n",
            "Action Difference: 7.62939453125e-06\n",
            "Value Difference: 4.76837158203125e-07\n",
            "\n",
            "Test Input 8:\n",
            "PyTorch Action: [[ 1.6091155 13.576336 ]], TensorFlow Action: [[ 1.609115 13.576335]]\n",
            "PyTorch Value: [[3.9197407]], TensorFlow Value: [[3.919742]]\n",
            "Action Difference: 1.430511474609375e-06\n",
            "Value Difference: 1.430511474609375e-06\n",
            "\n",
            "Test Input 9:\n",
            "PyTorch Action: [[ 6.356987 59.752792]], TensorFlow Action: [[ 6.356985 59.752792]]\n",
            "PyTorch Value: [[-3.5725503]], TensorFlow Value: [[-3.5725498]]\n",
            "Action Difference: 1.9073486328125e-06\n",
            "Value Difference: 4.76837158203125e-07\n",
            "\n",
            "Test Input 10:\n",
            "PyTorch Action: [[ -5.0240135 -49.755493 ]], TensorFlow Action: [[ -5.0240135 -49.755505 ]]\n",
            "PyTorch Value: [[-11.206833]], TensorFlow Value: [[-11.206834]]\n",
            "Action Difference: 1.1444091796875e-05\n",
            "Value Difference: 9.5367431640625e-07\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=keras --output_format=tfjs_graph_model tf_custom_pong_model.keras tfjs_custom_pong_model"
      ],
      "metadata": {
        "id": "wnaMw0pAURsM",
        "outputId": "4e3fe7ef-c29a-45ba-c079-5a721aa58887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-07 15:24:01.679860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/serialization_lib.py\", line 807, in _retrieve_class_or_fn\n",
            "    mod = importlib.import_module(module)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'tf_keras.src.models.functional'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorflowjs_converter\", line 8, in <module>\n",
            "    sys.exit(pip_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 959, in pip_main\n",
            "    main([' '.join(sys.argv[1:])])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 963, in main\n",
            "    convert(argv[0].split(' '))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 949, in convert\n",
            "    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 635, in _dispatch_converter\n",
            "    dispatch_keras_h5_to_tfjs_graph_model_conversion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflowjs/converters/converter.py\", line 227, in dispatch_keras_h5_to_tfjs_graph_model_conversion\n",
            "    model = tf_keras.models.load_model(h5_path, compile=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_api.py\", line 254, in load_model\n",
            "    return saving_lib.load_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_lib.py\", line 271, in load_model\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_lib.py\", line 236, in load_model\n",
            "    model = deserialize_keras_object(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/serialization_lib.py\", line 704, in deserialize_keras_object\n",
            "    cls = _retrieve_class_or_fn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/serialization_lib.py\", line 809, in _retrieve_class_or_fn\n",
            "    raise TypeError(\n",
            "TypeError: Could not deserialize class 'Functional' because its parent module tf_keras.src.models.functional cannot be imported. Full object config: {'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {'name': 'functional_6', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 8], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_3'}, 'registered_name': None, 'name': 'input_layer_3', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense1_policy', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8]}, 'name': 'dense1_policy', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8], 'dtype': 'float32', 'keras_history': ['input_layer_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense1_value', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8]}, 'name': 'dense1_value', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8], 'dtype': 'float32', 'keras_history': ['input_layer_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense2_policy', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'dense2_policy', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 64], 'dtype': 'float32', 'keras_history': ['dense1_policy', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense2_value', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'dense2_value', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 64], 'dtype': 'float32', 'keras_history': ['dense1_value', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'action_output', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'action_output', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 64], 'dtype': 'float32', 'keras_history': ['dense2_policy', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'value_output', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'value_output', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 64], 'dtype': 'float32', 'keras_history': ['dense2_value', 0, 0]}}], 'kwargs': {}}]}], 'input_layers': [['input_layer_3', 0, 0]], 'output_layers': [['action_output', 0, 0], ['value_output', 0, 0]]}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflowjs as tfjs\n",
        "# Load the saved Keras model with custom objects\n",
        "model = tf.keras.models.load_model(\"tf_custom_pong_model.keras\")\n",
        "# Save the model in TensorFlow.js format\n",
        "tfjs.converters.save_keras_model(model, \"tfjs_custom_pong_model_2\")"
      ],
      "metadata": {
        "id": "n-xVuXrgbJql",
        "outputId": "033ad6f4-72b5-42cd-e776-89fcefa7f069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}