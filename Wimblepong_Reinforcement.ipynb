{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/Wimblepong_Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NkhwGXZyRfWZ"
      },
      "outputs": [],
      "source": [
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install tensorflowjs\n",
        "%pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
        "\n",
        "# onnx2tf deps:\n",
        "%pip install onnx_graphsurgeon\n",
        "%pip install sng4onnx\n",
        "\n",
        "%pip install onnxscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYCJyx3kJikU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "DAY = \"26-thursday\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRePUZ1QRh6C"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import torch as th\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from onnx2tf import convert\n",
        "\n",
        "\n",
        "import onnx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15  # Example value, adjust as needed\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4  # Example value, adjust as needed\n",
        "SPEED_INCREMENT = 0.6  # Example value, adjust as needed\n",
        "SERVING_HEIGHT_MULTIPLIER = 2  # Example value, adjust as needed\n",
        "PLAYER_COLOURS = {'Player1': 'red', 'Player2': 'blue'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "rewards_map = {\n",
        "    \"hit_paddle\": lambda _: 50,\n",
        "    \"score_point\": lambda _: 100,\n",
        "    \"conceed_point\": lambda ball, paddle, rally_length: (-abs(ball['y'] - paddle['y']) / max(rally_length, 1)),\n",
        "    \"serve\": lambda ball_speed: ball_speed,\n",
        "    \"paddle_movement\": lambda dy: 0,\n",
        "    \"ball_distance\": lambda ball, paddle: 0\n",
        "}\n",
        "\n",
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "class GameEventType:\n",
        "    ResetBall = 'ResetBall'\n",
        "    Serve = 'Serve'\n",
        "    WallContact = 'WallContact'\n",
        "    HitPaddle = 'HitPaddle'\n",
        "    ScorePointLeft = 'ScorePointLeft'\n",
        "    ScorePointRight = 'ScorePointRight'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "def bounded_value(value, min_value, max_value):\n",
        "        return max(min_value, min(max_value, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N932taVhk14O"
      },
      "outputs": [],
      "source": [
        "class ComputerPlayer:\n",
        "    def __init__(self):\n",
        "        self.reset(serve_delay=50, initial_direction=15, offset=0, max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "\n",
        "    def reset(self, serve_delay, initial_direction, offset, max_speed):\n",
        "        # print(\"ComputerPlayer reset\")\n",
        "        self.serve_delay = serve_delay\n",
        "        self.serve_delay_counter = 0\n",
        "        self.direction = initial_direction\n",
        "        self.offset = initial_direction\n",
        "        self.max_speed = max_speed\n",
        "\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2  ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardSystem:\n",
        "    def __init__(self, rewarded_player):\n",
        "        self.rewarded_player = rewarded_player\n",
        "        self.total_reward = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.step_count += 1\n",
        "\n",
        "    def pre_serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            self.total_reward -= 0.01\n",
        "            # if self.step_count > 150:\n",
        "            #   self.total_reward -= self.step_count * 0.02\n",
        "            # else:\n",
        "            #   ball = game_state['ball']\n",
        "            #   reward = (abs(ball['dy']) * ((1000 - self.step_count) / 1000)) / 10\n",
        "            #   self.total_reward += reward\n",
        "\n",
        "    def serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            ball = game_state['ball']\n",
        "            reward = abs(ball['dy']) * abs(ball['dy'])\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def hit_paddle_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 50\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def conceed_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = (-abs(game_state['ball']['y'] - game_state[player]['y']) / max(game_state['stats']['rally_length'], 1))\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def score_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 100\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def paddle_movement_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            paddle = game_state[player]\n",
        "            reward = 0\n",
        "            self.total_reward += reward\n"
      ],
      "metadata": {
        "id": "b3MKATB64e3l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iIpdvdwMk14O"
      },
      "outputs": [],
      "source": [
        "class PongGame:\n",
        "    def __init__(self, server, positions_reversed, player, opponent):\n",
        "        self.game_state = {\n",
        "        'server': server,\n",
        "        'positions_reversed': positions_reversed,\n",
        "        'player': player,\n",
        "        'opponent': opponent,\n",
        "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'blue'},\n",
        "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': 'red'},\n",
        "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def update_game_state(self, actions, delta_time, reward_system):\n",
        "        reward = 0\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            return True\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "\n",
        "            reward_system.pre_serve_reward(server, game_state)\n",
        "\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "\n",
        "                reward_system.serve_reward(server, game_state)\n",
        "\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if not player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "\n",
        "        reward_system.paddle_movement_reward(self.game_state['player'], game_state)\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self, computer_player):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "        self.starting_states = [\n",
        "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "        self.starting_state_index = 0\n",
        "\n",
        "        self.computer_player = computer_player\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        # print(\"CustomPongEnv reset\")\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "        # print(self.starting_state_index)\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        player = starting_state['player']\n",
        "        opponent = starting_state['opponent']\n",
        "        self.computer_player.reset(serve_delay=random.randint(10, 10), initial_direction=random.randint(-60, 60), offset=random.randint(-PADDLE_HEIGHT/2, PADDLE_HEIGHT/2), max_speed=MAX_COMPUTER_PADDLE_SPEED)\n",
        "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
        "        self.reward_system = RewardSystem(rewarded_player=player)\n",
        "        self.step_count = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(f\"Action taken: {action}\")\n",
        "        self.step_count += 1\n",
        "        self.reward_system.reset()\n",
        "        button_pressed = action[0] > 0.5\n",
        "        paddle_direction = action[1]\n",
        "        model_player_actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 60}\n",
        "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
        "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
        "        terminated = self.game.update_game_state(actions, 2.5, self.reward_system)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        truncated = False\n",
        "        if self.step_count > 1000:\n",
        "            terminated = True\n",
        "        reward = self.reward_system.total_reward\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.game_state\n",
        "        player = state['player']\n",
        "        is_server = 1 if self.game.game_state['server'] == player else 0\n",
        "        paddle = state[player]\n",
        "        obs = np.array([\n",
        "            float(state['ball']['x'] / COURT_WIDTH),\n",
        "            float(state['ball']['y'] / COURT_HEIGHT),\n",
        "            float(state['ball']['dx'] / 40),\n",
        "            float(state['ball']['dy'] / 40),\n",
        "            float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
        "            float(paddle['y'] / COURT_HEIGHT),\n",
        "            float(int(state['ball']['serve_mode'])),\n",
        "            float(int(is_server)),\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n",
        "\n",
        "\n",
        "register(\n",
        "    id='CustomPongEnv-v0',\n",
        "    entry_point='__main__:CustomPongEnv',  # This entry point should match your custom environment class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w4LjxARS9zCF"
      },
      "outputs": [],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
        "# env = VecCheckNan(env, raise_exception=True)  # Wrap with VecCheckNan to detect NaNs\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(10000000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        break\n",
        "        print(\"Environment reset\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R172XbXX5Am5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.mean_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "            self.mean_rewards.append(self.last_mean_reward)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sjyFuJcGT-w"
      },
      "outputs": [],
      "source": [
        "# Create and train\n",
        "ent_coef=0.1\n",
        "eval_freq=2000\n",
        "gamma=0.95\n",
        "# Create vectorized environments for training and evaluation\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Train the model with the callback\n",
        "model = PPO('MlpPolicy', train_env, verbose=1, ent_coef=ent_coef, gamma=gamma)\n",
        "model.learn(total_timesteps=100000, callback=eval_callback, progress_bar=False)\n",
        "\n",
        "# Save the model\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL_LYMHeejpF"
      },
      "outputs": [],
      "source": [
        "# Load and train\n",
        "ent_coef=0.05\n",
        "gamma=0.999\n",
        "eval_freq=2000\n",
        "# Load the trained model and ensure the training environment is wrapped with VecNormalize\n",
        "train_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer())) for _ in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
        "train_env.training = True  # Ensure it's in training mode\n",
        "\n",
        "# Create the evaluation environment and wrap it with VecNormalize\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure it's not in training mode\n",
        "\n",
        "\n",
        "# Create the CustomEvalCallback with the evaluation environment\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Load the pre-trained model\n",
        "# model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\", env=train_env, ent_coef=ent_coef, gamma=gamma)\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\", env=train_env, ent_coef=ent_coef, gamma=gamma)\n",
        "\n",
        "# Resume training the model with the callback\n",
        "model.learn(total_timesteps=100000, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")\n",
        "\n",
        "# Plot the mean rewards\n",
        "plt.plot(eval_callback.mean_rewards)\n",
        "plt.xlabel('Evaluation step')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title(f'Mean Reward per Evaluation {ent_coef=} {eval_freq=}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FQZDHQX5lGpv"
      },
      "outputs": [],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\")\n",
        "# model = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "eval_env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer())])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False  # Ensure we're not in training mode to prevent normalization updates\n",
        "\n",
        "\n",
        "# Extract the first environment from the vectorized environment\n",
        "env = eval_env.envs[0]\n",
        "\n",
        "# Run a simple loop to demonstrate rendering with the trained model\n",
        "obs = eval_env.reset()\n",
        "count = 0\n",
        "\n",
        "while count < 4:\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Get action from the trained model\n",
        "    # print(obs, action)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      obs = eval_env.reset()\n",
        "      env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-hPY2KxRxsE",
        "outputId": "eab0d0ac-61fc-4c16-9188-19f3a634870d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Actions: [[-15.044194 370.17538 ]]\n",
            "ONNX Values: [[-6.150612]]\n",
            "ONNX Log Prob: [-31.393408]\n",
            "PyTorch Actions: [[-15.044194 370.17535 ]]\n",
            "PyTorch Values: [[-6.150613]]\n",
            "PyTorch Log Prob: [-31.393408]\n",
            "Actions match: True\n",
            "Values match: True\n",
            "Log prob match: True\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "from typing import Tuple\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        # Run the policy in deterministic mode\n",
        "        actions, values, log_prob = self.policy(observation, deterministic=True)\n",
        "        return actions, values, log_prob\n",
        "\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1\"\n",
        "model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "\n",
        "# Define dummy input based on the observation space shape\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_dynamo.onnx\"\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\", \"values\", \"log_prob\"]\n",
        ")\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(onnx_file_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Prepare a dummy observation for testing\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "\n",
        "# Create an ONNX runtime session\n",
        "ort_sess = ort.InferenceSession(onnx_file_path)\n",
        "ort_inputs = {\"input\": observation}\n",
        "ort_outputs = ort_sess.run(None, ort_inputs)\n",
        "\n",
        "# Output from ONNX\n",
        "onnx_actions, onnx_values, onnx_log_prob = ort_outputs\n",
        "\n",
        "# Print ONNX outputs\n",
        "print(\"ONNX Actions:\", onnx_actions)\n",
        "print(\"ONNX Values:\", onnx_values)\n",
        "print(\"ONNX Log Prob:\", onnx_log_prob)\n",
        "\n",
        "# Check that the predictions are the same in PyTorch\n",
        "with th.no_grad():\n",
        "    pytorch_outputs = onnx_policy(th.as_tensor(observation))\n",
        "\n",
        "# Print PyTorch outputs\n",
        "print(\"PyTorch Actions:\", pytorch_outputs[0].numpy())\n",
        "print(\"PyTorch Values:\", pytorch_outputs[1].numpy())\n",
        "print(\"PyTorch Log Prob:\", pytorch_outputs[2].numpy())\n",
        "\n",
        "# Comparison function\n",
        "def compare_outputs(pytorch_outputs, onnx_outputs):\n",
        "    pytorch_actions, pytorch_values, pytorch_log_prob = [output.numpy() for output in pytorch_outputs]\n",
        "    onnx_actions, onnx_values, onnx_log_prob = onnx_outputs\n",
        "\n",
        "    actions_match = np.allclose(pytorch_actions, onnx_actions, atol=1e-5)\n",
        "    values_match = np.allclose(pytorch_values, onnx_values, atol=1e-5)\n",
        "    log_prob_match = np.allclose(pytorch_log_prob, onnx_log_prob, atol=1e-5)\n",
        "\n",
        "    print(f\"Actions match: {actions_match}\")\n",
        "    print(f\"Values match: {values_match}\")\n",
        "    print(f\"Log prob match: {log_prob_match}\")\n",
        "\n",
        "# Compare the outputs\n",
        "compare_outputs(pytorch_outputs, ort_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx2tf import convert\n",
        "\n",
        "# Define paths\n",
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tf\"\n",
        "\n",
        "# Convert ONNX to TensorFlow SavedModel\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "CPSegjRq1cOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnaMw0pAURsM"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/ppo_custom_pong_1_tfjs_2\"\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow.js\n",
        "subprocess.run([\n",
        "    'tensorflowjs_converter',\n",
        "    '--input_format', 'tf_saved_model',\n",
        "    '--output_format', 'tfjs_graph_model',\n",
        "    \"--signature_name\", \"serving_default\",\n",
        "    tf_model_path,\n",
        "    tfjs_model_path\n",
        "])\n",
        "\n",
        "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}